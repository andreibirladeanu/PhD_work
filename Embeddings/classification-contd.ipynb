{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Classification/speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('classification.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.array(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "print(scaler.fit(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=1)\n",
    "group_k_fold = GroupKFold(n_splits=6)\n",
    "splits = group_k_fold.split(X_shuffled, y_shuffled, groups_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in splits:\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_shuffled[train_index], X_shuffled[test_index]\n",
    "    y_train, y_test = y_shuffled[train_index], y_shuffled[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in group_k_fold.split(X_shuffled, y_shuffled, groups_shuffled):\n",
    "    print(np.unique(y_shuffled[test_index], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_shuffled, y_shuffled, scoring='accuracy', cv=group_k_fold, n_jobs=-1, groups=groups_shuffled)\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=list()\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled):\n",
    "    labels.append(y_shuffled[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=list()\n",
    "coefs = list()\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled):\n",
    "    model.fit(X_shuffled[train_index], y_shuffled[train_index])\n",
    "    temp_list = model.predict(X_shuffled[test_index])\n",
    "    coefs.append(model.coef_)\n",
    "    predictions.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.array(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_coefs = coefs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=0\n",
    "for x,y in zip(predictions,labels):\n",
    "    for prediction, label in zip(x,y):\n",
    "        if int(prediction)==int(label):\n",
    "            accuracy +=1\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy/len(X)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_recall = cross_val_score(model, X_shuffled, y_shuffled, scoring='recall', cv=group_k_fold, n_jobs=-1, groups=groups_shuffled)\n",
    "print('recall: %.3f (%.3f)' % (mean(scores_recall), std(scores_recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_precision = cross_val_score(model, X_shuffled, y_shuffled, scoring='precision', cv=group_k_fold, n_jobs=-1, groups=groups_shuffled)\n",
    "print('precision: %.3f (%.3f)' % (mean(scores_precision), std(scores_precision)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [data.iloc[:,1:12].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.transpose(abs(avg_coefs[0])), index=features_names[0]).nlargest(12).plot(kind='barh', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_holder = []\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled):\n",
    "    model.fit(X_shuffled[train_index], y_shuffled[train_index])\n",
    "    print(confusion_matrix(y_shuffled[test_index], model.predict(X_shuffled[test_index])))\n",
    "    cm_holder.append(confusion_matrix(y_shuffled[test_index], model.predict(X_shuffled[test_index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cm = sum(cm_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.imshow(final_cm,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "tick_marks = np.arange(len(set(y_shuffled[test_index]))) # length of classes\n",
    "class_labels = ['no rad','rad']\n",
    "tick_marks\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks(tick_marks,class_labels)\n",
    "# plotting text value inside cells\n",
    "thresh = final_cm.max() / 2.\n",
    "for i,j in itertools.product(range(final_cm.shape[0]),range(final_cm.shape[1])):\n",
    "    plt.text(j,i,format(final_cm[i,j],'d'),horizontalalignment='center',color='white' if final_cm[i,j] >thresh else 'black')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives=0\n",
    "false_positives=0\n",
    "true_negatives=0\n",
    "for x,y in zip(labels,predictions):\n",
    "    for label, prediction in zip(x,y):\n",
    "        if int(label)==int(prediction)==1:\n",
    "            true_positives +=1\n",
    "\n",
    "for x,y in zip(labels,predictions):\n",
    "    for label, prediction in zip(x,y):\n",
    "        if int(label)==int(prediction)==0:\n",
    "            true_negatives +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "# code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "for i, (train, test) in enumerate(group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled)):\n",
    "    model.fit(X_shuffled[train], y_shuffled[train])\n",
    "    viz = plot_roc_curve(model, X_shuffled[test], y_shuffled[test],\n",
    "                         name='ROC fold {}'.format(i),\n",
    "                         alpha=0.3, lw=1, ax=ax)\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.xaxis.label.set_size(20)\n",
    "ax.yaxis.label.set_size(20)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indexes_train = list()\n",
    "list_indexes_test =list()\n",
    "temp_list = list()\n",
    "predictions = list()\n",
    "y_test_index = list()\n",
    "\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled):\n",
    "    model.fit(X_shuffled[train_index], y_shuffled[train_index])\n",
    "    temp_list = model.predict(X_shuffled[test_index])\n",
    "    temp_list2=list()\n",
    "    temp_list3=list()\n",
    "    temp_list2.append(groups_shuffled[test_index])\n",
    "    temp_list3.append(y_shuffled[test_index])\n",
    "    y_test_index.append(temp_list3)\n",
    "    list_indexes_test.append(temp_list2)\n",
    "    #coefs.append(model.coef_)\n",
    "    predictions.append(temp_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = list()\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled):\n",
    "    temp=list()\n",
    "    temp.append(test_index)\n",
    "    test_indexes.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "tuple_index_pred = list()\n",
    "while k<6:\n",
    "    for x, y,z in zip(list_indexes_test[k][0], predictions[k], y_test_index[k][0]):\n",
    "        tuple_index_pred.append((x,y,z))\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_index_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = list()\n",
    "t_list2 = list()\n",
    "t_list3=list()\n",
    "for x in tuple_index_pred:\n",
    "    t_list.append(x[0])\n",
    "    t_list2.append(x[1])\n",
    "    t_list3.append(x[2])\n",
    "\n",
    "final_df = pd.DataFrame(list(zip(t_list, t_list2, t_list3)), columns = ['index', 'label', 'ground_truth'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['label']==1].duplicated().describe() #46 with False and 19 with True so 19 people have both 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['label']==0].duplicated().describe() #38 with False and 14 with True so 14 people have both 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of 56 we have 12 that have both recordings correct and 44 where either both are or only one is incorrect\n",
    "# ok so 3 measures: how many where both are correct, how many where both are incorrect, how many where only one is correct\n",
    "# so 12 where both recordings are correct, but how many where both are incorrect? \n",
    "# and how many where only one is? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. discard all people where there is a disagree in label \n",
    "final_df['discarded'] = final_df[['index','label']].duplicated(keep=False) # duplicated means True; disagree in label means false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those where there is agree  in label ; so duplicated ones \n",
    "final_df.loc[final_df['discarded']==True] # so we have 66 where there is agreement in label in the classifier output\n",
    "# and 51 where there isn't\n",
    "# so 66 with both recordings in agreement; so actually 33 people out of the 56 that have both recordings\n",
    "# so the rest of 23 people we leave to clinician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['discarded']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree = final_df.loc[final_df['discarded']==False]\n",
    "len(df_disagree) -5 = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree= df_disagree.drop([102, 73, 83, 44, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_disagree.loc[df_disagree['ground_truth']==0])  # 22 out of 46 \n",
    "# 11 out of 23 children for whom there was a disagree actually got a no_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate accuracy over these\n",
    "final_discarded = final_df.loc[final_df['discarded']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discarded[final_discarded['ground_truth']==1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=0\n",
    "for x,y in zip(final_discarded['label'], final_discarded['ground_truth']):\n",
    "        if x==y:\n",
    "            accuracy +=1\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "for x,y in zip(final_discarded['label'], final_discarded['ground_truth']):\n",
    "        if x!=y and x==1:\n",
    "            fp +=1\n",
    "        if x!=y and x==0:\n",
    "            fn +=1\n",
    "        if x==y and x==1:\n",
    "            tp +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy/66*100 # out of those that there's agreement 69% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy # chi square value is 2.77 (significant at p<.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what would we expect if it was random: \n",
    "final_discarded['ground_truth'].value_counts() # 18 people with no rad and 15 with rad out of those we keep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discarded['ground_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot new cm and maybe ROC \n",
    "# confusion matrix at person level \n",
    "y_true = np.array(final_discarded['ground_truth'])\n",
    "y_pred = np.array(final_discarded['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_person = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.imshow(cm_person,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix Person level')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "tick_marks = np.arange(len(set(y_shuffled[test_index]))) # length of classes\n",
    "class_labels = ['no rad','rad']\n",
    "tick_marks\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks(tick_marks,class_labels)\n",
    "# plotting text value inside cells\n",
    "thresh = cm_person.max() / 2.\n",
    "for i,j in itertools.product(range(cm_person.shape[0]),range(cm_person.shape[1])):\n",
    "    plt.text(j,i,format(cm_person[i,j],'d'),horizontalalignment='center',color='white' if cm_person[i,j] >thresh else 'black')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = [.378, .122, .161, .379, .721, .910, .367, .892, .979, .381, .007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipletests(pvals, method='fdr_bh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
