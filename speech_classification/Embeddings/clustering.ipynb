{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyannote.core import Segment, Timeline\n",
    "import os\n",
    "from numpy import loadtxt\n",
    "from numpy import genfromtxt\n",
    "import statistics\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/Embeddings/Meal/Embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/andrei-macpro/Documents/Data/Audio/Embeddings/Meal/Embeddings'  #if string starts with slash it is considered absolute\n",
    "dirs = os.listdir( path )\n",
    "file_names=sorted([i for i in os.listdir(\".\") if not i.startswith(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/overlapping_speech/no_os_meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing for pydub \n",
    "path = '/Users/andrei-macpro/Documents/Data/Audio/overlapping_speech/no_os_meal'  #if string starts with slash it is considered absolute\n",
    "dirs = os.listdir( path )\n",
    "file_names=sorted([i for i in os.listdir(\".\") if not i.startswith(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydub_files_meal = [AudioSegment.from_wav(file) for file in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=list()\n",
    "for file in file_names:\n",
    "    embeddings.append(genfromtxt(file, delimiter=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timestamps\n",
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/Embeddings/Meal/Timestamps')\n",
    "dirs = os.listdir('/Users/andrei-macpro/Documents/Data/Audio/Embeddings/Meal/Timestamps')\n",
    "file_names=sorted([i for i in os.listdir(\".\") if not i.startswith(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps=list()\n",
    "for file in file_names: \n",
    "    timestamps.append(genfromtxt(file, dtype=None, encoding = None, delimiter=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps=list()\n",
    "import pandas\n",
    "for file in file_names:\n",
    "    data = pandas.read_csv(file).to_numpy()\n",
    "    timestamps.append(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "for file in embeddings:       \n",
    "    labels = kmeans.fit_predict(file)\n",
    "    predictions.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters0, clusters1, clusters2, clusters3 = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters0 = [np.array([0 for label in prediction if label==0])   for prediction in predictions]\n",
    "clusters1 = [np.array([1 for label in prediction if label==1])   for prediction in predictions]\n",
    "clusters2 = [np.array([2 for label in prediction if label==2])   for prediction in predictions]\n",
    "clusters3 = [np.array([3 for label in prediction if label==3])  for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters0[0] + clusters1[0] + clusters2[0]+ clusters3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets calculate variance of these clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need the cluster sizes data \n",
    "clusters=np.array(clusters0+clusters1+clusters2+clusters3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters0_size = np.array([len(cluster) for cluster in clusters0])\n",
    "clusters1_size = np.array([len(cluster) for cluster in clusters1])\n",
    "clusters2_size = np.array([len(cluster) for cluster in clusters2])\n",
    "clusters3_size = np.array([len(cluster) for cluster in clusters3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters1_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import variance\n",
    "\n",
    "variance([clusters0_size[0], clusters1_size[0], clusters2_size[0], clusters3_size[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "[clusters0_size[0], clusters1_size[0], clusters2_size[0], clusters3_size[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_list=list()\n",
    "for i in range(len(pydub_files_meal)):\n",
    "    variance_list.append(variance([clusters0_size[i], clusters1_size[i], clusters2_size[i], clusters3_size[i]])/pydub_files_meal[i].duration_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_list=np.array(variance_list)\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "savetxt('variance.csv', variance_list, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_sizes = list()\n",
    "for cluster in range(0, len(clusters), 61):\n",
    "       clusters[cluster]\n",
    "    temp_list=list()\n",
    "    temp_list.append(len(clusters[cluster]))\n",
    "    cluster_sizes.append(len(clusters[cluster]))\n",
    "    continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while cluster<len(clusters):\n",
    "    for cluster in range(0, len(clusters),61):\n",
    "        temp_list=list()\n",
    "        temp_list.append(len(clusters[cluster]))\n",
    "        \n",
    "    cluster=cluster+61\n",
    "    cluster_sizes.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = list()\n",
    "i=0\n",
    "while i <= len(clusters):\n",
    "    cluster_sizes.append(len(clusters[i])\n",
    "    i=i+1                  \n",
    "                             \n",
    "                             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i <= len(clusters):\n",
    "    i=i+1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-collectible",
   "metadata": {},
   "source": [
    "Ok so idea: get silhouette score for all clusterings and see if there's variation based on RAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
