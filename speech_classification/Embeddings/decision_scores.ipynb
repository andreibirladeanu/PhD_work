{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Classification/speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('classification.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.array(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=0)\n",
    "group_k_fold = GroupKFold(n_splits=6)\n",
    "splits = group_k_fold.split(X_shuffled, y_shuffled, groups_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "decision = []\n",
    "label = []\n",
    "predictions = []\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y_shuffled, groups_shuffled):\n",
    "    model.fit(X_shuffled[train_index], y_shuffled[train_index])\n",
    "    predictions.append(model.predict(X_shuffled[test_index]))\n",
    "    indices.append(groups_shuffled[test_index])\n",
    "    decision.append(model.decision_function(X_shuffled[test_index]))\n",
    "    label.append(y_shuffled[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_shuffled, y_shuffled, scoring='accuracy', cv=splits, groups = groups_shuffled, n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "decision_scores = list()\n",
    "while k<6:\n",
    "    for x, y,z,w in zip(indices[k], label[k], predictions[k], decision[k]):\n",
    "        decision_scores.append((x,y,z,w))\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(decision_scores, columns =['s_id', 'label', 'prediction', 'score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disagreement'] = df[['s_id','prediction']].duplicated(keep=False)\n",
    "df.sort_values(by='s_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([102, 73, 83, 44, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree = df.loc[df['disagreement']==False].sort_values(by='s_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement = [key for key in  dict(disagree.groupby('s_id').groups)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(disagreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the individual classification now\n",
    "os.chdir('/Users/andrei-macpro/Documents/Data/classification/speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('features.xlsx', sheet_name=\"Meal\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = np.array(data[\"Subject_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "splits = k_fold.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "labels = []\n",
    "subjects = []\n",
    "for train_indexes, test_indexes in splits:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        predictions.append(model.predict(X[test_indexes]))\n",
    "        labels.append(y[test_indexes])\n",
    "        subjects.append(subject_id[test_indexes])\n",
    "#meal = pd.DataFrame(list(zip(subjects, labels, predictions)), columns = ['subject_id', 'label', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = []\n",
    "k=0\n",
    "while k<5:\n",
    "    for x, y,z in zip(subjects[k], labels[k], predictions[k]):\n",
    "        meal.append((x,y,z))\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = pd.DataFrame(meal, columns =['subject_id', 'label', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shuffles = 10\n",
    "predictions = list()\n",
    "labels = []\n",
    "decisions = []\n",
    "indices = []\n",
    "for i in range(n_shuffles):\n",
    "    k_fold = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    split = k_fold.split(X,y)\n",
    "    for train_indexes, test_indexes in split:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        predictions.append(model.predict(X[test_indexes]))\n",
    "        labels.append(y[test_indexes])\n",
    "        indices.append(test_indexes)\n",
    "        decisions.append(model.decision_function(X[test_indexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_scores =[]\n",
    "k=0\n",
    "while k<50:\n",
    "    for x, y,z,w in zip(indices[k], labels[k], predictions[k], decisions[k]):\n",
    "        decision_scores.append((x,y,z,w))\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_meal = pd.DataFrame(decision_scores, columns =['s_id', 'label', 'prediction', 'decision_score'])\n",
    "decisions_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_meal = decisions_meal.sort_values(by='s_id')\n",
    "decisions_meal\n",
    "final_decisions = decisions_meal.groupby('s_id').agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_meal = decisions_meal.groupby('s_id').sum()\n",
    "predictions_meal = predictions_meal.set_index(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to check where there is a lot of variability \n",
    "missclassified_meal = [] # participants who tend to alternate between neg and pos in those 10 iterations \n",
    "for index, row in predictions_meal.iterrows():\n",
    "    if row['prediction'] >3 and row['prediction'] <8:\n",
    "        missclassified_meal.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions = final_decisions.drop(columns =['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy \n",
    "for index, row in final_decisions.iterrows():\n",
    "    if row['decision_score']>0 and row['label'] ==1:\n",
    "        accuracy +=1\n",
    "    elif row['decision_score']<0 and row['label'] ==0:\n",
    "        accuracy +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for index, row in final_decisions.iterrows():\n",
    "    if row['decision_score']>0:\n",
    "        prediction.append(1)\n",
    "    elif row['decision_score']<0:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions['predictions'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('features.xlsx', sheet_name=\"Play\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = np.array(data[\"Subject_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "splits = k_fold.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "labels = []\n",
    "subjects = []\n",
    "for train_indexes, test_indexes in splits:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        predictions.append(model.predict(X[test_indexes]))\n",
    "        labels.append(y[test_indexes])\n",
    "        subjects.append(subject_id[test_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = []\n",
    "k=0\n",
    "while k<5:\n",
    "    for x, y,z in zip(subjects[k], labels[k], predictions[k]):\n",
    "        play.append((x,y,z))\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = pd.DataFrame(play, columns =['subject_id', 'label', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shuffles = 10\n",
    "predictions = list()\n",
    "labels = []\n",
    "decisions = []\n",
    "indices = []\n",
    "for i in range(n_shuffles):\n",
    "    k_fold = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    split = k_fold.split(X,y)\n",
    "    for train_indexes, test_indexes in split:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        predictions.append(model.predict(X[test_indexes]))\n",
    "        labels.append(y[test_indexes])\n",
    "        indices.append(test_indexes)\n",
    "        decisions.append(model.decision_function(X[test_indexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_scores =[]\n",
    "k=0\n",
    "while k<50:\n",
    "    for x, y,z,w in zip(indices[k], labels[k], predictions[k], decisions[k]):\n",
    "        decision_scores.append((x,y,z,w))\n",
    "    k = k+1\n",
    "    \n",
    "decisions_play = pd.DataFrame(decision_scores, columns =['s_id', 'label', 'prediction', 'decision_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_play = decisions_play.sort_values(by='s_id')\n",
    "#final_decisions_play = decisions_play.groupby('s_id').agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_play = decisions_play.groupby('s_id').sum()\n",
    "predictions_play = predictions_play.set_index(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassified_play = [] # participants who tend to alternate between neg and pos in those 10 iterations \n",
    "# between 40 and 70% of the times tend to be misclassified \n",
    "for index, row in predictions_play.iterrows():\n",
    "    if row['prediction'] >3 and row['prediction'] <8:\n",
    "        missclassified_play.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassified_play, missclassified_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_play.loc[1134], predictions_meal.loc[1134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions_play = final_decisions_play.drop(columns =['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for index, row in final_decisions_play.iterrows():\n",
    "    if row['decision_score']>0 and row['label'] ==1:\n",
    "        accuracy +=1\n",
    "    elif row['decision_score']<0 and row['label'] ==0:\n",
    "        accuracy +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy/len(final_decisions_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for index, row in final_decisions_play.iterrows():\n",
    "    if row['decision_score']>0:\n",
    "        predictions.append(1)\n",
    "    elif row['decision_score']<0:\n",
    "        predictions.append(0)\n",
    "final_decisions_play['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions_play  = final_decisions_play.set_index(data['Subject_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions_play= final_decisions_play.rename(columns={\"label\":\"label_play\", \"decision_score\":\"decision score play\",\"predictions\":\"predictions play\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions_meal = final_decisions.set_index(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decisions_meal= final_decisions_meal.rename(columns={\"label\":\"label_meal\", \"decision_score\":\"decision score meal\",\"predictions\":\"predictions meal\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions = pd.concat([final_decisions_play, final_decisions_meal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions['agreement'] = all_decisions[['predictions play','predictions meal']].duplicated(keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = []\n",
    "for index, row in all_decisions.iterrows():\n",
    "    if abs(row['decision score play']) > abs(row['decision score meal']):\n",
    "        final_pred.append(row['predictions play'])\n",
    "    else:\n",
    "        final_pred.append(row['predictions meal'])\n",
    "all_decisions['final pred'] = final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_agreement = 0\n",
    "for index, row in all_decisions.iterrows():\n",
    "    if row['final pred'] == row['label_meal']:\n",
    "        final_agreement +=1\n",
    "final_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of people who the classifier finds hard to put label on\n",
    "import itertools\n",
    "nest = [disagreement,missclassified_play, missclassified_meal]\n",
    "confusing = pd.DataFrame((_ for _ in itertools.zip_longest(*nest)), columns=['confused_general', 'confused_play', 'confused_meal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = pd.unique(confusing[['confused_general', 'confused_play', 'confused_meal']].values.ravel('K'))\n",
    "len(unique)\n",
    "# 38 people are unique out of the confusing ones (so they only appear in one column) \n",
    "# 44 in total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusing # need to get some descriptive statistics on these \n",
    "# look at the predictions for these for both play and meal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions.loc[missclassified_play]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions.loc[missclassified_meal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's combine the classifiers based on errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = meal.set_index(meal['subject_id'])\n",
    "play = play.set_index(play['subject_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = meal.rename(columns = {'prediction' :'prediction_meal'})\n",
    "play = play.rename(columns = {'prediction' :'prediction_play'}).drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([meal, play], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop('subject_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop([1049, 1089, 1250, 1096, 1195])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = []\n",
    "for index, row, in combined.iterrows():\n",
    "    if row['prediction_meal'] ==1 and row['prediction_play'] ==0:\n",
    "        final_pred.append(0)\n",
    "    elif row['prediction_meal'] ==0 and row['prediction_play'] ==1:\n",
    "        final_pred.append(0)\n",
    "    elif row['prediction_meal'] ==0 and row['prediction_play'] ==0:\n",
    "        final_pred.append(0)\n",
    "    elif row['prediction_meal'] ==1 and row['prediction_play'] ==1:\n",
    "        final_pred.append(1)\n",
    "combined['final_pred'] = final_pred\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for index, row, in combined.iterrows():\n",
    "    if row['label'] == row['final_pred']:\n",
    "        accuracy += 1\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy/len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('features.xlsx', sheet_name=\"Meal\", engine='openpyxl')\n",
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])\n",
    "X = scaler.fit_transform(X,y)\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "splits = k_fold.split(X,y)\n",
    "subject_id = np.array(data[\"Subject_ID\"])\n",
    "predictions = list()\n",
    "labels = []\n",
    "subjects = []\n",
    "for train_indexes, test_indexes in splits:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        predictions.append(model.predict(X[test_indexes]))\n",
    "        labels.append(y[test_indexes])\n",
    "        subjects.append(subject_id[test_indexes])\n",
    "        \n",
    "meal = []\n",
    "k=0\n",
    "while k<5:\n",
    "    for x, y,z in zip(subjects[k], labels[k], predictions[k]):\n",
    "        meal.append((x,y,z))\n",
    "    k = k+1\n",
    "    \n",
    "    \n",
    "meal = pd.DataFrame(meal, columns =['subject_id', 'label', 'prediction'])\n",
    "meal = meal.set_index(meal['subject_id']).rename(columns = {'prediction' :'prediction_meal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('features.xlsx', sheet_name=\"Play\", engine='openpyxl')\n",
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])\n",
    "X = scaler.fit_transform(X,y)\n",
    "subject_id = np.array(data[\"Subject_ID\"])\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "splits = k_fold.split(X,y)\n",
    "\n",
    "predictions = list()\n",
    "labels = []\n",
    "subjects = []\n",
    "for train_indexes, test_indexes in splits:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        predictions.append(model.predict(X[test_indexes]))\n",
    "        labels.append(y[test_indexes])\n",
    "        subjects.append(subject_id[test_indexes])\n",
    "play = []\n",
    "k=0\n",
    "while k<5:\n",
    "    for x, y,z in zip(subjects[k], labels[k], predictions[k]):\n",
    "        play.append((x,y,z))\n",
    "    k = k+1\n",
    "    \n",
    "    \n",
    "play = pd.DataFrame(play, columns =['subject_id', 'label', 'prediction'])\n",
    "play = play.set_index(play['subject_id']).rename(columns = {'prediction' :'prediction_play'}).drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([meal, play], axis=1)\n",
    "combined = combined.drop([1049, 1089, 1250, 1096, 1195], axis=0)\n",
    "combined = combined.drop('subject_id', axis=1)\n",
    "final_pred = []\n",
    "for index, row, in combined.iterrows():\n",
    "    if row['prediction_meal'] ==1 and row['prediction_play'] ==0:\n",
    "        final_pred.append(0)\n",
    "    elif row['prediction_meal'] ==0 and row['prediction_play'] ==1:\n",
    "        final_pred.append(0)\n",
    "    elif row['prediction_meal'] ==0 and row['prediction_play'] ==0:\n",
    "        final_pred.append(0)\n",
    "    elif row['prediction_meal'] ==1 and row['prediction_play'] ==1:\n",
    "        final_pred.append(1)\n",
    "   # elif row['prediction_meal'] ==1 and row['prediction_play'] =='NaN' and row['label']==1:\n",
    "    #    final_pred.append(1)\n",
    "    #elif row['prediction_meal'] ==0 and row['prediction_play'] =='NaN' and row['label']==0:\n",
    "     #   final_pred.append(1)\n",
    "   # elif row['prediction_meal'] =='NaN' and row['prediction_play'] ==1 and row['label']==1:\n",
    "    #    final_pred.append(1)\n",
    "   # elif row['prediction_meal'] =='NaN' and row['prediction_play'] ==0 and row['label']==0:\n",
    "    #    final_pred.append(1)\n",
    "   # else:\n",
    "    #    final_pred.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "combined['final_pred'] = final_pred\n",
    "\n",
    "accuracy = []\n",
    "for index, row, in combined.iterrows():\n",
    "    if row['label'] == row['final_pred']:\n",
    "        accuracy.append(1)\n",
    "sum(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(accuracy)/len(combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [62.5, 64.2, 58.9, 62.5,58.9, 58.9, 62.5,58.9, 60.7,62.5]\n",
    "mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore who tends to get the most errors in final pred \n",
    "errors = []\n",
    "for index, row in combined.iterrows():\n",
    "    if row['label'] != row['final_pred']:\n",
    "        errors.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.array(errors)\n",
    "errors = errors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_unique = np.array(errors)\n",
    "errors_unique = np.unique(errors_unique)\n",
    "errors_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_errors = []\n",
    "for error in errors_unique:\n",
    "    no_errors.append(errors.count(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass = pd.DataFrame(list(zip(errors_unique, no_errors)), columns = ['s_id', 'misclassifications'])\n",
    "\n",
    "misclass = misclass.loc[misclass['misclassifications'] > 4] # more than half the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
