{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d96aac8-9f0e-43c8-bb8e-9a20c79952e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tf_pose import common\n",
    "import tf_slim as slim\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad9359c-d557-4585-9e1c-0f867d9c8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='cmu'\n",
    "resize='480x432'\n",
    "w, h = model_wh(resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb26ded-ebb0-441e-8744-d3a32b133819",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-06 18:31:58,486] [TfPoseEstimator] [INFO] loading graph from /Users/andreibirladeanu/opt/anaconda3/envs/openpose/lib/python3.7/site-packages/tf_pose_data/graph/cmu/graph_opt.pb(default size=480x432)\n",
      "2022-06-06 18:31:58,486 INFO loading graph from /Users/andreibirladeanu/opt/anaconda3/envs/openpose/lib/python3.7/site-packages/tf_pose_data/graph/cmu/graph_opt.pb(default size=480x432)\n",
      "2022-06-06 18:31:59.938925: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfPoseEstimator/Openpose/concat_stage7/axis\n",
      "TfPoseEstimator/Mconv7_stage6_L2/biases\n",
      "TfPoseEstimator/Mconv7_stage6_L2/weights\n",
      "TfPoseEstimator/Mconv6_stage6_L2/biases\n",
      "TfPoseEstimator/Mconv6_stage6_L2/weights\n",
      "TfPoseEstimator/Mconv5_stage6_L2/biases\n",
      "TfPoseEstimator/Mconv5_stage6_L2/weights\n",
      "TfPoseEstimator/Mconv4_stage6_L2/biases\n",
      "TfPoseEstimator/Mconv4_stage6_L2/weights\n",
      "TfPoseEstimator/Mconv3_stage6_L2/biases\n",
      "TfPoseEstimator/Mconv3_stage6_L2/weights\n",
      "TfPoseEstimator/Mconv2_stage6_L2/biases\n",
      "TfPoseEstimator/Mconv2_stage6_L2/weights\n",
      "TfPoseEstimator/Mconv1_stage6_L2/biases\n",
      "TfPoseEstimator/Mconv1_stage6_L2/weights\n",
      "TfPoseEstimator/Mconv7_stage6_L1/biases\n",
      "TfPoseEstimator/Mconv7_stage6_L1/weights\n",
      "TfPoseEstimator/Mconv6_stage6_L1/biases\n",
      "TfPoseEstimator/Mconv6_stage6_L1/weights\n",
      "TfPoseEstimator/Mconv5_stage6_L1/biases\n",
      "TfPoseEstimator/Mconv5_stage6_L1/weights\n",
      "TfPoseEstimator/Mconv4_stage6_L1/biases\n",
      "TfPoseEstimator/Mconv4_stage6_L1/weights\n",
      "TfPoseEstimator/Mconv3_stage6_L1/biases\n",
      "TfPoseEstimator/Mconv3_stage6_L1/weights\n",
      "TfPoseEstimator/Mconv2_stage6_L1/biases\n",
      "TfPoseEstimator/Mconv2_stage6_L1/weights\n",
      "TfPoseEstimator/Mconv1_stage6_L1/biases\n",
      "TfPoseEstimator/Mconv1_stage6_L1/weights\n",
      "TfPoseEstimator/concat_stage6/axis\n",
      "TfPoseEstimator/Mconv7_stage5_L2/biases\n",
      "TfPoseEstimator/Mconv7_stage5_L2/weights\n",
      "TfPoseEstimator/Mconv6_stage5_L2/biases\n",
      "TfPoseEstimator/Mconv6_stage5_L2/weights\n",
      "TfPoseEstimator/Mconv5_stage5_L2/biases\n",
      "TfPoseEstimator/Mconv5_stage5_L2/weights\n",
      "TfPoseEstimator/Mconv4_stage5_L2/biases\n",
      "TfPoseEstimator/Mconv4_stage5_L2/weights\n",
      "TfPoseEstimator/Mconv3_stage5_L2/biases\n",
      "TfPoseEstimator/Mconv3_stage5_L2/weights\n",
      "TfPoseEstimator/Mconv2_stage5_L2/biases\n",
      "TfPoseEstimator/Mconv2_stage5_L2/weights\n",
      "TfPoseEstimator/Mconv1_stage5_L2/biases\n",
      "TfPoseEstimator/Mconv1_stage5_L2/weights\n",
      "TfPoseEstimator/Mconv7_stage5_L1/biases\n",
      "TfPoseEstimator/Mconv7_stage5_L1/weights\n",
      "TfPoseEstimator/Mconv6_stage5_L1/biases\n",
      "TfPoseEstimator/Mconv6_stage5_L1/weights\n",
      "TfPoseEstimator/Mconv5_stage5_L1/biases\n",
      "TfPoseEstimator/Mconv5_stage5_L1/weights\n",
      "TfPoseEstimator/Mconv4_stage5_L1/biases\n",
      "TfPoseEstimator/Mconv4_stage5_L1/weights\n",
      "TfPoseEstimator/Mconv3_stage5_L1/biases\n",
      "TfPoseEstimator/Mconv3_stage5_L1/weights\n",
      "TfPoseEstimator/Mconv2_stage5_L1/biases\n",
      "TfPoseEstimator/Mconv2_stage5_L1/weights\n",
      "TfPoseEstimator/Mconv1_stage5_L1/biases\n",
      "TfPoseEstimator/Mconv1_stage5_L1/weights\n",
      "TfPoseEstimator/concat_stage5/axis\n",
      "TfPoseEstimator/Mconv7_stage4_L2/biases\n",
      "TfPoseEstimator/Mconv7_stage4_L2/weights\n",
      "TfPoseEstimator/Mconv6_stage4_L2/biases\n",
      "TfPoseEstimator/Mconv6_stage4_L2/weights\n",
      "TfPoseEstimator/Mconv5_stage4_L2/biases\n",
      "TfPoseEstimator/Mconv5_stage4_L2/weights\n",
      "TfPoseEstimator/Mconv4_stage4_L2/biases\n",
      "TfPoseEstimator/Mconv4_stage4_L2/weights\n",
      "TfPoseEstimator/Mconv3_stage4_L2/biases\n",
      "TfPoseEstimator/Mconv3_stage4_L2/weights\n",
      "TfPoseEstimator/Mconv2_stage4_L2/biases\n",
      "TfPoseEstimator/Mconv2_stage4_L2/weights\n",
      "TfPoseEstimator/Mconv1_stage4_L2/biases\n",
      "TfPoseEstimator/Mconv1_stage4_L2/weights\n",
      "TfPoseEstimator/Mconv7_stage4_L1/biases\n",
      "TfPoseEstimator/Mconv7_stage4_L1/weights\n",
      "TfPoseEstimator/Mconv6_stage4_L1/biases\n",
      "TfPoseEstimator/Mconv6_stage4_L1/weights\n",
      "TfPoseEstimator/Mconv5_stage4_L1/biases\n",
      "TfPoseEstimator/Mconv5_stage4_L1/weights\n",
      "TfPoseEstimator/Mconv4_stage4_L1/biases\n",
      "TfPoseEstimator/Mconv4_stage4_L1/weights\n",
      "TfPoseEstimator/Mconv3_stage4_L1/biases\n",
      "TfPoseEstimator/Mconv3_stage4_L1/weights\n",
      "TfPoseEstimator/Mconv2_stage4_L1/biases\n",
      "TfPoseEstimator/Mconv2_stage4_L1/weights\n",
      "TfPoseEstimator/Mconv1_stage4_L1/biases\n",
      "TfPoseEstimator/Mconv1_stage4_L1/weights\n",
      "TfPoseEstimator/concat_stage4/axis\n",
      "TfPoseEstimator/Mconv7_stage3_L2/biases\n",
      "TfPoseEstimator/Mconv7_stage3_L2/weights\n",
      "TfPoseEstimator/Mconv6_stage3_L2/biases\n",
      "TfPoseEstimator/Mconv6_stage3_L2/weights\n",
      "TfPoseEstimator/Mconv5_stage3_L2/biases\n",
      "TfPoseEstimator/Mconv5_stage3_L2/weights\n",
      "TfPoseEstimator/Mconv4_stage3_L2/biases\n",
      "TfPoseEstimator/Mconv4_stage3_L2/weights\n",
      "TfPoseEstimator/Mconv3_stage3_L2/biases\n",
      "TfPoseEstimator/Mconv3_stage3_L2/weights\n",
      "TfPoseEstimator/Mconv2_stage3_L2/biases\n",
      "TfPoseEstimator/Mconv2_stage3_L2/weights\n",
      "TfPoseEstimator/Mconv1_stage3_L2/biases\n",
      "TfPoseEstimator/Mconv1_stage3_L2/weights\n",
      "TfPoseEstimator/Mconv7_stage3_L1/biases\n",
      "TfPoseEstimator/Mconv7_stage3_L1/weights\n",
      "TfPoseEstimator/Mconv6_stage3_L1/biases\n",
      "TfPoseEstimator/Mconv6_stage3_L1/weights\n",
      "TfPoseEstimator/Mconv5_stage3_L1/biases\n",
      "TfPoseEstimator/Mconv5_stage3_L1/weights\n",
      "TfPoseEstimator/Mconv4_stage3_L1/biases\n",
      "TfPoseEstimator/Mconv4_stage3_L1/weights\n",
      "TfPoseEstimator/Mconv3_stage3_L1/biases\n",
      "TfPoseEstimator/Mconv3_stage3_L1/weights\n",
      "TfPoseEstimator/Mconv2_stage3_L1/biases\n",
      "TfPoseEstimator/Mconv2_stage3_L1/weights\n",
      "TfPoseEstimator/Mconv1_stage3_L1/biases\n",
      "TfPoseEstimator/Mconv1_stage3_L1/weights\n",
      "TfPoseEstimator/concat_stage3/axis\n",
      "TfPoseEstimator/Mconv7_stage2_L2/biases\n",
      "TfPoseEstimator/Mconv7_stage2_L2/weights\n",
      "TfPoseEstimator/Mconv6_stage2_L2/biases\n",
      "TfPoseEstimator/Mconv6_stage2_L2/weights\n",
      "TfPoseEstimator/Mconv5_stage2_L2/biases\n",
      "TfPoseEstimator/Mconv5_stage2_L2/weights\n",
      "TfPoseEstimator/Mconv4_stage2_L2/biases\n",
      "TfPoseEstimator/Mconv4_stage2_L2/weights\n",
      "TfPoseEstimator/Mconv3_stage2_L2/biases\n",
      "TfPoseEstimator/Mconv3_stage2_L2/weights\n",
      "TfPoseEstimator/Mconv2_stage2_L2/biases\n",
      "TfPoseEstimator/Mconv2_stage2_L2/weights\n",
      "TfPoseEstimator/Mconv1_stage2_L2/biases\n",
      "TfPoseEstimator/Mconv1_stage2_L2/weights\n",
      "TfPoseEstimator/Mconv7_stage2_L1/biases\n",
      "TfPoseEstimator/Mconv7_stage2_L1/weights\n",
      "TfPoseEstimator/Mconv6_stage2_L1/biases\n",
      "TfPoseEstimator/Mconv6_stage2_L1/weights\n",
      "TfPoseEstimator/Mconv5_stage2_L1/biases\n",
      "TfPoseEstimator/Mconv5_stage2_L1/weights\n",
      "TfPoseEstimator/Mconv4_stage2_L1/biases\n",
      "TfPoseEstimator/Mconv4_stage2_L1/weights\n",
      "TfPoseEstimator/Mconv3_stage2_L1/biases\n",
      "TfPoseEstimator/Mconv3_stage2_L1/weights\n",
      "TfPoseEstimator/Mconv2_stage2_L1/biases\n",
      "TfPoseEstimator/Mconv2_stage2_L1/weights\n",
      "TfPoseEstimator/Mconv1_stage2_L1/biases\n",
      "TfPoseEstimator/Mconv1_stage2_L1/weights\n",
      "TfPoseEstimator/concat_stage2/axis\n",
      "TfPoseEstimator/conv5_5_CPM_L2/biases\n",
      "TfPoseEstimator/conv5_5_CPM_L2/weights\n",
      "TfPoseEstimator/conv5_4_CPM_L2/biases\n",
      "TfPoseEstimator/conv5_4_CPM_L2/weights\n",
      "TfPoseEstimator/conv5_3_CPM_L2/biases\n",
      "TfPoseEstimator/conv5_3_CPM_L2/weights\n",
      "TfPoseEstimator/conv5_2_CPM_L2/biases\n",
      "TfPoseEstimator/conv5_2_CPM_L2/weights\n",
      "TfPoseEstimator/conv5_1_CPM_L2/biases\n",
      "TfPoseEstimator/conv5_1_CPM_L2/weights\n",
      "TfPoseEstimator/conv5_5_CPM_L1/biases\n",
      "TfPoseEstimator/conv5_5_CPM_L1/weights\n",
      "TfPoseEstimator/conv5_4_CPM_L1/biases\n",
      "TfPoseEstimator/conv5_4_CPM_L1/weights\n",
      "TfPoseEstimator/conv5_3_CPM_L1/biases\n",
      "TfPoseEstimator/conv5_3_CPM_L1/weights\n",
      "TfPoseEstimator/conv5_2_CPM_L1/biases\n",
      "TfPoseEstimator/conv5_2_CPM_L1/weights\n",
      "TfPoseEstimator/conv5_1_CPM_L1/biases\n",
      "TfPoseEstimator/conv5_1_CPM_L1/weights\n",
      "TfPoseEstimator/conv4_4_CPM/biases\n",
      "TfPoseEstimator/conv4_4_CPM/weights\n",
      "TfPoseEstimator/conv4_3_CPM/biases\n",
      "TfPoseEstimator/conv4_3_CPM/weights\n",
      "TfPoseEstimator/conv4_2/biases\n",
      "TfPoseEstimator/conv4_2/weights\n",
      "TfPoseEstimator/conv4_1/biases\n",
      "TfPoseEstimator/conv4_1/weights\n",
      "TfPoseEstimator/conv3_4/biases\n",
      "TfPoseEstimator/conv3_4/weights\n",
      "TfPoseEstimator/conv3_3/biases\n",
      "TfPoseEstimator/conv3_3/weights\n",
      "TfPoseEstimator/conv3_2/biases\n",
      "TfPoseEstimator/conv3_2/weights\n",
      "TfPoseEstimator/conv3_1/biases\n",
      "TfPoseEstimator/conv3_1/weights\n",
      "TfPoseEstimator/conv2_2/biases\n",
      "TfPoseEstimator/conv2_2/weights\n",
      "TfPoseEstimator/conv2_1/biases\n",
      "TfPoseEstimator/conv2_1/weights\n",
      "TfPoseEstimator/conv1_2/biases\n",
      "TfPoseEstimator/conv1_2/weights\n",
      "TfPoseEstimator/conv1_1/biases\n",
      "TfPoseEstimator/conv1_1/weights\n",
      "TfPoseEstimator/preprocess_subtract/y\n",
      "TfPoseEstimator/preprocess_divide/y\n",
      "TfPoseEstimator/image\n",
      "TfPoseEstimator/preprocess_divide\n",
      "TfPoseEstimator/preprocess_subtract\n",
      "TfPoseEstimator/conv1_1/Conv2D\n",
      "TfPoseEstimator/conv1_1/BiasAdd\n",
      "TfPoseEstimator/conv1_1/conv1_1\n",
      "TfPoseEstimator/conv1_2/Conv2D\n",
      "TfPoseEstimator/conv1_2/BiasAdd\n",
      "TfPoseEstimator/conv1_2/conv1_2\n",
      "TfPoseEstimator/pool1_stage1\n",
      "TfPoseEstimator/conv2_1/Conv2D\n",
      "TfPoseEstimator/conv2_1/BiasAdd\n",
      "TfPoseEstimator/conv2_1/conv2_1\n",
      "TfPoseEstimator/conv2_2/Conv2D\n",
      "TfPoseEstimator/conv2_2/BiasAdd\n",
      "TfPoseEstimator/conv2_2/conv2_2\n",
      "TfPoseEstimator/pool2_stage1\n",
      "TfPoseEstimator/conv3_1/Conv2D\n",
      "TfPoseEstimator/conv3_1/BiasAdd\n",
      "TfPoseEstimator/conv3_1/conv3_1\n",
      "TfPoseEstimator/conv3_2/Conv2D\n",
      "TfPoseEstimator/conv3_2/BiasAdd\n",
      "TfPoseEstimator/conv3_2/conv3_2\n",
      "TfPoseEstimator/conv3_3/Conv2D\n",
      "TfPoseEstimator/conv3_3/BiasAdd\n",
      "TfPoseEstimator/conv3_3/conv3_3\n",
      "TfPoseEstimator/conv3_4/Conv2D\n",
      "TfPoseEstimator/conv3_4/BiasAdd\n",
      "TfPoseEstimator/conv3_4/conv3_4\n",
      "TfPoseEstimator/pool3_stage1\n",
      "TfPoseEstimator/conv4_1/Conv2D\n",
      "TfPoseEstimator/conv4_1/BiasAdd\n",
      "TfPoseEstimator/conv4_1/conv4_1\n",
      "TfPoseEstimator/conv4_2/Conv2D\n",
      "TfPoseEstimator/conv4_2/BiasAdd\n",
      "TfPoseEstimator/conv4_2/conv4_2\n",
      "TfPoseEstimator/conv4_3_CPM/Conv2D\n",
      "TfPoseEstimator/conv4_3_CPM/BiasAdd\n",
      "TfPoseEstimator/conv4_3_CPM/conv4_3_CPM\n",
      "TfPoseEstimator/conv4_4_CPM/Conv2D\n",
      "TfPoseEstimator/conv4_4_CPM/BiasAdd\n",
      "TfPoseEstimator/conv4_4_CPM/conv4_4_CPM\n",
      "TfPoseEstimator/conv5_1_CPM_L2/Conv2D\n",
      "TfPoseEstimator/conv5_1_CPM_L2/BiasAdd\n",
      "TfPoseEstimator/conv5_1_CPM_L2/conv5_1_CPM_L2\n",
      "TfPoseEstimator/conv5_2_CPM_L2/Conv2D\n",
      "TfPoseEstimator/conv5_2_CPM_L2/BiasAdd\n",
      "TfPoseEstimator/conv5_2_CPM_L2/conv5_2_CPM_L2\n",
      "TfPoseEstimator/conv5_3_CPM_L2/Conv2D\n",
      "TfPoseEstimator/conv5_3_CPM_L2/BiasAdd\n",
      "TfPoseEstimator/conv5_3_CPM_L2/conv5_3_CPM_L2\n",
      "TfPoseEstimator/conv5_4_CPM_L2/Conv2D\n",
      "TfPoseEstimator/conv5_4_CPM_L2/BiasAdd\n",
      "TfPoseEstimator/conv5_4_CPM_L2/conv5_4_CPM_L2\n",
      "TfPoseEstimator/conv5_5_CPM_L2/Conv2D\n",
      "TfPoseEstimator/conv5_5_CPM_L2/BiasAdd\n",
      "TfPoseEstimator/conv5_1_CPM_L1/Conv2D\n",
      "TfPoseEstimator/conv5_1_CPM_L1/BiasAdd\n",
      "TfPoseEstimator/conv5_1_CPM_L1/conv5_1_CPM_L1\n",
      "TfPoseEstimator/conv5_2_CPM_L1/Conv2D\n",
      "TfPoseEstimator/conv5_2_CPM_L1/BiasAdd\n",
      "TfPoseEstimator/conv5_2_CPM_L1/conv5_2_CPM_L1\n",
      "TfPoseEstimator/conv5_3_CPM_L1/Conv2D\n",
      "TfPoseEstimator/conv5_3_CPM_L1/BiasAdd\n",
      "TfPoseEstimator/conv5_3_CPM_L1/conv5_3_CPM_L1\n",
      "TfPoseEstimator/conv5_4_CPM_L1/Conv2D\n",
      "TfPoseEstimator/conv5_4_CPM_L1/BiasAdd\n",
      "TfPoseEstimator/conv5_4_CPM_L1/conv5_4_CPM_L1\n",
      "TfPoseEstimator/conv5_5_CPM_L1/Conv2D\n",
      "TfPoseEstimator/conv5_5_CPM_L1/BiasAdd\n",
      "TfPoseEstimator/concat_stage2\n",
      "TfPoseEstimator/Mconv1_stage2_L2/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage2_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage2_L2/Mconv1_stage2_L2\n",
      "TfPoseEstimator/Mconv2_stage2_L2/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage2_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage2_L2/Mconv2_stage2_L2\n",
      "TfPoseEstimator/Mconv3_stage2_L2/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage2_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage2_L2/Mconv3_stage2_L2\n",
      "TfPoseEstimator/Mconv4_stage2_L2/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage2_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage2_L2/Mconv4_stage2_L2\n",
      "TfPoseEstimator/Mconv5_stage2_L2/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage2_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage2_L2/Mconv5_stage2_L2\n",
      "TfPoseEstimator/Mconv6_stage2_L2/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage2_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage2_L2/Mconv6_stage2_L2\n",
      "TfPoseEstimator/Mconv7_stage2_L2/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage2_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage2_L1/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage2_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage2_L1/Mconv1_stage2_L1\n",
      "TfPoseEstimator/Mconv2_stage2_L1/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage2_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage2_L1/Mconv2_stage2_L1\n",
      "TfPoseEstimator/Mconv3_stage2_L1/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage2_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage2_L1/Mconv3_stage2_L1\n",
      "TfPoseEstimator/Mconv4_stage2_L1/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage2_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage2_L1/Mconv4_stage2_L1\n",
      "TfPoseEstimator/Mconv5_stage2_L1/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage2_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage2_L1/Mconv5_stage2_L1\n",
      "TfPoseEstimator/Mconv6_stage2_L1/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage2_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage2_L1/Mconv6_stage2_L1\n",
      "TfPoseEstimator/Mconv7_stage2_L1/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage2_L1/BiasAdd\n",
      "TfPoseEstimator/concat_stage3\n",
      "TfPoseEstimator/Mconv1_stage3_L2/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage3_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage3_L2/Mconv1_stage3_L2\n",
      "TfPoseEstimator/Mconv2_stage3_L2/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage3_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage3_L2/Mconv2_stage3_L2\n",
      "TfPoseEstimator/Mconv3_stage3_L2/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage3_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage3_L2/Mconv3_stage3_L2\n",
      "TfPoseEstimator/Mconv4_stage3_L2/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage3_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage3_L2/Mconv4_stage3_L2\n",
      "TfPoseEstimator/Mconv5_stage3_L2/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage3_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage3_L2/Mconv5_stage3_L2\n",
      "TfPoseEstimator/Mconv6_stage3_L2/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage3_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage3_L2/Mconv6_stage3_L2\n",
      "TfPoseEstimator/Mconv7_stage3_L2/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage3_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage3_L1/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage3_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage3_L1/Mconv1_stage3_L1\n",
      "TfPoseEstimator/Mconv2_stage3_L1/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage3_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage3_L1/Mconv2_stage3_L1\n",
      "TfPoseEstimator/Mconv3_stage3_L1/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage3_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage3_L1/Mconv3_stage3_L1\n",
      "TfPoseEstimator/Mconv4_stage3_L1/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage3_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage3_L1/Mconv4_stage3_L1\n",
      "TfPoseEstimator/Mconv5_stage3_L1/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage3_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage3_L1/Mconv5_stage3_L1\n",
      "TfPoseEstimator/Mconv6_stage3_L1/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage3_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage3_L1/Mconv6_stage3_L1\n",
      "TfPoseEstimator/Mconv7_stage3_L1/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage3_L1/BiasAdd\n",
      "TfPoseEstimator/concat_stage4\n",
      "TfPoseEstimator/Mconv1_stage4_L2/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage4_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage4_L2/Mconv1_stage4_L2\n",
      "TfPoseEstimator/Mconv2_stage4_L2/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage4_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage4_L2/Mconv2_stage4_L2\n",
      "TfPoseEstimator/Mconv3_stage4_L2/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage4_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage4_L2/Mconv3_stage4_L2\n",
      "TfPoseEstimator/Mconv4_stage4_L2/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage4_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage4_L2/Mconv4_stage4_L2\n",
      "TfPoseEstimator/Mconv5_stage4_L2/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage4_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage4_L2/Mconv5_stage4_L2\n",
      "TfPoseEstimator/Mconv6_stage4_L2/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage4_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage4_L2/Mconv6_stage4_L2\n",
      "TfPoseEstimator/Mconv7_stage4_L2/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage4_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage4_L1/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage4_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage4_L1/Mconv1_stage4_L1\n",
      "TfPoseEstimator/Mconv2_stage4_L1/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage4_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage4_L1/Mconv2_stage4_L1\n",
      "TfPoseEstimator/Mconv3_stage4_L1/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage4_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage4_L1/Mconv3_stage4_L1\n",
      "TfPoseEstimator/Mconv4_stage4_L1/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage4_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage4_L1/Mconv4_stage4_L1\n",
      "TfPoseEstimator/Mconv5_stage4_L1/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage4_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage4_L1/Mconv5_stage4_L1\n",
      "TfPoseEstimator/Mconv6_stage4_L1/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage4_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage4_L1/Mconv6_stage4_L1\n",
      "TfPoseEstimator/Mconv7_stage4_L1/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage4_L1/BiasAdd\n",
      "TfPoseEstimator/concat_stage5\n",
      "TfPoseEstimator/Mconv1_stage5_L2/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage5_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage5_L2/Mconv1_stage5_L2\n",
      "TfPoseEstimator/Mconv2_stage5_L2/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage5_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage5_L2/Mconv2_stage5_L2\n",
      "TfPoseEstimator/Mconv3_stage5_L2/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage5_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage5_L2/Mconv3_stage5_L2\n",
      "TfPoseEstimator/Mconv4_stage5_L2/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage5_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage5_L2/Mconv4_stage5_L2\n",
      "TfPoseEstimator/Mconv5_stage5_L2/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage5_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage5_L2/Mconv5_stage5_L2\n",
      "TfPoseEstimator/Mconv6_stage5_L2/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage5_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage5_L2/Mconv6_stage5_L2\n",
      "TfPoseEstimator/Mconv7_stage5_L2/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage5_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage5_L1/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage5_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage5_L1/Mconv1_stage5_L1\n",
      "TfPoseEstimator/Mconv2_stage5_L1/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage5_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage5_L1/Mconv2_stage5_L1\n",
      "TfPoseEstimator/Mconv3_stage5_L1/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage5_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage5_L1/Mconv3_stage5_L1\n",
      "TfPoseEstimator/Mconv4_stage5_L1/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage5_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage5_L1/Mconv4_stage5_L1\n",
      "TfPoseEstimator/Mconv5_stage5_L1/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage5_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage5_L1/Mconv5_stage5_L1\n",
      "TfPoseEstimator/Mconv6_stage5_L1/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage5_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage5_L1/Mconv6_stage5_L1\n",
      "TfPoseEstimator/Mconv7_stage5_L1/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage5_L1/BiasAdd\n",
      "TfPoseEstimator/concat_stage6\n",
      "TfPoseEstimator/Mconv1_stage6_L2/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage6_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage6_L2/Mconv1_stage6_L2\n",
      "TfPoseEstimator/Mconv2_stage6_L2/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage6_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage6_L2/Mconv2_stage6_L2\n",
      "TfPoseEstimator/Mconv3_stage6_L2/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage6_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage6_L2/Mconv3_stage6_L2\n",
      "TfPoseEstimator/Mconv4_stage6_L2/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage6_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage6_L2/Mconv4_stage6_L2\n",
      "TfPoseEstimator/Mconv5_stage6_L2/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage6_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage6_L2/Mconv5_stage6_L2\n",
      "TfPoseEstimator/Mconv6_stage6_L2/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage6_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage6_L2/Mconv6_stage6_L2\n",
      "TfPoseEstimator/Mconv7_stage6_L2/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage6_L2/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage6_L1/Conv2D\n",
      "TfPoseEstimator/Mconv1_stage6_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv1_stage6_L1/Mconv1_stage6_L1\n",
      "TfPoseEstimator/Mconv2_stage6_L1/Conv2D\n",
      "TfPoseEstimator/Mconv2_stage6_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv2_stage6_L1/Mconv2_stage6_L1\n",
      "TfPoseEstimator/Mconv3_stage6_L1/Conv2D\n",
      "TfPoseEstimator/Mconv3_stage6_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv3_stage6_L1/Mconv3_stage6_L1\n",
      "TfPoseEstimator/Mconv4_stage6_L1/Conv2D\n",
      "TfPoseEstimator/Mconv4_stage6_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv4_stage6_L1/Mconv4_stage6_L1\n",
      "TfPoseEstimator/Mconv5_stage6_L1/Conv2D\n",
      "TfPoseEstimator/Mconv5_stage6_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv5_stage6_L1/Mconv5_stage6_L1\n",
      "TfPoseEstimator/Mconv6_stage6_L1/Conv2D\n",
      "TfPoseEstimator/Mconv6_stage6_L1/BiasAdd\n",
      "TfPoseEstimator/Mconv6_stage6_L1/Mconv6_stage6_L1\n",
      "TfPoseEstimator/Mconv7_stage6_L1/Conv2D\n",
      "TfPoseEstimator/Mconv7_stage6_L1/BiasAdd\n",
      "TfPoseEstimator/Openpose/concat_stage7\n"
     ]
    }
   ],
   "source": [
    "e = TfPoseEstimator(get_graph_path(model), target_size=(w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a04763-03a5-404c-96ce-faab2e04ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/Users/andreibirladeanu/Documents/Data/meal_videos/1117_meal.mp4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cb1f70-a827-41fa-8ca5-e2c33dbc9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "fps_time = 0\n",
    "while True:\n",
    "    ret_val, image = cap1qqq.read()\n",
    "    humans = e.inference(image,\n",
    "                         resize_to_default=(w > 0 and h > 0),\n",
    "                         upsample_size=4.0)\n",
    "\n",
    "    #image = np.zeros(image.shape)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    cv2.putText(image, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.imshow('tf-pose-estimation result', image)\n",
    "    fps_time = time.time()\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fc4227-726e-429b-b02b-51949e504664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract(human):\n",
    "    \"\"\"takes a human openpose object and returns parsed skeleton coordinates and probabilities \"\"\"\n",
    "    all_points = [x for x in range(18)]\n",
    "    skeleton =[]\n",
    "    probs = []\n",
    "    kpts = []\n",
    "    coords_x= []\n",
    "    coords_y= []\n",
    "    subscriptable = str(human).split(\"BodyPart:\")[1:]\n",
    "    for x in subscriptable:\n",
    "        kpts.append(int((x.split('-')[0])))\n",
    "        coords_x.append(float((str(x.split('-')[1]).split(\" score=\")[0][1:5]))*image.shape[1])\n",
    "        coords_y.append(float((str(x.split('-')[1]).split(\" score=\")[0][6:11]))*image.shape[0])\n",
    "        probs.append(float(str(x.split('-')[1]).split(\" score=\")[1]))\n",
    "    \n",
    "    for point in range(len(all_points)): # this ensures that undetected kpoints still have a value (nan) \n",
    "        if all_points[point] not in kpts:\n",
    "            kpts.insert(all_points[point], point)\n",
    "            coords_x.insert(all_points[point], \"nan\")\n",
    "            coords_y.insert(all_points[point], \"nan\")\n",
    "            probs.insert(all_points[point], 'nan')\n",
    "    \n",
    "    for k,cox, coy, prob in zip(kpts, coords_x, coords_y, probs): \n",
    "        if type(cox) == str:\n",
    "            skeleton.append([cox,coy,prob])\n",
    "        else:\n",
    "            skeleton.append([round(cox,2),round(coy,2), prob])\n",
    "    return(np.array(skeleton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf0163-afd7-4493-81b4-0fdf896abb4c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "human_no = []\n",
    "frame_no = 0\n",
    "failed_frames=[]\n",
    "folder_path = '/Users/andrei-macpro/Documents/Data/csv_files/'\n",
    "while(cap.isOpened()):\n",
    "    ret_val, image = cap.read()\n",
    "    if ret_val == False:\n",
    "        break\n",
    "    frame_no+=1\n",
    "    humans = e.inference(image,\n",
    "                         resize_to_default=(w > 0 and h > 0),\n",
    "                         upsample_size=4.0)\n",
    "    human_no.append(len(humans))\n",
    "    \n",
    "    if len(humans)>3:\n",
    "        skeleton_4 = extract(humans[3])\n",
    "        skeleton_3 = extract(humans[2])\n",
    "        skeleton_2 = extract(humans[1])\n",
    "        skeleton_1 = extract(humans[0])\n",
    "        data = pd.DataFrame((np.concatenate((skeleton_1,skeleton_2, skeleton_3, skeleton_4), axis=1)),\n",
    "                            columns = ['x_1', 'y_1', 'prob_1', 'x_2', 'y_2', 'prob_2', 'x_3', 'y_3', 'prob_3', \n",
    "                                      'x_4', 'y_4', 'prob_4'])\n",
    "        data.to_csv(folder_path + 'frame_' + str(frame_no) + '.csv')  \n",
    "    \n",
    "    \n",
    "    elif len(humans)>2:\n",
    "        skeleton_3 = extract(humans[2])\n",
    "        skeleton_2 = extract(humans[1])\n",
    "        skeleton_1 = extract(humans[0])\n",
    "        data = pd.DataFrame((np.concatenate((skeleton_1,skeleton_2, skeleton_3), axis=1)),\n",
    "                            columns = ['x_1', 'y_1', 'prob_1', 'x_2', 'y_2', 'prob_2', 'x_3', 'y_3', 'prob_3'])\n",
    "        data.to_csv(folder_path + 'frame_' + str(frame_no) + '.csv') \n",
    "        \n",
    "        \n",
    "    elif len(humans)>1:\n",
    "        skeleton_2 = extract(humans[1])\n",
    "        skeleton_1 = extract(humans[0])\n",
    "        data = pd.DataFrame((np.concatenate((skeleton_1, skeleton_2), axis=1)),\n",
    "                            columns = ['x_1', 'y_1', 'prob_1', 'x_2', 'y_2', 'prob_2'])\n",
    "        data.to_csv(folder_path + 'frame_' + str(frame_no) + '.csv') \n",
    "                            \n",
    "\n",
    "    elif len(humans) == 1:\n",
    "        skeleton_1 = extract(humans[0])\n",
    "        data = pd.DataFrame(skeleton_1,\n",
    "                            columns = ['x_1', 'y_1', 'prob_1'])\n",
    "        data.to_csv(folder_path + 'frame_' + str(frame_no) + '.csv') \n",
    "                \n",
    "\n",
    "    else:\n",
    "        failed_frames.append(frame_no)\n",
    "        print(frame_no)\n",
    "cap.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846f3a3-77c6-4d1d-86a3-eeb9fc4e892f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "kpts_final = []\n",
    "coords_x_final = []\n",
    "coords_y_final = []\n",
    "\n",
    "all_points = [x for x in range(18)]\n",
    "big_dic = []\n",
    "original_kpts =[]\n",
    "probs_final = []\n",
    "while(cap.isOpened()):\n",
    "\n",
    "    ret_val, image = cap.read()\n",
    "    probs = []\n",
    "    kpts = []\n",
    "    coords_x= []\n",
    "    coords_y= []\n",
    "    if ret_val == False:\n",
    "        break\n",
    "    humans = e.inference(image,\n",
    "                         resize_to_default=(w > 0 and h > 0),\n",
    "                         upsample_size=4.0)\n",
    "    print(humans[0])\n",
    "    subscriptable = str(humans[0]).split(\"BodyPart:\")[1:]\n",
    "\n",
    "    for x in subscriptable:\n",
    "        kpts.append(int((x.split('-')[0])))\n",
    "        original_kpts.append(int((x.split('-')[0])))\n",
    "        coords_x.append(float((str(x.split('-')[1]).split(\" score=\")[0][1:5]))*image.shape[1])\n",
    "        coords_y.append(float((str(x.split('-')[1]).split(\" score=\")[0][6:11]))*image.shape[0])\n",
    "        probs.append(float(str(x.split('-')[1]).split(\" score=\")[1]))\n",
    "    \n",
    "    for point in range(len(all_points)):\n",
    "        if all_points[point] not in kpts:\n",
    "            kpts.insert(all_points[point], point)\n",
    "            coords_x.insert(all_points[point], \"nan\")\n",
    "            coords_y.insert(all_points[point], \"nan\")\n",
    "            probs.insert(all_points[point], 'nan')\n",
    "    #kpts_final.append(kpts)\n",
    "    #coords_x_final.append(coords_x)\n",
    "    #coords_y_final.append(coords_y)\n",
    "    #probs_final.append(probs)\n",
    "    \n",
    "    for k,cox, coy, prob in zip(kpts, coords_x, coords_y, probs): # when kpts is too short it will stop early \n",
    "        if type(cox) == str:\n",
    "            big_dic.append({k:[cox,coy,prob]})\n",
    "        else:\n",
    "            big_dic.append({k:[round(cox,2),round(coy,2), prob]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20d0272-69f2-4614-b382-11d525b92bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1096_meal.mp4', '1096_meal_1.mp4', '1098_meal.mp4', '1098_meal_1.mp4']\n"
     ]
    }
   ],
   "source": [
    "path_videos = '/Users/andreibirladeanu/Documents/Data/meal_to_extract/'\n",
    "\n",
    "videos = [file for file in sorted(os.listdir(path_videos)) if file[0] !=\".\"]\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880d831d-377e-4dfc-960f-dfda074a9851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▎                           | 1/4 [6:05:12<18:15:38, 21912.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4/4 [13:00:19<00:00, 11704.84s/it]\n"
     ]
    }
   ],
   "source": [
    "frame_count = []\n",
    "failed_frames = []\n",
    "width = []\n",
    "height = []\n",
    "for video in tqdm(videos):\n",
    "    cap = cv2.VideoCapture(path_videos+video)\n",
    "    frame_no = 0\n",
    "    failed_frame =[]\n",
    "    os.makedirs('/Users/andreibirladeanu/Documents/Data/' +video.split(\".\")[0], exist_ok=True)  \n",
    "    folder_path = '/Users/andreibirladeanu/Documents/Data/' +video.split(\".\")[0]+'/'\n",
    "    while(cap.isOpened()):\n",
    "        ret_val, image = cap.read()\n",
    "        if ret_val == False:\n",
    "            break\n",
    "        frame_no+=1\n",
    "        humans = e.inference(image,\n",
    "                             resize_to_default=(w > 0 and h > 0),\n",
    "                             upsample_size=4.0)\n",
    "\n",
    "\n",
    "        if len(humans)==4:\n",
    "            skeleton_4 = extract(humans[3])\n",
    "            skeleton_3 = extract(humans[2])\n",
    "            skeleton_2 = extract(humans[1])\n",
    "            skeleton_1 = extract(humans[0])\n",
    "            data = pd.DataFrame((np.concatenate((skeleton_1,skeleton_2, skeleton_3, skeleton_4), axis=1)),\n",
    "                                columns = ['x_1', 'y_1', 'prob_1', 'x_2', 'y_2', 'prob_2', 'x_3', 'y_3', 'prob_3', \n",
    "                                          'x_4', 'y_4', 'prob_4'])\n",
    "            data.to_csv(folder_path + 'frame_' + str(frame_no) + '.csv')  \n",
    "\n",
    "\n",
    "        elif len(humans)==3:\n",
    "            skeleton_3 = extract(humans[2])\n",
    "            skeleton_2 = extract(humans[1])\n",
    "            skeleton_1 = extract(humans[0])\n",
    "            data = pd.DataFrame((np.concatenate((skeleton_1,skeleton_2, skeleton_3), axis=1)),\n",
    "                                columns = ['x_1', 'y_1', 'prob_1', 'x_2', 'y_2', 'prob_2', 'x_3', 'y_3', 'prob_3'])\n",
    "            data.to_csv(folder_path + 'frame_' +str(frame_no) + '.csv') \n",
    "\n",
    "\n",
    "        elif len(humans)==2:\n",
    "            skeleton_2 = extract(humans[1])\n",
    "            skeleton_1 = extract(humans[0])\n",
    "            data = pd.DataFrame((np.concatenate((skeleton_1, skeleton_2), axis=1)),\n",
    "                                columns = ['x_1', 'y_1', 'prob_1', 'x_2', 'y_2', 'prob_2'])\n",
    "            data.to_csv(folder_path + 'frame_' +str(frame_no) + '.csv') \n",
    "\n",
    "\n",
    "        elif len(humans) == 1:\n",
    "            skeleton_1 = extract(humans[0])\n",
    "            data = pd.DataFrame(skeleton_1,\n",
    "                                columns = ['x_1', 'y_1', 'prob_1'])\n",
    "            data.to_csv(folder_path + 'frame_' + str(frame_no) + '.csv') \n",
    "\n",
    "\n",
    "        else:\n",
    "            failed_frame.append(frame_no)\n",
    "            print(frame_no)\n",
    "            ## output failed frame file to keep consistency\n",
    "            ## output video resolution\n",
    "    failed_frames.append({video.split(\".\")[0]:failed_frame})\n",
    "    frame_count.append({video.split(\".\")[0]:frame_no})\n",
    "    width.append({video.split(\".\")[0]:(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))})\n",
    "    height.append({video.split(\".\")[0]:int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))})\n",
    "    cap.release()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b50295-7b77-44cc-a870-db6a8ee74465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('something')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c976c2d-2e75-4c69-8e7c-d18f37359243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "metadata = defaultdict(list)\n",
    "\n",
    "for w, h, count in zip(width, height, frame_count):\n",
    "    for d in (w,h, count): # you can list as many input dicts as you want here\n",
    "        for key, value in d.items():\n",
    "            metadata[key].append(value)\n",
    "    \n",
    "metad = pd.DataFrame(metadata, index = ['width','height', 'frame_count']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c161f58-ed43-4552-be46-6f1399cb0f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metad.to_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57d2ff6b-b18b-432d-b435-598d20fb9220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>frame_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [width, height, frame_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fa286-dde6-494a-9bb2-b1cfb1735512",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_frames[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595d2b1-30e2-48b8-ac77-6c0d2394ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}d\n",
    "for key,value, ls in zip(new_dict, no_frames):\n",
    "    print (ls)\n",
    "    new_dict.key == ls.key()\n",
    "    print (a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615216bf-6eff-48e1-aa29-304f5ffe7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.DataFrame.from_dict(no_frames[1],orient='index').transpose()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c37a6-0457-42a7-90f8-9d0bcec8a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838bcee-1e04-49b9-8041-a679592522f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('failed_frames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6ba99-12b6-4ae8-9ad0-ba1bc2070560",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_frames_2 =[]\n",
    "\n",
    "for x in no_frames:\n",
    "    failed_frames_2.append(pd.DataFrame.from_dict(x ,orient='index').transpose())\n",
    "     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b2047-272b-43b6-965c-cd7aefd50e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_frames_2 = pd.concat([x for x in failed_frames_2], axis=1)\n",
    "failed_frames_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7265a-9270-4bba-a2b3-e4cdf6e807e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_frames_2.to_csv('failed_frames_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f31fe3-9d98-46c0-a9bc-ee0aa48176b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot joint 0 \n",
    "\n",
    "joint_0_x = []\n",
    "joint_0_y = []\n",
    "folder_path = '/Users/andrei-macpro/Documents/Data/1042_meal/'\n",
    "for file in sorted_alphanumeric(os.listdir(folder_path)):\n",
    "    data = pd.read_csv(folder_path+file) \n",
    "    joint_0_x.append(float(data.loc[0,['x_1']]))\n",
    "    joint_0_y.append(float(data.loc[0,['y_1']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842f6ab-e5f5-4afb-bb21-267c7ee60294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6810a-da1e-4e66-8727-82ece8bc5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(joint_0_x, joint_0_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed248cd-8971-43c5-bfc5-4675ee8c3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_0_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa3168-adc9-4408-8499-28c2afd08158",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_0_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc432d32-cfc8-40db-ba92-32911c80991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1977fc-10ea-46dc-944d-9099a824fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "x = sorted(joint_0_x)\n",
    "y = sorted(joint_0_y)\n",
    "\n",
    "\n",
    "tck = interpolate.splrep(x, y, s=0)\n",
    "xnew = np.arange(0, 2*np.pi, np.pi/50)\n",
    "ynew = interpolate.splev(xnew, tck, der=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddfcab-f02f-4e2f-a0bc-700c63fd256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, y, 'x', xnew, ynew, xnew, np.sin(xnew), x, y, 'b')\n",
    "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
    "#plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
    "plt.title('Cubic-spline interpolation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6771f-017c-499e-a733-81d3bac66b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(joint_0_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe1421-a586-4ec5-9289-a634176626aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xnew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
