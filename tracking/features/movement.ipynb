{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = '/Users/andrei-macpro/Documents/Data/openpose/play/tracking/tracking/1080_play'\n",
    "# do it for one folder first\n",
    "# load each file in a pandas dataframe\n",
    "\n",
    "tracks = []\n",
    "for filename in sorted(os.listdir(video_folder)):\n",
    "        # load the file in a pandas dataframe\n",
    "    if filename == \".DS_Store\":\n",
    "        continue    \n",
    "    print(filename)\n",
    "    filepath = os.path.join(folder_name, filename)\n",
    "    df = pd.read_csv(filepath, index_col=0)\n",
    "    # Remove rows with duplicated index values because the interpolation sometimes created duplicates\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    df = clean_outliers(df)\n",
    "    tracks.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = tracks[0]\n",
    "child = tracks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cg has duplicate indices\n",
    "if cg.index.duplicated().any():\n",
    "    print(\"cg has duplicate indices\")\n",
    "\n",
    "# Check if child has duplicate indices\n",
    "if child.index.duplicated().any():\n",
    "    print(\"child has duplicate indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_indices_child = child[child.index.duplicated(keep=False)]\n",
    "print(duplicate_indices_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([cg, child], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first column from df \n",
    "df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores for x and y columns\n",
    "# make this code into a function that takes a dataframe and returns the cleaned dataframe\n",
    "\n",
    "from scipy.stats import zscore\n",
    "def clean_outliers(df):\n",
    "    z_scores_x = zscore(df.iloc[:,0])\n",
    "    z_scores_y = zscore(df.iloc[:,1])\n",
    "\n",
    "    # Define threshold for z-score (e.g., 3)\n",
    "    threshold = 3\n",
    "\n",
    "    # Identify outliers based on z-scores\n",
    "    outliers_x = df[abs(z_scores_x) > threshold]\n",
    "    outliers_y = df[abs(z_scores_y) > threshold]\n",
    "\n",
    "    # Remove outliers from DataFrame\n",
    "    cleaned_df = df[~df.index.isin(outliers_x.index) & ~df.index.isin(outliers_y.index)]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scaled[len(child):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine both datasets for joint scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "combined_scaled = scaler.fit_transform(combined_data)\n",
    "\n",
    "# Split the scaled data back into caregiver and child data\n",
    "scaled_caregiver = combined_scaled[:len(cg)]\n",
    "scaled_child = combined_scaled[len(cg):]\n",
    "\n",
    "cg[['x', 'y']] = scaled_caregiver\n",
    "child[['x', 'y']] = scaled_child\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_positions = pd.merge(child, cg, on='frame', suffixes=('_caregiver', '_child'))\n",
    "print(\"Aligned Positions Data:\")\n",
    "print(aligned_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_positions.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "frame_indices = aligned_positions.index\n",
    "for i in range(len(aligned_positions) - 1):\n",
    "    if frame_indices[i + 1] - frame_indices[i] < 5:\n",
    "        distance = np.sqrt((aligned_positions['x_caregiver'].iloc[i] - aligned_positions['x_child'].iloc[i])**2 + \n",
    "                           (aligned_positions['y_caregiver'].iloc[i] - aligned_positions['y_child'].iloc[i])**2)\n",
    "        distances.append(distance)\n",
    "    else:\n",
    "        distances.append(np.nan)  # Or some placeholder for frames that don't meet the criteria\n",
    "\n",
    "distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok but what if there are actually more gaps than just the one ? how do we keep both the aligned positions and the distances in sync?\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for all now \n",
    "from moviepy.editor import VideoFileClip\n",
    "video_folder = '/Users/andrei-macpro/Documents/Data/videos/meal_videos' \n",
    "\n",
    "durations = []\n",
    "file_names = []\n",
    "\n",
    "for folder_name in sorted(os.listdir(video_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(video_folder, folder_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        clip = VideoFileClip(file_path)\n",
    "        durations.append(clip.duration)\n",
    "        file_names.append(folder_name.split('.')[0])\n",
    "\n",
    "durations = pd.DataFrame({'file_name': file_names, 'duration': durations})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to select a row by its cell name in a dataframe\n",
    "df.loc[df['file_name'] == '1043_play']['duration'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv in the folder /Users/andrei-macpro/Documents/Data/videos/durations\n",
    "df.to_csv('/Users/andrei-macpro/Documents/Data/videos/durations/durations_play.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_folder = '/Users/andrei-macpro/Documents/Data/openpose/meal/tracking/meal_tracking_2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do it for all videos\n",
    "stats = {}\n",
    "scaler = StandardScaler()\n",
    "for folder_name in sorted(os.listdir(tracking_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(tracking_folder, folder_name)\n",
    "    tracks = {}\n",
    "    for file in sorted(os.listdir(file_path)):\n",
    "                # load the file in a pandas dataframe\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            df = clean_outliers(df)\n",
    "            tracks[file.split('.')[0]] = df\n",
    "\n",
    "    print(folder_name)\n",
    "    combined_data = pd.concat([df for df in tracks.values()], axis=1)\n",
    "    combined_scaled = scaler.fit_transform(combined_data)\n",
    "    # Convert the scaled data back to a DataFrame\n",
    "    combined_scaled_df = pd.DataFrame(combined_scaled, index=combined_data.index, columns=combined_data.columns)\n",
    "\n",
    "# Split the combined_scaled_df back into separate DataFrames and assign them back to the tracks dictionary\n",
    "    for track_name in tracks.keys():\n",
    "        x_cols = [col for col in combined_scaled_df.columns if col.startswith('x_' + track_name)]\n",
    "        y_cols = [col for col in combined_scaled_df.columns if col.startswith('y_' + track_name)]\n",
    "        tracks[track_name] = combined_scaled_df[x_cols + y_cols]\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    for track_name, track_df in tracks.items():\n",
    "        distances = []\n",
    "        frame_indices = track_df.index\n",
    "        \n",
    "        for i in range(len(track_df)-1):\n",
    "            \n",
    "            if frame_indices[i + 1] - frame_indices[i] < 5:\n",
    "                distance = np.sqrt((track_df['x_' + track_name].iloc[i] - track_df['x_' + track_name].iloc[i+1])**2 + \n",
    "                            (track_df['y_' + track_name].iloc[i] - track_df['y_' + track_name].iloc[i+1])**2)\n",
    "                distances.append(distance)\n",
    "            else:\n",
    "                distances.append(np.nan)\n",
    "        \n",
    "         # Append np.nan to match the number of rows in the DataFrame\n",
    "        distances.append(np.nan)\n",
    "        track_df['distance'] = distances\n",
    "\n",
    "        duration = durations.loc[durations['file_name'] == folder_name]['duration'].values[0]\n",
    "        mean_movement = track_df['distance'].mean() / duration*100\n",
    "        median_movement = track_df['distance'].median() / duration*100\n",
    "        var_movement = track_df['distance'].var() / duration*100\n",
    "        std_movement = track_df['distance'].std() / duration*100\n",
    "        min_movement = track_df['distance'].min() / duration*100\n",
    "        max_movement =  track_df['distance'].max() / duration*100\n",
    "        skewness_movement = track_df['distance'].skew() / duration*100\n",
    "        kurtosis_movement = track_df['distance'].kurtosis() / duration*100\n",
    "\n",
    "        \n",
    "        if folder_name not in stats:\n",
    "            stats[folder_name] = {}\n",
    "        stats[folder_name][track_name] = {'movement_mean': mean_movement, 'movement_var': var_movement, 'movement_std': std_movement, 'movement_min': min_movement, 'movement_max': max_movement, 'movement_skew': skewness_movement, 'movement_kurt': kurtosis_movement}\n",
    "\n",
    "# normalize the distances (mean, variance etc) by the duration of the video\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the stats dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict({(i,j): stats[i][j] \n",
    "                           for i in stats.keys() \n",
    "                           for j in stats[i].keys()},\n",
    "                           orient='index')\n",
    "\n",
    "# Set the index names\n",
    "df.index.names = ['s_id', 'track_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset only the second level of the index (track_name)\n",
    "df.reset_index(level=1, inplace=True)\n",
    "\n",
    "# Create new DataFrame to hold results\n",
    "new_df = pd.DataFrame(index=df.index.unique())\n",
    "\n",
    "# Loop over each unique track_name\n",
    "for track_name in df['track_name'].unique():\n",
    "    # Select rows for this track_name\n",
    "    temp_df = df[df['track_name'] == track_name].copy()\n",
    "    # Drop the 'track_name' column as it's no longer needed\n",
    "    temp_df.drop(columns=['track_name'], inplace=True)\n",
    "    # Add the track_name as a prefix to each column name\n",
    "    temp_df.columns = [f'{track_name}_{col}' for col in temp_df.columns]\n",
    "    # Add the results to new_df\n",
    "    new_df = new_df.join(temp_df)\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to /Users/andrei-macpro/Documents/Data/tracking/features\n",
    "new_df.to_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/movement_each.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
