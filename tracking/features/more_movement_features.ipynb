{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract transition counts, dwell time, autocorrelation, entropy, slope, fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "def clean_outliers(df):\n",
    "    z_scores_x = zscore(df.iloc[:,0])\n",
    "    z_scores_y = zscore(df.iloc[:,1])\n",
    "\n",
    "    # Define threshold for z-score (e.g., 3)\n",
    "    threshold = 3\n",
    "\n",
    "    # Identify outliers based on z-scores\n",
    "    outliers_x = df[abs(z_scores_x) > threshold]\n",
    "    outliers_y = df[abs(z_scores_y) > threshold]\n",
    "\n",
    "    # Remove outliers from DataFrame\n",
    "    cleaned_df = df[~df.index.isin(outliers_x.index) & ~df.index.isin(outliers_y.index)]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "video_folder = '/Users/andrei-macpro/Documents/Data/videos/meal_videos' \n",
    "\n",
    "durations = []\n",
    "file_names = []\n",
    "\n",
    "for folder_name in sorted(os.listdir(video_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(video_folder, folder_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        clip = VideoFileClip(file_path)\n",
    "        durations.append(clip.duration)\n",
    "        file_names.append(folder_name.split('.')[0])\n",
    "\n",
    "durations = pd.DataFrame({'file_name': file_names, 'duration': durations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_folder = '/Users/andrei-macpro/Documents/Data/openpose/meal/tracking/tracking' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract movement states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do it for all videos\n",
    "stats = {}\n",
    "window_size = 200\n",
    "step_size = 5\n",
    "segments_stats_child = []\n",
    "scaler = StandardScaler()\n",
    "for folder_name in sorted(os.listdir(tracking_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(tracking_folder, folder_name)\n",
    "    tracks = {}\n",
    "    for file in sorted(os.listdir(file_path)):\n",
    "                # load the file in a pandas dataframe\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            df = clean_outliers(df)\n",
    "            tracks[file.split('.')[0]] = df\n",
    "\n",
    "    print(folder_name)\n",
    "    combined_data = pd.concat([df for df in tracks.values()], axis=1)\n",
    "    combined_scaled = scaler.fit_transform(combined_data)\n",
    "    # Convert the scaled data back to a DataFrame\n",
    "    combined_scaled_df = pd.DataFrame(combined_scaled, index=combined_data.index, columns=combined_data.columns)\n",
    "\n",
    "# Split the combined_scaled_df back into separate DataFrames and assign them back to the tracks dictionary\n",
    "    for track_name in tracks.keys():\n",
    "        x_cols = [col for col in combined_scaled_df.columns if col.startswith('x_' + track_name)]\n",
    "        y_cols = [col for col in combined_scaled_df.columns if col.startswith('y_' + track_name)]\n",
    "        tracks[track_name] = combined_scaled_df[x_cols + y_cols]\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    for track_name, track_df in tracks.items():\n",
    "        distances = []\n",
    "        frame_indices = track_df.index\n",
    "        \n",
    "        for i in range(len(track_df)-1):\n",
    "            \n",
    "            if frame_indices[i + 1] - frame_indices[i] < 5:\n",
    "                distance = np.sqrt((track_df['x_' + track_name].iloc[i] - track_df['x_' + track_name].iloc[i+1])**2 + \n",
    "                            (track_df['y_' + track_name].iloc[i] - track_df['y_' + track_name].iloc[i+1])**2)\n",
    "                distances.append(distance)\n",
    "            else:\n",
    "                distances.append(np.nan)\n",
    "        \n",
    "         # Append np.nan to match the number of rows in the DataFrame\n",
    "        distances.append(np.nan)\n",
    "        track_df['distance'] = distances\n",
    "        k_means_segments = []\n",
    "        for start in range(0, len(track_df) - window_size + 1, step_size):\n",
    "            if track_df.columns[0] == 'x_cg':\n",
    "                continue\n",
    "            end = start + window_size\n",
    "            segment = track_df['distance'].iloc[start:end]\n",
    "            \n",
    "            # Calculate statistics for the segment\n",
    "            mean_distance = segment.mean()\n",
    "            variance_distance = segment.var()\n",
    "            max_distance = segment.max()\n",
    "            \n",
    "            # Append the statistics to the list\n",
    "            k_means_segments.append({'id': folder_name , 'stats':[mean_distance, variance_distance, max_distance]})\n",
    "    segments_stats_child.append(k_means_segments)      \n",
    "\n",
    "        \n",
    "\n",
    "# normalize the distances (mean, variance etc) by the duration of the video\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_child[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_child_flattened = [{'id': s['id'], 'mean': s['stats'][0], 'variance': s['stats'][1], 'max': s['stats'][2]} for segment in segments_stats_child for s in segment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_df = pd.DataFrame(segments_stats_child_flattened)\n",
    "segments_stats_df = segments_stats_df.dropna()\n",
    "segments_stats_df_for_clustering = segments_stats_df.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_child_flattened = [segment for sublist in segments_stats_child for segment in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_child = [segment for segment in segments_stats_child if not any(np.isnan(value) for value in segment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)  # Adjust the number of clusters as needed\n",
    "kmeans.fit(segments_stats_df_for_clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_df['cluster'] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "grouped = segments_stats_df.groupby('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, group in grouped:\n",
    "    previous_cluster = None\n",
    "    for cluster in group['cluster']:\n",
    "        if previous_cluster is not None and previous_cluster != cluster:\n",
    "            transition_counts[id][(previous_cluster, cluster)] += 1\n",
    "        previous_cluster = cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transitions = {id: sum(count for (from_cluster, to_cluster), count in transitions.items() if from_cluster != to_cluster) for id, transitions in transition_counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = segments_stats_df['id'].unique()\n",
    "total_transitions = {id: total_transitions.get(id, 0) for id in all_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transitions_df = pd.DataFrame(list(total_transitions.items()), columns=['id', 'total_transitions'])\n",
    "total_transitions_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Group the DataFrame by 'id'\n",
    "grouped = segments_stats_df.groupby('id')\n",
    "\n",
    "# Iterate through each group and count transitions\n",
    "for id, group in grouped:\n",
    "    previous_cluster = None\n",
    "    for cluster in group['cluster']:\n",
    "        if previous_cluster is not None:\n",
    "            if (previous_cluster == 0 and cluster == 2) or (previous_cluster == 2 and cluster == 0):\n",
    "                transition_counts[id][(previous_cluster, cluster)] += 1\n",
    "        previous_cluster = cluster\n",
    "\n",
    "# Calculate the total number of transitions from 0 to 2 or from 2 to 0 for each id\n",
    "total_transitions_0_2 = {id: sum(count for (from_cluster, to_cluster), count in transitions.items() if (from_cluster == 0 and to_cluster == 2) or (from_cluster == 2 and to_cluster == 0)) for id, transitions in transition_counts.items()}\n",
    "\n",
    "# Ensure all ids are included, even if no transitions are found\n",
    "all_ids = segments_stats_df['id'].unique()\n",
    "total_transitions_0_2 = {id: total_transitions_0_2.get(id, 0) for id in all_ids}\n",
    "\n",
    "# Convert the total transitions to a DataFrame\n",
    "total_transitions_0_2_df = pd.DataFrame(list(total_transitions_0_2.items()), columns=['id', 'total_transitions_0_2'])\n",
    "\n",
    "total_transitions_0_2_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate dwell time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = segments_stats_df.groupby(['id', 'cluster']).size().reset_index(name='count')\n",
    "cluster_counts_pivot = cluster_counts.pivot(index='id', columns='cluster', values='count').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_counts_pivot), len(total_transitions_df), len(total_transitions_0_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = cluster_counts_pivot.index.union(total_transitions_0_2_df.index).union(total_transitions_df.index)\n",
    "\n",
    "cluster_counts_pivot = cluster_counts_pivot.reindex(all_ids).fillna(0)\n",
    "total_transitions_0_2_df = total_transitions_0_2_df.reindex(all_ids).fillna(0)\n",
    "total_transitions_df = total_transitions_df.reindex(all_ids).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = pd.concat([cluster_counts_pivot, total_transitions_0_2_df, total_transitions_df], axis=1)\n",
    "temporal_features.columns = ['cluster_0', 'cluster_1', 'cluster_2', 'jump_transitions', 'total_transitions']\n",
    "temporal_features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
