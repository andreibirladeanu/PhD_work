{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract transition counts, dwell time, autocorrelation, entropy, slope, fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "def clean_outliers(df):\n",
    "    z_scores_x = zscore(df.iloc[:,0])\n",
    "    z_scores_y = zscore(df.iloc[:,1])\n",
    "\n",
    "    # Define threshold for z-score (e.g., 3)\n",
    "    threshold = 3\n",
    "\n",
    "    # Identify outliers based on z-scores\n",
    "    outliers_x = df[abs(z_scores_x) > threshold]\n",
    "    outliers_y = df[abs(z_scores_y) > threshold]\n",
    "\n",
    "    # Remove outliers from DataFrame\n",
    "    cleaned_df = df[~df.index.isin(outliers_x.index) & ~df.index.isin(outliers_y.index)]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "video_folder = '/Users/andrei-macpro/Documents/Data/videos/meal_videos' \n",
    "\n",
    "durations = []\n",
    "file_names = []\n",
    "\n",
    "for folder_name in sorted(os.listdir(video_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(video_folder, folder_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        clip = VideoFileClip(file_path)\n",
    "        durations.append(clip.duration)\n",
    "        file_names.append(folder_name.split('.')[0])\n",
    "\n",
    "durations = pd.DataFrame({'file_name': file_names, 'duration': durations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_folder = '/Users/andrei-macpro/Documents/Data/openpose/meal/tracking/tracking' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract movement states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do it for all videos\n",
    "stats = {}\n",
    "window_size = 200\n",
    "step_size = 5\n",
    "segments_stats_child = []\n",
    "scaler = StandardScaler()\n",
    "for folder_name in sorted(os.listdir(tracking_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(tracking_folder, folder_name)\n",
    "    tracks = {}\n",
    "    for file in sorted(os.listdir(file_path)):\n",
    "                # load the file in a pandas dataframe\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            df = clean_outliers(df)\n",
    "            tracks[file.split('.')[0]] = df\n",
    "\n",
    "    print(folder_name)\n",
    "    combined_data = pd.concat([df for df in tracks.values()], axis=1)\n",
    "    combined_scaled = scaler.fit_transform(combined_data)\n",
    "    # Convert the scaled data back to a DataFrame\n",
    "    combined_scaled_df = pd.DataFrame(combined_scaled, index=combined_data.index, columns=combined_data.columns)\n",
    "\n",
    "# Split the combined_scaled_df back into separate DataFrames and assign them back to the tracks dictionary\n",
    "    for track_name in tracks.keys():\n",
    "        x_cols = [col for col in combined_scaled_df.columns if col.startswith('x_' + track_name)]\n",
    "        y_cols = [col for col in combined_scaled_df.columns if col.startswith('y_' + track_name)]\n",
    "        tracks[track_name] = combined_scaled_df[x_cols + y_cols]\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    for track_name, track_df in tracks.items():\n",
    "        distances = []\n",
    "        frame_indices = track_df.index\n",
    "        \n",
    "        for i in range(len(track_df)-1):\n",
    "            \n",
    "            if frame_indices[i + 1] - frame_indices[i] < 5:\n",
    "                distance = np.sqrt((track_df['x_' + track_name].iloc[i] - track_df['x_' + track_name].iloc[i+1])**2 + \n",
    "                            (track_df['y_' + track_name].iloc[i] - track_df['y_' + track_name].iloc[i+1])**2)\n",
    "                distances.append(distance)\n",
    "            else:\n",
    "                distances.append(np.nan)\n",
    "        \n",
    "         # Append np.nan to match the number of rows in the DataFrame\n",
    "        distances.append(np.nan)\n",
    "        track_df['distance'] = distances\n",
    "        k_means_segments = []\n",
    "        for start in range(0, len(track_df) - window_size + 1, step_size):\n",
    "            if track_df.columns[0] == 'x_cg':\n",
    "                continue\n",
    "            end = start + window_size\n",
    "            segment = track_df['distance'].iloc[start:end]\n",
    "            \n",
    "            # Calculate statistics for the segment\n",
    "            mean_distance = segment.mean()\n",
    "            variance_distance = segment.var()\n",
    "            max_distance = segment.max()\n",
    "            \n",
    "            # Append the statistics to the list\n",
    "            k_means_segments.append({'id': folder_name , 'stats':[mean_distance, variance_distance, max_distance]})\n",
    "    segments_stats_child.append(k_means_segments)      \n",
    "\n",
    "        \n",
    "\n",
    "# normalize the distances (mean, variance etc) by the duration of the video\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_child[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_child_flattened = [{'id': s['id'], 'mean': s['stats'][0], 'variance': s['stats'][1], 'max': s['stats'][2]} for segment in segments_stats_child for s in segment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_df = pd.DataFrame(segments_stats_child_flattened)\n",
    "segments_stats_df = segments_stats_df.dropna()\n",
    "segments_stats_df_for_clustering = segments_stats_df.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)  # Adjust the number of clusters as needed\n",
    "kmeans.fit(segments_stats_df_for_clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_df['cluster'] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "grouped = segments_stats_df.groupby('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, group in grouped:\n",
    "    previous_cluster = None\n",
    "    for cluster in group['cluster']:\n",
    "        if previous_cluster is not None and previous_cluster != cluster:\n",
    "            transition_counts[id][(previous_cluster, cluster)] += 1\n",
    "        previous_cluster = cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transitions = {id: sum(count for (from_cluster, to_cluster), count in transitions.items() if from_cluster != to_cluster) for id, transitions in transition_counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = segments_stats_df['id'].unique()\n",
    "total_transitions = {id: total_transitions.get(id, 0) for id in all_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transitions_df = pd.DataFrame(list(total_transitions.items()), columns=['id', 'total_transitions'])\n",
    "total_transitions_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Group the DataFrame by 'id'\n",
    "grouped = segments_stats_df.groupby('id')\n",
    "\n",
    "# Iterate through each group and count transitions\n",
    "for id, group in grouped:\n",
    "    previous_cluster = None\n",
    "    for cluster in group['cluster']:\n",
    "        if previous_cluster is not None:\n",
    "            if (previous_cluster == 0 and cluster == 2) or (previous_cluster == 2 and cluster == 0):\n",
    "                transition_counts[id][(previous_cluster, cluster)] += 1\n",
    "        previous_cluster = cluster\n",
    "\n",
    "# Calculate the total number of transitions from 0 to 2 or from 2 to 0 for each id\n",
    "total_transitions_0_2 = {id: sum(count for (from_cluster, to_cluster), count in transitions.items() if (from_cluster == 0 and to_cluster == 2) or (from_cluster == 2 and to_cluster == 0)) for id, transitions in transition_counts.items()}\n",
    "\n",
    "# Ensure all ids are included, even if no transitions are found\n",
    "all_ids = segments_stats_df['id'].unique()\n",
    "total_transitions_0_2 = {id: total_transitions_0_2.get(id, 0) for id in all_ids}\n",
    "\n",
    "# Convert the total transitions to a DataFrame\n",
    "total_transitions_0_2_df = pd.DataFrame(list(total_transitions_0_2.items()), columns=['id', 'total_transitions_0_2'])\n",
    "\n",
    "total_transitions_0_2_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate dwell time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = segments_stats_df.groupby(['id', 'cluster']).size().reset_index(name='count')\n",
    "cluster_counts_pivot = cluster_counts.pivot(index='id', columns='cluster', values='count').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_counts_pivot), len(total_transitions_df), len(total_transitions_0_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = cluster_counts_pivot.index.union(total_transitions_0_2_df.index).union(total_transitions_df.index)\n",
    "\n",
    "cluster_counts_pivot = cluster_counts_pivot.reindex(all_ids).fillna(0)\n",
    "total_transitions_0_2_df = total_transitions_0_2_df.reindex(all_ids).fillna(0)\n",
    "total_transitions_df = total_transitions_df.reindex(all_ids).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = pd.concat([cluster_counts_pivot, total_transitions_0_2_df, total_transitions_df], axis=1)\n",
    "temporal_features.columns = ['cluster_0', 'cluster_1', 'cluster_2', 'jump_transitions', 'total_transitions']\n",
    "temporal_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations.index = durations['file_name']\n",
    "durations.index.name = 'id'\n",
    "durations.drop(columns=['file_name'], inplace=True)\n",
    "durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not temporal_features.index.equals(durations.index):\n",
    "    print(\"Indices are not aligned. Aligning indices...\")\n",
    "    durations = durations.reindex(temporal_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = temporal_features.div(durations['duration'], axis=0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import entropy\n",
    "def calculate_entropy(series):\n",
    "    # Normalize the series to get probabilities\n",
    "    value_counts = series.value_counts(normalize=True)\n",
    "    return entropy(value_counts)\n",
    "\n",
    "distance_entropy = calculate_entropy(track_df['distance'])\n",
    "\n",
    "# Calculate slope using linear regression\n",
    "def calculate_slope(series):\n",
    "    X = np.arange(len(series)).reshape(-1, 1)\n",
    "    y = series.values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do it for all videos\n",
    "stats = {}\n",
    "lag = 50\n",
    "scaler = StandardScaler()\n",
    "for folder_name in sorted(os.listdir(tracking_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(tracking_folder, folder_name)\n",
    "    tracks = {}\n",
    "    for file in sorted(os.listdir(file_path)):\n",
    "                # load the file in a pandas dataframe\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            df = clean_outliers(df)\n",
    "            tracks[file.split('.')[0]] = df\n",
    "\n",
    "    print(folder_name)\n",
    "    combined_data = pd.concat([df for df in tracks.values()], axis=1)\n",
    "    combined_scaled = scaler.fit_transform(combined_data)\n",
    "    # Convert the scaled data back to a DataFrame\n",
    "    combined_scaled_df = pd.DataFrame(combined_scaled, index=combined_data.index, columns=combined_data.columns)\n",
    "\n",
    "# Split the combined_scaled_df back into separate DataFrames and assign them back to the tracks dictionary\n",
    "    for track_name in tracks.keys():\n",
    "        x_cols = [col for col in combined_scaled_df.columns if col.startswith('x_' + track_name)]\n",
    "        y_cols = [col for col in combined_scaled_df.columns if col.startswith('y_' + track_name)]\n",
    "        tracks[track_name] = combined_scaled_df[x_cols + y_cols]\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    for track_name, track_df in tracks.items():\n",
    "        distances = []\n",
    "        frame_indices = track_df.index\n",
    "        \n",
    "        for i in range(len(track_df)-1):\n",
    "            \n",
    "            if frame_indices[i + 1] - frame_indices[i] < 5:\n",
    "                distance = np.sqrt((track_df['x_' + track_name].iloc[i] - track_df['x_' + track_name].iloc[i+1])**2 + \n",
    "                            (track_df['y_' + track_name].iloc[i] - track_df['y_' + track_name].iloc[i+1])**2)\n",
    "                distances.append(distance)\n",
    "            else:\n",
    "                distances.append(np.nan)\n",
    "        \n",
    "         # Append np.nan to match the number of rows in the DataFrame\n",
    "        distances.append(np.nan)\n",
    "        track_df['distance'] = distances\n",
    "        autocorrelation_50 = track_df['distance'].autocorr(lag=50)\n",
    "        autocorrelation_100 = track_df['distance'].autocorr(lag=100)\n",
    "        distance_entropy = calculate_entropy(track_df['distance'])\n",
    "        distance_slope = calculate_slope(track_df['distance'].dropna())\n",
    "        fft_values = np.fft.fft(track_df['distance'].dropna())\n",
    "        fft_magnitude = np.abs(fft_values)\n",
    "        fft_frequencies = np.fft.fftfreq(len(track_df['distance'].dropna()))\n",
    "        spectral_centroid = np.sum(fft_frequencies * fft_magnitude) / np.sum(fft_magnitude)\n",
    "        signal_power = np.sum(fft_magnitude**2)\n",
    "        high_freq_power = np.sum(fft_magnitude[fft_frequencies > 0.5]**2)\n",
    "        if folder_name not in stats:\n",
    "            stats[folder_name] = {}\n",
    "        stats[folder_name][track_name] = {'autocorrelation_50': autocorrelation_50, 'autocorrelation_100': autocorrelation_100,\n",
    "                                          'entropy': distance_entropy, 'slope': distance_slope, 'spectral_centroid': spectral_centroid,\n",
    "                                          'signal_power': signal_power, 'low_freq_power': low_freq_power}\n",
    "        \n",
    "\n",
    "# normalize the distances (mean, variance etc) by the duration of the video\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the stats dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict({(i,j): stats[i][j] \n",
    "                           for i in stats.keys() \n",
    "                           for j in stats[i].keys()},\n",
    "                           orient='index')\n",
    "\n",
    "# Set the index names\n",
    "df.index.names = ['id', 'track_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset only the second level of the index (track_name)\n",
    "df.reset_index(level=1, inplace=True)\n",
    "\n",
    "# Create new DataFrame to hold results\n",
    "new_df = pd.DataFrame(index=df.index.unique())\n",
    "\n",
    "# Loop over each unique track_name\n",
    "for track_name in df['track_name'].unique():\n",
    "    # Select rows for this track_name\n",
    "    temp_df = df[df['track_name'] == track_name].copy()\n",
    "    # Drop the 'track_name' column as it's no longer needed\n",
    "    temp_df.drop(columns=['track_name'], inplace=True)\n",
    "    # Add the track_name as a prefix to each column name\n",
    "    temp_df.columns = [f'{track_name}_{col}' for col in temp_df.columns]\n",
    "    # Add the results to new_df\n",
    "    new_df = new_df.join(temp_df)\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat([temporal_features, new_df], axis=1)\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.to_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/temporal_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
