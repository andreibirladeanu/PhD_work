{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meal = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/combined_features.csv', index_col=0)\n",
    "df_meal = df_meal.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])\n",
    "\n",
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df_meal['label'] = df_meal['label'].map({'no_rad': 0, 'rad': 1})\n",
    "df_play['label'] = df_play['label'].map({'no_rad': 0, 'rad': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reset the index\n",
    "df_meal = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_meal['group'] = df_meal['s_id'].str.split('_').str[0]\n",
    "df_grouped_meal = df_meal.groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_meal.index.name = 's_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_play = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_play['group'] = df_play['s_id'].str.split('_').str[0]\n",
    "df_grouped_play = df_play.groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_play.index.name = 's_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the grouped dataframes\n",
    "df_grouped_meal = df_grouped_meal.reset_index()\n",
    "df_grouped_play = df_grouped_play.reset_index()\n",
    "\n",
    "# Add a new 'group' column to each DataFrame\n",
    "df_grouped_meal['group'] = 'meal'\n",
    "df_grouped_play['group'] = 'play'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat([df_grouped_meal, df_grouped_play], axis=0)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.set_index('s_id', inplace=True)\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop last column from final_df\n",
    "final_df = final_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "final_df['label'] = final_df['label'].map({0: 'no_rad', 1: 'rad'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = final_df.drop(['label','group'], axis=1)\n",
    "y = final_df['label']\n",
    "groups = final_df['group']\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "('lr', LogisticRegression(), {'lr__C': [0.01, 0.1, 1, 10, 100], 'lr__penalty': ['l1', 'l2'], 'lr__solver': ['liblinear', 'saga']}),\n",
    "    ('svc_linear', SVC(kernel='linear'), {'svc_linear__C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {'svc_rbf__C': [0.01, 0.1, 1, 10, 100], 'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('rf', RandomForestClassifier(), {'rf__n_estimators': [10, 50, 100, 200], 'rf__max_depth': [None, 5, 10, 15], 'rf__min_samples_split': [2, 5, 10]})\n",
    "]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled)\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X = final_df.drop(['label','group'], axis=1)\n",
    "y = final_df['label']\n",
    "groups = final_df['group']\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Initialize the selector\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients of the model\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Get the absolute values of the coefficients\n",
    "abs_coefficients = abs(coefficients)\n",
    "\n",
    "# Create a DataFrame with the feature names and coefficients\n",
    "feature_importances = pd.DataFrame({'feature': X.columns, 'importance': abs_coefficients})\n",
    "\n",
    "# Sort the DataFrame by the coefficients in descending order\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "# Print the features in order of their importance\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients of the model\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame with the feature names and coefficients\n",
    "feature_importances = pd.DataFrame({'feature': X.columns, 'coefficient': coefficients})\n",
    "\n",
    "# Calculate the absolute values of the coefficients for the importance\n",
    "feature_importances['importance'] = abs(feature_importances['coefficient'])\n",
    "\n",
    "# Sort the DataFrame by the importance in descending order\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "# Print the features in order of their importance, including the sign of the coefficients\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume that 'feature_importances' is a DataFrame where the first column is the feature names\n",
    "# and the second column is the feature importances\n",
    "\n",
    "# Define the groups of features\n",
    "feature_groups = {\n",
    "    \"movement caregiver\": ['cg_movement_mean', 'cg_movement_var', 'cg_movement_std',\n",
    "       'cg_movement_min', 'cg_movement_max', 'cg_movement_skew',],\n",
    "    \"movement child\": ['child_movement_min', 'child_movement_max',\n",
    "       'child_movement_skew', 'child_movement_kurt',], \n",
    "       'area travelled':['cg_area_travelled',\n",
    "       'child_area_travelled'], \n",
    "       \"angle child\" : ['child_angle_mean',\n",
    "       'child_angle_sin', 'child_angle_cosin', 'child_angle_std',\n",
    "       'child_angle_var'], \n",
    "       'angle caregiver': ['cg_angle_mean', 'cg_angle_sin',\n",
    "       'cg_angle_cosin', 'cg_angle_std', 'cg_angle_var'],\n",
    "       'contact':['average_contact_duration',\n",
    "       'frequency_contact'],\n",
    "       'distance':['average_distance_duration', 'frequency_distance'],\n",
    "       'proximity':['average_proximity', 'variance_proximity', 'std_dev_proximity',\n",
    "       'min_proximity', 'max_proximity'],\n",
    "       'correlation':['correlation'],\n",
    "       'cross correlation':['cross_correlation']\n",
    "}\n",
    "\n",
    "    # Add more groups as needed\n",
    "\n",
    "# Calculate the total absolute coefficient for each group\n",
    "group_coefficients = {}\n",
    "for group, features in feature_groups.items():\n",
    "    group_coefficients[group] = feature_importances[feature_importances.iloc[:, 0].isin(features)].iloc[:, 1].abs().sum()\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easier plotting\n",
    "group_coefficients = pd.DataFrame(list(group_coefficients.items()), columns=[\"Group\", \"Coefficient\"])\n",
    "\n",
    "# Sort the DataFrame by the coefficients in descending order\n",
    "sorted_groups = group_coefficients.sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "# Plot the group coefficients\n",
    "fig, ax = plt.subplots(figsize=(20, 10))  # Increase the figure size to make the labels more readable\n",
    "ax.set_title(\"Group coefficients\")\n",
    "bars = ax.bar(sorted_groups[\"Group\"], sorted_groups[\"Coefficient\"], align=\"center\")\n",
    "\n",
    "# Move the x-axis ticks to the left\n",
    "ax.set_xticks(np.arange(len(sorted_groups[\"Group\"])) - 0.2)\n",
    "ax.set_xticklabels(sorted_groups[\"Group\"], rotation=45, fontsize=14)  # Rotate the x-axis labels to make them more readable\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total absolute coefficient for each group\n",
    "group_coefficients = {}\n",
    "for group, features in feature_groups.items():\n",
    "    group_coefficients[group] = feature_importances[feature_importances.iloc[:, 0].isin(features)].iloc[:, 1].abs().sum()\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easier plotting\n",
    "group_coefficients = pd.DataFrame(list(group_coefficients.items()), columns=[\"Group\", \"Coefficient\"])\n",
    "\n",
    "# Sort the DataFrame by the coefficients in descending order\n",
    "sorted_groups = group_coefficients.sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "# Plot the group coefficients\n",
    "fig, ax = plt.subplots(figsize=(20, 10))  # Increase the figure size to make the labels more readable\n",
    "ax.set_title(\"Group coefficients\")\n",
    "bars = ax.bar(sorted_groups[\"Group\"], sorted_groups[\"Coefficient\"], align=\"center\")\n",
    "\n",
    "# Move the x-axis ticks to the left\n",
    "ax.set_xticks(np.arange(len(sorted_groups[\"Group\"])) - 0.2)\n",
    "ax.set_xticklabels(sorted_groups[\"Group\"], rotation=45, fontsize=14)  # Rotate the x-axis labels to make them more readable\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = final_df['child_angle_horizontal_mean'].corr(final_df['child_movement_mean'])\n",
    "print(f\"The correlation between 'child_angle_horizontal_mean' and 'child_movement_mean' is {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = final_df['child_angle_horizontal_mean'].corr(final_df['child_movement_mean'])\n",
    "print(f\"The correlation between 'child_angle_horizontal_mean' and 'child_movement_mean' is {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = final_df['child_angle_horizontal_mean'].corr(final_df['child_movement_mean'])\n",
    "print(f\"The correlation between 'child_angle_horizontal_mean' and 'child_movement_mean' is {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's use PCA to reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming final_df is your DataFrame and the last column is the target\n",
    "\n",
    "# Separate out the features and the target\n",
    "X = final_df.iloc[:, :-2]\n",
    "y = final_df.iloc[:, -2]\n",
    "\n",
    "# Standardize the features to have a mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to the features\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert the PCA results back to a DataFrame\n",
    "final_df_pca = pd.DataFrame(data=X_pca)\n",
    "final_df_pca['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = final_df.drop(['label','group'], axis=1)\n",
    "y = final_df['label']\n",
    "groups = final_df['group']\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "('lr', LogisticRegression(), {'lr__C': [0.01, 0.1, 1, 10, 100], 'lr__penalty': ['l1', 'l2'], 'lr__solver': ['liblinear', 'saga']}),\n",
    "    ('svc_linear', SVC(kernel='linear'), {'svc_linear__C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {'svc_rbf__C': [0.01, 0.1, 1, 10, 100], 'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('rf', RandomForestClassifier(), {'rf__n_estimators': [10, 50, 100, 200], 'rf__max_depth': [None, 5, 10, 15], 'rf__min_samples_split': [2, 5, 10]})\n",
    "]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled)\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
