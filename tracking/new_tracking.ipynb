{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from sorted_alpha import sorted_alpha\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import interpolate\n",
    "from frame_count import frame_count\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth the data before tracking\n",
    "\n",
    "# read all csv files, then add them to one big df then smooth the data and export  back to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/andrei-macpro/Documents/Data/pose/play_openpose/1264_play'\n",
    "detects = sorted_alpha(path)\n",
    "numbers = [int((x.split(\".\"))[0].split(\"_\")[1]) for x in detects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_skeleton_data(csv_file_path):\n",
    "    # 1. read data\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    temp_data = data.copy()\n",
    "\n",
    "    \n",
    "    # 2. drop skeletons with missing neck\n",
    "    for i in range(0, len(data.columns[1:]), 3):\n",
    "        neck_value = data[data.columns[1:][i]].loc[1]\n",
    "        if pd.isna(neck_value) or neck_value == 0:\n",
    "            temp_data = temp_data.drop([data.columns[1:][i], data.columns[1:][i+1], data.columns[1:][i+2]], axis=1)  # Drop skeletons with missing neck\n",
    "    data = temp_data\n",
    "\n",
    "    # 3. if no skeletons with neck return None\n",
    "    if len(data.columns) < 4:  # If not enough skeletons with neck present, return None\n",
    "        return None, None\n",
    "\n",
    "    # remove fourth skeleton if detected\n",
    "    elif len(data.columns) > 10:\n",
    "        # Drop the fourth skeleton if it has been detected\n",
    "        data = data.drop(data.columns[10:], axis=1) \n",
    "\n",
    "    # 4. initialize the tracking with the first two skeletons and save the neck_points\n",
    "    detections = []\n",
    "    neck_keypoints = []\n",
    "    for col in range(0, len(data.columns[1:]), 3):\n",
    "        skeleton = data.iloc[:, col:col+3].values\n",
    "        detections.append(skeleton[:,1:])  # Select only x and y columns\n",
    "        neck_keypoints.append(skeleton[:,1:][1])  # Assuming neck keypoint is the second row (index 1)\n",
    "    \n",
    "    detections = np.array(detections)  # Shape: (num_skeletons, num_keypoints)\n",
    "    neck_keypoints = np.array(neck_keypoints)  # Shape: (num_skeletons, 2)\n",
    "\n",
    "    return detections, neck_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/Users/andrei-macpro/Documents/Data/pose/play_openpose/1264_play/frame_3318.csv'\n",
    "detections, neck_keypoints = process_skeleton_data(csv_file_path)\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Initialize tracking variables\n",
    "tracks = []\n",
    "frames = []\n",
    "knn = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "\n",
    "# Example usage\n",
    "path = '/Users/andrei-macpro/Documents/Data/pose/meal_openpose/1043_meal'\n",
    "detects = sorted_alpha(path)\n",
    "numbers = [int(f.split('_')[-1].split('.')[0]) for f in detects]\n",
    "\n",
    "for i, number in zip(range(len(detects)), numbers):\n",
    "    frames.append(number)\n",
    "    print('frame no:', number)\n",
    "    \n",
    "    data = pd.read_csv(os.path.join(path, detects[i]))\n",
    "    \n",
    "    if i == 0:  # For the first frame, just get the skeletons, don't do anything\n",
    "        detections, necks = process_skeleton_data(os.path.join(path, detects[i]))\n",
    "        # Initialize track lists for each detection\n",
    "        for _ in range(len(detections)):\n",
    "            tracks.append([])\n",
    "\n",
    "        # Append each detection to its corresponding track list\n",
    "        for j, detection in enumerate(detections):\n",
    "            tracks[j].append(detection)\n",
    "\n",
    "    else:\n",
    "        detections, necks = process_skeleton_data(os.path.join(path, detects[i]))\n",
    "        # Get the last neck positions from the tracks\n",
    "        last_necks = np.array([track[-1][1] for track in tracks])\n",
    "\n",
    "        # Fit the KNN model on the last neck positions\n",
    "        knn.fit(last_necks)\n",
    "        # Keep track of which detections have been assigned\n",
    "        assigned_detections = set()\n",
    "\n",
    "        # Store distances and indices\n",
    "        distances_indices = []\n",
    "        # Compute Euclidean distances between each pair of new detections \n",
    "        necks_array = np.array(necks)\n",
    "        num_detections = len(necks_array)\n",
    "        distances = np.zeros((num_detections, num_detections))\n",
    "\n",
    "\n",
    "        for j in range(num_detections):\n",
    "            for k in range(j + 1, num_detections):\n",
    "                dist = euclidean(necks_array[j], necks_array[k])\n",
    "                distances[j, k] = dist\n",
    "                distances[k, j] = dist  # Symmetric matrix\n",
    "        # Check if distance between the first 2 detections are more than 200\n",
    "        if num_detections > 1:\n",
    "            if i <50:\n",
    "                if  np.any(distances[0:2,0:2]) > 200:\n",
    "                    # Find the closest track for each new detection\n",
    "                    for j, neck in enumerate(necks):\n",
    "                        dist, index = knn.kneighbors([neck])\n",
    "                        distances_indices.append((dist[0][0], index[0][0], j))\n",
    "\n",
    "                    # Sort detections by distance to the closest track\n",
    "                    distances_indices.sort()\n",
    "\n",
    "                    # Assign the closest detections to the tracks\n",
    "                    for dist, closest_track_index, j in distances_indices[:len(tracks)]:\n",
    "                        tracks[closest_track_index].append(detections[j])\n",
    "                        assigned_detections.add(j)\n",
    "\n",
    "                    # If there are more detections than tracks, create new tracks for the remaining detections\n",
    "                    for dist, closest_track_index, j in distances_indices[len(tracks):]:\n",
    "                        new_track_index = len(tracks)\n",
    "                        tracks.append([])  # Initialize a new track\n",
    "                        tracks[new_track_index].append(detections[j])\n",
    "                        assigned_detections.add(j)\n",
    "                else:\n",
    "                    relative_position = relative_position(tracks[0], tracks[1], necks[0], necks[1])\n",
    "                    if relative_position == 1:\n",
    "                        tracks[0].append(detections[0])\n",
    "                        tracks[1].append(detections[1])\n",
    "                    else:\n",
    "                        tracks[0].append(detections[1])\n",
    "                        tracks[1].append(detections[0])\n",
    "                    \n",
    "        elif num_detections == 1:\n",
    "            if np.any(distances) > 200:\n",
    "            # this should still do the relative position calculation if distance<200 \n",
    "                for j, neck in enumerate(necks):\n",
    "                    dist, index = knn.kneighbors([neck])\n",
    "                    tracks[index[0][0]].append(detections[j])\n",
    "                    assigned_detections.add(j)\n",
    "            else:\n",
    "                if i <50:\n",
    "                    for j, neck in enumerate(necks):\n",
    "                        dist, index = knn.kneighbors([neck])\n",
    "                        tracks[index[0][0]].append(detections[j])\n",
    "                        assigned_detections.add(j)\n",
    "\n",
    "                   ## think about this : what if there's only one detection and i>50?  \n",
    "                else:\n",
    "                    relative_position = relative_position(tracks[0], tracks[1], necks[0], necks[1])\n",
    "                    if relative_position == 1:\n",
    "                        tracks[0].append(detections[0])\n",
    "                        tracks[1].append(detections[1])\n",
    "                    else:\n",
    "                        tracks[0].append(detections[1])\n",
    "                        tracks[1].append(detections[0])\n",
    "\n",
    "\n",
    "\n",
    "### the case where num_detections > 1 and distances <200  and i < 50 is not handled yet\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_position(tracks, necks):\n",
    "    num_tracks = len(tracks)\n",
    "    num_necks = len(necks)\n",
    "\n",
    "    if num_necks == 1:\n",
    "        neck = necks[0]\n",
    "        track1, track2 = tracks[:2]\n",
    "\n",
    "        x_diff = [d1[0][0] - d2[0][0] for d1, d2 in zip(track1[-50:], track2[-50:])]\n",
    "        y_diff = [d1[0][1] - d2[0][1] for d1, d2 in zip(track1[-50:], track2[-50:])]\n",
    "\n",
    "        mean_x_diff = np.mean(x_diff)\n",
    "        mean_y_diff = np.mean(y_diff)\n",
    "\n",
    "        if abs(mean_x_diff) > abs(mean_y_diff):\n",
    "            return 1 if neck[0] > (track1[-1][0][0] + track2[-1][0][0]) / 2 else 2\n",
    "        else:\n",
    "            return 1 if neck[1] > (track1[-1][0][1] + track2[-1][0][1]) / 2 else 2\n",
    "\n",
    "    track1, track2 = tracks[:2]\n",
    "    neck1, neck2 = necks[:2]\n",
    "\n",
    "    x_diff = [d1[0][0] - d2[0][0] for d1, d2 in zip(track1[-50:], track2[-50:])]\n",
    "    y_diff = [d1[0][1] - d2[0][1] for d1, d2 in zip(track1[-50:], track2[-50:])]\n",
    "\n",
    "    mean_x_diff = np.mean(x_diff)\n",
    "    mean_y_diff = np.mean(y_diff)\n",
    "\n",
    "    if abs(mean_x_diff) > abs(mean_y_diff):  # is x axis difference greater than y axis difference\n",
    "        if mean_x_diff > 0:  # what's their relative position on x axis\n",
    "            return 1 if neck1[0] > neck2[0] else 2  # maintain relative position\n",
    "        else:\n",
    "            return 1 if neck1[0] < neck2[0] else 2\n",
    "    else:  # is y axis difference greater than x axis difference\n",
    "        if mean_y_diff > 0:  # what's their relative position on y axis\n",
    "            return 1 if neck1[1] > neck2[1] else 2  # maintain relative position\n",
    "        else:\n",
    "            return 1 if neck1[1] < neck2[1] else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ASPECT RATIO INSTEAD OF RELATIVE POSITION\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relative_position(tracks, detections):\n",
    "    num_tracks = len(tracks)\n",
    "    num_detections = len(detections)\n",
    "\n",
    "    if num_detections == 1:\n",
    "        detection = detections[0]\n",
    "        track1, track2 = tracks[:2]\n",
    "\n",
    "        aspect_ratio_track1 = [detection[1][0] / detection[1][1] for detection, _ in track1[-50:]]\n",
    "        aspect_ratio_track2 = [detection[1][0] / detection[1][1] for detection, _ in track2[-50:]]\n",
    "\n",
    "        mean_aspect_ratio_track1 = np.mean(aspect_ratio_track1)\n",
    "        mean_aspect_ratio_track2 = np.mean(aspect_ratio_track2)\n",
    "\n",
    "        aspect_ratio_detection = detection[1][0] / detection[1][1]\n",
    "\n",
    "        if abs(aspect_ratio_detection - mean_aspect_ratio_track1) < abs(aspect_ratio_detection - mean_aspect_ratio_track2):\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    track1, track2 = tracks[:2]\n",
    "    detection1, detection2 = detections[:2]\n",
    "\n",
    "    aspect_ratio_track1 = [detection[1][0] / detection[1][1] for detection, _ in track1[-50:]]\n",
    "    aspect_ratio_track2 = [detection[1][0] / detection[1][1] for detection, _ in track2[-50:]]\n",
    "\n",
    "    mean_aspect_ratio_track1 = np.mean(aspect_ratio_track1)\n",
    "    mean_aspect_ratio_track2 = np.mean(aspect_ratio_track2)\n",
    "\n",
    "    aspect_ratio_detection1 = detection1[1][0] / detection1[1][1]\n",
    "    aspect_ratio_detection2 = detection2[1][0] / detection2[1][1]\n",
    "\n",
    "    if abs(aspect_ratio_detection1 - mean_aspect_ratio_track1) < abs(aspect_ratio_detection1 - mean_aspect_ratio_track2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE AREA OF BOUNDING BOX INSTEAD OF ASPECT RATIO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relative_position(tracks, detections):\n",
    "    num_tracks = len(tracks)\n",
    "    num_detections = len(detections)\n",
    "\n",
    "    if num_detections == 1:\n",
    "        detection = detections[0]\n",
    "        track1, track2 = tracks[:2]\n",
    "\n",
    "        area_track1 = [detection[1][0] * detection[1][1] for detection, _ in track1[-50:]]\n",
    "        area_track2 = [detection[1][0] * detection[1][1] for detection, _ in track2[-50:]]\n",
    "\n",
    "        mean_area_track1 = np.mean(area_track1)\n",
    "        mean_area_track2 = np.mean(area_track2)\n",
    "\n",
    "        area_detection = detection[1][0] * detection[1][1]\n",
    "\n",
    "        if abs(area_detection - mean_area_track1) < abs(area_detection - mean_area_track2):\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    track1, track2 = tracks[:2]\n",
    "    detection1, detection2 = detections[:2]\n",
    "\n",
    "    area_track1 = [detection[1][0] * detection[1][1] for detection, _ in track1[-50:]]\n",
    "    area_track2 = [detection[1][0] * detection[1][1] for detection, _ in track2[-50:]]\n",
    "\n",
    "    mean_area_track1 = np.mean(area_track1)\n",
    "    mean_area_track2 = np.mean(area_track2)\n",
    "\n",
    "    area_detection1 = detection1[1][0] * detection1[1][1]\n",
    "    area_detection2 = detection2[1][0] * detection2[1][1]\n",
    "\n",
    "    if abs(area_detection1 - mean_area_track1) < abs(area_detection1 - mean_area_track2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMBINE AREA AND ASPECT RATIO \n",
    "import numpy as np\n",
    "\n",
    "def relative_position(tracks, detections):\n",
    "    def calculate_bounding_box(keypoints):\n",
    "        if keypoints is None or len(keypoints) == 0 or not isinstance(keypoints[0], (list, np.ndarray)):\n",
    "            print(f\"Invalid keypoints format: {keypoints}\")\n",
    "            raise ValueError(\"Invalid keypoints format\")\n",
    "        x_coordinates = []\n",
    "        y_coordinates = []\n",
    "        for point in keypoints:\n",
    "            if isinstance(point, (list, np.ndarray)) and len(point) == 2:\n",
    "                if not np.isnan(point[0]) and not np.isnan(point[1]):\n",
    "                    x_coordinates.append(point[0])\n",
    "                    y_coordinates.append(point[1])\n",
    "            else:\n",
    "                print(f\"Invalid point format: {point}\")\n",
    "        if len(x_coordinates) == 0 or len(y_coordinates) == 0:\n",
    "            return 0, 0  # Return zero width and height if all coordinates are NaN\n",
    "        x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
    "        y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        return width, height\n",
    "    \n",
    "    def calculate_mean_area_aspect_ratio(track):\n",
    "        areas = []\n",
    "        aspect_ratios = []\n",
    "        for detection, _ in track[-50:]:\n",
    "            width, height = calculate_bounding_box(detection)\n",
    "            areas.append(width * height)\n",
    "            aspect_ratios.append(width / height if height != 0 else 0)\n",
    "        return np.mean(areas), np.mean(aspect_ratios)\n",
    "\n",
    "    def calculate_score(detection, mean_area, mean_aspect_ratio):\n",
    "        width, height = calculate_bounding_box(detection)\n",
    "        area = width * height\n",
    "        aspect_ratio = width / height if height != 0 else 0\n",
    "        return abs(area - mean_area) + abs(aspect_ratio - mean_aspect_ratio)\n",
    "\n",
    "    track1, track2 = tracks[:2]\n",
    "    mean_area_track1, mean_aspect_ratio_track1 = calculate_mean_area_aspect_ratio(track1)\n",
    "    mean_area_track2, mean_aspect_ratio_track2 = calculate_mean_area_aspect_ratio(track2)\n",
    "\n",
    "    if len(detections) == 1:\n",
    "        detection = detections[0]\n",
    "        score_track1 = calculate_score(detection, mean_area_track1, mean_aspect_ratio_track1)\n",
    "        score_track2 = calculate_score(detection, mean_area_track2, mean_aspect_ratio_track2)\n",
    "        return 1 if score_track1 < score_track2 else 2\n",
    "\n",
    "    detection1, detection2 = detections[:2]\n",
    "    score_track1_detection1 = calculate_score(detection1, mean_area_track1, mean_aspect_ratio_track1)\n",
    "    score_track2_detection1 = calculate_score(detection1, mean_area_track2, mean_aspect_ratio_track2)\n",
    "    score_track1_detection2 = calculate_score(detection2, mean_area_track1, mean_aspect_ratio_track1)\n",
    "    score_track2_detection2 = calculate_score(detection2, mean_area_track2, mean_aspect_ratio_track2)\n",
    "\n",
    "    if score_track1_detection1 + score_track2_detection2 < score_track2_detection1 + score_track1_detection2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMBINE multiple bbox features\n",
    "import numpy as np\n",
    "\n",
    "def relative_position(tracks, detections):\n",
    "    def calculate_bounding_box(keypoints):\n",
    "        if keypoints is None or len(keypoints) == 0 or not isinstance(keypoints[0], (list, np.ndarray)):\n",
    "            print(f\"Invalid keypoints format: {keypoints}\")\n",
    "            raise ValueError(\"Invalid keypoints format\")\n",
    "        x_coordinates = []\n",
    "        y_coordinates = []\n",
    "        for point in keypoints:\n",
    "                x_coordinates.append(point[0])\n",
    "                y_coordinates.append(point[1])\n",
    "\n",
    "        if len(x_coordinates) == 0 or len(y_coordinates) == 0:\n",
    "            return 0, 0, 0, 0  # Return zero width and height if all coordinates are NaN\n",
    "        x_min, x_max = np.nanmin(x_coordinates), np.nanmax(x_coordinates)\n",
    "        y_min, y_max = np.nanmin(y_coordinates), np.nanmax(y_coordinates)\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        return x_min, y_min, x_max, y_max, width, height\n",
    "\n",
    "\n",
    "    def calculate_iou(box1, box2):\n",
    "        x1_min, y1_min, x1_max, y1_max = box1\n",
    "        x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "        xi1 = max(x1_min, x2_min)\n",
    "        yi1 = max(y1_min, y2_min)\n",
    "        xi2 = min(x1_max, x2_max)\n",
    "        yi2 = min(y1_max, y2_max)\n",
    "        inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "        box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "        box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "    def calculate_center(keypoints):\n",
    "        if keypoints is None or len(keypoints) == 0 or not isinstance(keypoints[0], (list, np.ndarray)):\n",
    "            print(f\"Invalid keypoints format: {keypoints}\")\n",
    "            raise ValueError(\"Invalid keypoints format\")\n",
    "        x_coordinates = []\n",
    "        y_coordinates = []\n",
    "        for point in keypoints:\n",
    "            if isinstance(point, (list, np.ndarray)) and len(point) == 2:\n",
    "                if not np.isnan(point[0]) and not np.isnan(point[1]):\n",
    "                    x_coordinates.append(point[0])\n",
    "                    y_coordinates.append(point[1])\n",
    "            else:\n",
    "                print(f\"Invalid point format center: {point}\")\n",
    "        if len(x_coordinates) == 0 or len(y_coordinates) == 0:\n",
    "            return 0, 0  # Return zero width and height if all coordinates are NaN\n",
    "        center_x = np.mean(x_coordinates)\n",
    "        center_y = np.mean(y_coordinates)\n",
    "        return center_x, center_y\n",
    "\n",
    "    def calculate_diagonal(width, height):\n",
    "        return np.sqrt(width**2 + height**2)\n",
    "\n",
    "    def calculate_velocity(center1, center2):\n",
    "        return np.sqrt((center2[0] - center1[0])**2 + (center2[1] - center1[1])**2)\n",
    "\n",
    "    def calculate_mean_features(track):\n",
    "        areas = []\n",
    "        aspect_ratios = []\n",
    "        centers = []\n",
    "        diagonals = []\n",
    "        velocities = []\n",
    "        for i in range(1, len(track[-50:])):\n",
    "            detection, _ = track[-50:][i]\n",
    "            prev_detection, _ = track[-50:][i-1]\n",
    "            _, _, _, _, width, height = calculate_bounding_box(detection)\n",
    "            prev_center = calculate_center(prev_detection)\n",
    "            center = calculate_center(detection)\n",
    "            areas.append(width * height)\n",
    "            aspect_ratios.append(width / height if height != 0 else 0)\n",
    "            centers.append(center)\n",
    "            diagonals.append(calculate_diagonal(width, height))\n",
    "            velocities.append(calculate_velocity(prev_center, center))\n",
    "        return np.mean(areas), np.mean(aspect_ratios), np.mean(centers, axis=0), np.mean(diagonals), np.mean(velocities)\n",
    "\n",
    "    def calculate_score(detection, mean_area, mean_aspect_ratio, mean_center, mean_diagonal, mean_velocity, last_detection):\n",
    "        x_min, y_min, x_max, y_max, width, height = calculate_bounding_box(detection)\n",
    "        area = width * height\n",
    "        aspect_ratio = width / height if height != 0 else 0\n",
    "        center = calculate_center(detection)\n",
    "        diagonal = calculate_diagonal(width, height)\n",
    "        velocity = calculate_velocity(calculate_center(last_detection), center)\n",
    "        iou = calculate_iou((x_min, y_min, x_max, y_max), calculate_bounding_box(last_detection)[:4])\n",
    "        return (abs(area - mean_area) + abs(aspect_ratio - mean_aspect_ratio) +\n",
    "                np.linalg.norm(np.array(center) - np.array(mean_center)) + abs(diagonal - mean_diagonal) +\n",
    "                abs(velocity - mean_velocity) - iou)\n",
    "\n",
    "    track1, track2 = tracks[:2]\n",
    "    mean_features_track1 = calculate_mean_features(track1)\n",
    "    mean_features_track2 = calculate_mean_features(track2)\n",
    "\n",
    "    if len(detections) == 1:\n",
    "        detection = detections[0]\n",
    "        score_track1 = calculate_score(detection, *mean_features_track1, track1[-1][0])\n",
    "        score_track2 = calculate_score(detection, *mean_features_track2, track2[-1][0])\n",
    "        return 1 if score_track1 < score_track2 else 2\n",
    "\n",
    "    detection1, detection2 = detections[:2]\n",
    "    score_track1_detection1 = calculate_score(detection1, *mean_features_track1, track1[-1][0])\n",
    "    score_track2_detection1 = calculate_score(detection1, *mean_features_track2, track2[-1][0])\n",
    "    score_track1_detection2 = calculate_score(detection2, *mean_features_track1, track1[-1][0])\n",
    "    score_track2_detection2 = calculate_score(detection2, *mean_features_track2, track2[-1][0])\n",
    "\n",
    "    if score_track1_detection1 + score_track2_detection2 < score_track2_detection1 + score_track1_detection2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounding_box(keypoints):\n",
    "    x_coordinates = [point[0] for point in keypoints if not np.isnan(point[0])]\n",
    "    y_coordinates = [point[1] for point in keypoints if not np.isnan(point[1])]\n",
    "    if not x_coordinates or not y_coordinates:\n",
    "        return 0, 0  # Return zero width and height if all coordinates are NaN\n",
    "    x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
    "    y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    return width, height\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Initialize tracking variables\n",
    "tracks = []\n",
    "frames = []\n",
    "knn = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "threshold = 80\n",
    "# Example usage\n",
    "path = '/Users/andrei-macpro/Documents/Data/pose/play_openpose/1264_play'\n",
    "detects = sorted_alpha(path)\n",
    "numbers = [int(f.split('_')[-1].split('.')[0]) for f in detects]\n",
    "\n",
    "\n",
    "for i, number in zip(range(len(detects)), numbers):\n",
    "    frames.append(number)\n",
    "    print('frame no:', number)\n",
    "    \n",
    "    data = pd.read_csv(os.path.join(path, detects[i]))\n",
    "    \n",
    "    if i == 0:  # For the first frame, just get the skeletons, don't do anything\n",
    "        detections, necks = process_skeleton_data(os.path.join(path, detects[i]))\n",
    "        # Initialize track lists for each detection\n",
    "        for _ in range(len(detections)):\n",
    "            tracks.append([])\n",
    "\n",
    "        # Append each detection to its corresponding track list with the frame number\n",
    "        for j, detection in enumerate(detections):\n",
    "            tracks[j].append((detection, number))\n",
    "\n",
    "    else:\n",
    "        detections, necks = process_skeleton_data(os.path.join(path, detects[i]))\n",
    "\n",
    "        # Check if there are valid neck joints\n",
    "        if necks is None or len(necks) == 0 or all(neck is None for neck in necks):\n",
    "            continue\n",
    "        \n",
    "        # Get the last neck positions from the tracks\n",
    "        last_necks = np.array([track[-1][0][1] for track in tracks])\n",
    "\n",
    "        # Fit the KNN model on the last neck positions\n",
    "        knn.fit(last_necks)\n",
    "        # Keep track of which detections have been assigned\n",
    "        assigned_detections = set()\n",
    "        # Store distances and indices\n",
    "        distances_indices = []\n",
    "        # Compute Euclidean distances between each pair of new detections \n",
    "        necks_array = np.array(necks)\n",
    "        num_detections = len(necks_array)\n",
    "        distances = np.zeros((num_detections, num_detections))\n",
    "\n",
    "        for j in range(num_detections):\n",
    "            for k in range(j + 1, num_detections):\n",
    "                dist = euclidean(necks_array[j], necks_array[k])\n",
    "                distances[j, k] = dist\n",
    "                distances[k, j] = dist  # Symmetric matrix\n",
    "\n",
    "        # Handle the case where there is only one detection\n",
    "        if num_detections == 1:\n",
    "            if len(tracks) == 1:\n",
    "                tracks[0].append((detections[0], number))\n",
    "\n",
    "            else:\n",
    "                # Assign the single detection to the closest track\n",
    "                dist, index = knn.kneighbors([necks[0]])\n",
    "                tracks[index[0][0]].append((detections[0], number))\n",
    "\n",
    "        else:\n",
    "            # Check if distance between the first 2 detections is less than 80\n",
    "            if i > 50 and np.any(distances[0,1] < threshold):\n",
    "                if len(tracks) == 1:\n",
    "                    for j, neck in enumerate(necks):\n",
    "                        dist, index = knn.kneighbors([neck])\n",
    "                        distances_indices.append((dist[0][0], index[0][0], j))\n",
    "                        # Sort detections by distance to the closest track\n",
    "                        distances_indices.sort()\n",
    "                        # Assign the closest detections to the tracks\n",
    "                        for dist, closest_track_index, j in distances_indices[:len(tracks)]:\n",
    "                            tracks[closest_track_index].append((detections[j], number))\n",
    "                            assigned_detections.add(j)\n",
    "                else:\n",
    "                    relative_pos = relative_position(tracks, detections)\n",
    "                    \n",
    "\n",
    "                    if relative_pos == 1:\n",
    "                        tracks[0].append((detections[0], number))\n",
    "                        tracks[1].append((detections[1], number))\n",
    "                    else:\n",
    "                        tracks[0].append((detections[1], number))\n",
    "                        tracks[1].append((detections[0], number))\n",
    "            else:\n",
    "                # Find the closest track for each new detection\n",
    "                for j, neck in enumerate(necks):\n",
    "                    dist, index = knn.kneighbors([neck])\n",
    "                    distances_indices.append((dist[0][0], index[0][0], j))\n",
    "\n",
    "\n",
    "                # Sort detections by distance to the closest track\n",
    "                distances_indices.sort()\n",
    "                if len(detections) == 3 and len(tracks) == 2:\n",
    "                     # Assign the closest detections to the tracks using k-NN\n",
    "                    assigned_detections = set()\n",
    "                    for dist, closest_track_index, j in distances_indices[:2]:\n",
    "                        tracks[closest_track_index].append((detections[j], number))\n",
    "                        assigned_detections.add(j)\n",
    "\n",
    "          \n",
    "       \n",
    "                else:\n",
    "\n",
    "                    # Assign the closest detections to the tracks\n",
    "                    for dist, closest_track_index, j in distances_indices[:len(tracks)]:\n",
    "                        tracks[closest_track_index].append((detections[j], number))\n",
    "                        assigned_detections.add(j)\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example structure of a detection (assuming each detection is a list of keypoints, each keypoint is a list [x, y])\n",
    "# detections = [[[x1, y1], [x2, y2], ...], [[x1, y1], [x2, y2], ...], ...]\n",
    "\n",
    "# Flatten the tracks list and extract the detection, frame number, and track index\n",
    "flattened_tracks = [(detection, frame, track_index) for track_index, track in enumerate(tracks) for detection, frame in track]\n",
    "\n",
    "# Initialize a list to store the data for the DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate over the flattened tracks to extract keypoints\n",
    "for detection, frame, track_index in flattened_tracks:\n",
    "    for keypoint_index, (x, y) in enumerate(detection):\n",
    "        data.append({'Frame': frame, 'Track': track_index, 'Keypoint': keypoint_index, 'X': x, 'Y': y})\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df_tracks = pd.DataFrame(data)\n",
    "\n",
    "# Set the frame number, keypoint, and track as a multi-index\n",
    "df_tracks.set_index(['Frame', 'Keypoint', 'Track'], inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example structure of a detection (assuming each detection is a list of keypoints, each keypoint is a list [x, y])\n",
    "# detections = [[[x1, y1], [x2, y2], ...], [[x1, y1], [x2, y2], ...], ...]\n",
    "\n",
    "# Flatten the tracks list and extract the detection, frame number, and track index\n",
    "flattened_tracks = [(detection, frame, track_index) for track_index, track in enumerate(tracks) for detection, frame in track]\n",
    "\n",
    "# Initialize a list to store the data for the DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate over the flattened tracks to extract keypoints\n",
    "for detection, frame, track_index in flattened_tracks:\n",
    "    for keypoint_index, (x, y) in enumerate(detection):\n",
    "        data.append({'Frame': frame, 'Track': track_index, 'Keypoint': keypoint_index, 'X': x, 'Y': y})\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df_tracks = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to have 'Track' as columns and keypoints as rows\n",
    "df_pivot = df_tracks.pivot_table(index=['Frame', 'Keypoint'], columns='Track', values=['X', 'Y'])\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "df_pivot.columns = [f'{coord}_Track{track}' for coord, track in df_pivot.columns]\n",
    "\n",
    "# Reset the index to make 'Frame' and 'Keypoint' columns\n",
    "df_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def interpolate_missing_skeletons(tracks):\n",
    "    \"\"\"\n",
    "    Interpolates missing skeletons in the tracks and returns a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    tracks (list of list of tuples): The tracks where each track is a list of (detection, frame_number) tuples.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with interpolated skeletons, with tracks as columns.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for track_index, track in enumerate(tracks):\n",
    "        for detection, frame_number in track:\n",
    "            for keypoint_index, (x, y) in enumerate(detection):\n",
    "                data.append({'Frame': frame_number, 'Track': track_index, 'Keypoint': keypoint_index, 'X': x, 'Y': y})\n",
    "\n",
    "    # Create a DataFrame from the data\n",
    "    df_tracks = pd.DataFrame(data)\n",
    "\n",
    "    # Ensure all frames are present\n",
    "    all_frames = pd.DataFrame({'Frame': range(df_tracks['Frame'].min(), df_tracks['Frame'].max() + 1)})\n",
    "    df_tracks = all_frames.merge(df_tracks, on='Frame', how='left')\n",
    "\n",
    "    # Pivot the DataFrame to have 'Frame' and 'Keypoint' as index, and 'Track' as columns\n",
    "    df_pivot = df_tracks.pivot_table(index=['Frame', 'Keypoint'], columns='Track', values=['X', 'Y'])\n",
    "\n",
    "    # Interpolate missing values using linear interpolation\n",
    "    df_interpolated = df_pivot.groupby(level='Keypoint').apply(lambda group: group.interpolate(method='linear', limit_direction='both'))\n",
    "\n",
    "    # Apply Savitzky-Golay filter for smoothing\n",
    "    def apply_savgol_filter(group):\n",
    "        return group.apply(lambda col: savgol_filter(col, window_length=11, polyorder=3) if col.notna().sum() > 11 else col)\n",
    "\n",
    "    df_smoothed = df_interpolated.groupby(level='Keypoint').apply(apply_savgol_filter)\n",
    "\n",
    "    # Convert the Track level to integer\n",
    "    df_smoothed.columns = pd.MultiIndex.from_tuples([(coord, int(track)) for coord, track in df_smoothed.columns])\n",
    "\n",
    "    # Flatten the multi-level columns\n",
    "    df_smoothed.columns = [f'{coord}_Track{track}' for coord, track in df_smoothed.columns]\n",
    "\n",
    "    # Reset the index to make 'Frame' and 'Keypoint' columns\n",
    "    df_smoothed.reset_index(inplace=True)\n",
    "\n",
    "    return df_smoothed\n",
    "\n",
    "\n",
    "interpolated_df = interpolate_missing_skeletons(tracks)\n",
    "print(interpolated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## let's see how the interpolated tracks look like in a dataframe \n",
    "\n",
    "# Example structure of a detection (assuming each detection is a list of keypoints, each keypoint is a list [x, y])\n",
    "# detections = [[[x1, y1], [x2, y2], ...], [[x1, y1], [x2, y2], ...], ...]\n",
    "\n",
    "# Flatten the tracks list and extract the detection, frame number, and track index\n",
    "flattened_tracks = [(detection, frame, track_index) for track_index, track in enumerate(tracks) for detection, frame in track]\n",
    "\n",
    "# Initialize a list to store the data for the DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate over the flattened tracks to extract keypoints\n",
    "for detection, frame, track_index in flattened_tracks:\n",
    "    for keypoint_index, (x, y) in enumerate(detection):\n",
    "        data.append({'Frame': frame, 'Track': track_index, 'Keypoint': keypoint_index, 'X': x, 'Y': y})\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df_tracks = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to have 'Track' as columns and keypoints as rows\n",
    "df_pivot = df_tracks.pivot_table(index=['Frame', 'Keypoint'], columns='Track', values=['X', 'Y'])\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "df_pivot.columns = [f'{coord}_Track{track}' for coord, track in df_pivot.columns]\n",
    "\n",
    "# Reset the index to make 'Frame' and 'Keypoint' columns\n",
    "df_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## let's see how the interpolated tracks look like in a dataframe \n",
    "\n",
    "# Example structure of a detection (assuming each detection is a list of keypoints, each keypoint is a list [x, y])\n",
    "# detections = [[[x1, y1], [x2, y2], ...], [[x1, y1], [x2, y2], ...], ...]\n",
    "tracks = interpolated_tracks\n",
    "# Flatten the tracks list and extract the detection, frame number, and track index\n",
    "flattened_tracks = [(detection, frame, track_index) for track_index, track in enumerate(tracks) for detection, frame in track]\n",
    "\n",
    "# Initialize a list to store the data for the DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate over the flattened tracks to extract keypoints\n",
    "for detection, frame, track_index in flattened_tracks:\n",
    "    for keypoint_index, (x, y) in enumerate(detection):\n",
    "        data.append({'Frame': frame, 'Track': track_index, 'Keypoint': keypoint_index, 'X': x, 'Y': y})\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df_tracks = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to have 'Track' as columns and keypoints as rows\n",
    "df_pivot = df_tracks.pivot_table(index=['Frame', 'Keypoint'], columns='Track', values=['X', 'Y'])\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "df_pivot.columns = [f'{coord}_Track{track}' for coord, track in df_pivot.columns]\n",
    "\n",
    "# Reset the index to make 'Frame' and 'Keypoint' columns\n",
    "df_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.to_csv('interpolated_output_all_frames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_df.to_csv('interpolated_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now draw bounding boxes before imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/Users/andrei-macpro/Documents/Data/videos/play_videos/1264_play.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n",
    "# Get the width and height of the frames\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "output_path = '/Users/andrei-macpro/Documents/Data/videos/1264_play_annotated_multiple_.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Flag to toggle frame number display\n",
    "show_frame_number = True\n",
    "\n",
    "# Iterate over the video frames\n",
    "frame_count = 1\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get the rows for the current frame\n",
    "    frame_data = df_pivot[df_pivot['Frame'] == frame_count]\n",
    "\n",
    "    # Iterate over each track\n",
    "    for track in range(len(tracks)):\n",
    "        # Get the keypoints for the current track\n",
    "        keypoints = frame_data[[f'X_Track{track}', f'Y_Track{track}']].values\n",
    "\n",
    "        # Choose a color for the current track\n",
    "        color = colors[track % len(colors)]\n",
    "\n",
    "        # Draw bounding boxes around the keypoints and display track number\n",
    "        for x, y in keypoints:\n",
    "            if not pd.isna(x) and not pd.isna(y):\n",
    "                cv2.rectangle(frame, (int(x) - 5, int(y) - 5), (int(x) + 5, int(y) + 5), color, 2)\n",
    "                cv2.putText(frame, f'Track {track}', (int(x) + 10, int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame number if the flag is set\n",
    "    if show_frame_number:\n",
    "        cv2.putText(frame, f'Frame {frame_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_pivot to a csv file\n",
    "df_pivot.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next up: interpolation + see how it performs on harder ones\n",
    "tracks = interpolated_tracks\n",
    "\n",
    "video_path = '/Users/andrei-macpro/Documents/Data/videos/play_videos/1264_play.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n",
    "# Get the width and height of the frames\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "output_path = '/Users/andrei-macpro/Documents/Data/videos/1264_play_annotated_imputation.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Flag to toggle frame number display\n",
    "show_frame_number = True\n",
    "\n",
    "# Iterate over the video frames\n",
    "frame_count = 1\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get the rows for the current frame\n",
    "    frame_data = df_pivot[df_pivot['Frame'] == frame_count]\n",
    "\n",
    "    # Iterate over each track\n",
    "    for track in range(len(tracks)):\n",
    "        # Get the keypoints for the current track\n",
    "        keypoints = frame_data[[f'X_Track{track}', f'Y_Track{track}']].values\n",
    "\n",
    "        # Choose a color for the current track\n",
    "        color = colors[track % len(colors)]\n",
    "\n",
    "        # Draw bounding boxes around the keypoints and display track number\n",
    "        for x, y in keypoints:\n",
    "            if not pd.isna(x) and not pd.isna(y):\n",
    "                cv2.rectangle(frame, (int(x) - 5, int(y) - 5), (int(x) + 5, int(y) + 5), color, 2)\n",
    "                cv2.putText(frame, f'Track {track}', (int(x) + 10, int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame number if the flag is set\n",
    "    if show_frame_number:\n",
    "        cv2.putText(frame, f'Frame {frame_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/Users/andrei-macpro/Documents/Data/videos/play_videos/1264_play.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "output_path = '/Users/andrei-macpro/Documents/Data/videos/1264_play_annotated.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Flag to toggle frame number display\n",
    "show_frame_number = True\n",
    "\n",
    "# Iterate over the video frames\n",
    "frame_count = 1\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get the rows for the current frame\n",
    "    frame_data = interpolated_df[interpolated_df['Frame'] == frame_count]\n",
    "\n",
    "    # Iterate over each track\n",
    "    for track in range(len(tracks)):\n",
    "        # Get the keypoints for the current track\n",
    "        keypoints = frame_data[[f'X_Track{track}', f'Y_Track{track}']].values\n",
    "\n",
    "        # Choose a color for the current track\n",
    "        color = colors[track % len(colors)]\n",
    "\n",
    "        # Draw bounding boxes around the keypoints and display track number\n",
    "        for x, y in keypoints:\n",
    "            if not pd.isna(x) and not pd.isna(y):\n",
    "                cv2.rectangle(frame, (int(x) - 5, int(y) - 5), (int(x) + 5, int(y) + 5), color, 2)\n",
    "                cv2.putText(frame, f'Track {track}', (int(x) + 10, int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame number if the flag is set\n",
    "    if show_frame_number:\n",
    "        cv2.putText(frame, f'Frame {frame_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Increment the frame count\n",
    "    frame_count += 1\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
