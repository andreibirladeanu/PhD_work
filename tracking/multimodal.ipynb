{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# import dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meal = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/combined_features.csv', index_col=0)\n",
    "df_meal = df_meal.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])\n",
    "\n",
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df_meal['label'] = df_meal['label'].map({'no_rad': 0, 'rad': 1})\n",
    "df_play['label'] = df_play['label'].map({'no_rad': 0, 'rad': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reset the index\n",
    "df_meal = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_meal['group'] = df_meal['s_id'].str.split('_').str[0]\n",
    "df_grouped_meal = df_meal.groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_meal.index.name = 's_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_play = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_play['group'] = df_play['s_id'].str.split('_').str[0]\n",
    "df_grouped_play = df_play.groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_play.index.name = 's_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the grouped dataframes\n",
    "df_grouped_meal = df_grouped_meal.reset_index()\n",
    "df_grouped_play = df_grouped_play.reset_index()\n",
    "\n",
    "# Add a new 'group' column to each DataFrame\n",
    "df_grouped_meal['group'] = 'meal'\n",
    "df_grouped_play['group'] = 'play'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat([df_grouped_meal, df_grouped_play], axis=0)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop last column from final_df\n",
    "final_df = final_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.set_index('s_id', inplace=True)\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "final_df['label'] = final_df['label'].map({0: 'no_rad', 1: 'rad'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = pd.read_excel('/Users/andrei-macpro/Documents/Data/classification/speech/classification.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index name to s_id\n",
    "speech.index.name = 's_id'\n",
    "# remove age column from speech\n",
    "speech = speech.drop(columns=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a boolean mask where True indicates a duplicated index\n",
    "mask = speech.index.duplicated(keep='first')\n",
    "\n",
    "# Use np.where to assign 'meal' to the first occurrence and 'play' to the second\n",
    "speech['group'] = np.where(mask, 'play', 'meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([final_df, speech], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.set_index('group', append=True, inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.set_index('group', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "final_df.reset_index(inplace=True)\n",
    "\n",
    "# Convert the first level of the index to integers\n",
    "final_df.iloc[:, 0] = final_df.iloc[:, 0].astype(int)\n",
    "\n",
    "# Set the index again\n",
    "final_df.set_index(final_df.columns[:2].tolist(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'label' column from 'speech' dataframe\n",
    "speech = speech.drop('label', axis=1)\n",
    "\n",
    "# Concatenate 'final_df' and 'speech' dataframes\n",
    "df = pd.concat([final_df, speech], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'subject_id' is the column containing the subject IDs\n",
    "# Get the 's_id' values from the index of 'final_df'\n",
    "final_df_s_ids = final_df.index.get_level_values('s_id')\n",
    "\n",
    "# Keep only the rows in 'speech' where the 's_id' is also found in 'final_df'\n",
    "speech = speech[speech.index.get_level_values('s_id').isin(final_df_s_ids)]\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.reset_index()\n",
    "# set s_id as index for final_df\n",
    "final_df.set_index('s_id', inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "# set s_id as index for final_df\n",
    "df.set_index('s_id', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = df.drop(['label','group'], axis=1)\n",
    "y = df['label']\n",
    "groups = df.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {'lr__C': [0.01, 0.1, 1, 10, 100], 'lr__penalty': ['l1', 'l2'], 'lr__solver': ['liblinear', 'saga']}),\n",
    "    ('svc_linear', SVC(kernel='linear'), {'svc_linear__C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {'svc_rbf__C': [0.01, 0.1, 1, 10, 100], 'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('rf', RandomForestClassifier(), {'rf__n_estimators': [10, 50, 100, 200], 'rf__max_depth': [None, 5, 10, 15], 'rf__min_samples_split': [2, 5, 10]})\n",
    "]\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled)\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "feature_names = X.columns\n",
    "best_model_row = results_df.loc[results_df['best_score'].idxmax()]\n",
    "best_model_name = best_model_row['classifier']\n",
    "best_model_params = best_model_row['best_params']\n",
    "\n",
    "# Get the best model from the classifiers list\n",
    "best_model = [clf for name, clf, params in classifiers if name == best_model_name][0]\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create the RFE object and rank each pixel\n",
    "rfe = RFE(estimator=best_model, n_features_to_select=5, step=1)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Print the names of the most important features\n",
    "for i in range(X.shape[1]):\n",
    "    if rfe.support_[i]:\n",
    "        print('Feature: %s, Selected %s, Rank: %.3f' % (feature_names[i], rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN WITH THE MOST IMPORTANT FEATURES \n",
    "\n",
    "\n",
    "# Select only the important features\n",
    "selected_features = [i for i, x in enumerate(rfe.support_) if x]\n",
    "\n",
    "X_selected = X.iloc[:, selected_features]\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = X_selected\n",
    "y = df['label']\n",
    "groups = df.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {'lr__C': [0.01, 0.1, 1, 10, 100], 'lr__penalty': ['l1', 'l2'], 'lr__solver': ['liblinear', 'saga']}),\n",
    "    ('svc_linear', SVC(kernel='linear'), {'svc_linear__C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {'svc_rbf__C': [0.01, 0.1, 1, 10, 100], 'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('rf', RandomForestClassifier(), {'rf__n_estimators': [10, 50, 100, 200], 'rf__max_depth': [None, 5, 10, 15], 'rf__min_samples_split': [2, 5, 10]})\n",
    "]\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled)\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('rf', RandomForestClassifier(max_depth=5, min_samples_split=10, n_estimators=50))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = pipeline.named_steps['rf'].feature_importances_\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 10 features from importance_df\n",
    "importance_df[:10].plot(kind='bar', x='feature', y='importance', legend=False)\n",
    "\n",
    "#make the graph bigger\n",
    "plt.gcf().set_size_inches(10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make two different models and combine them \n",
    "\n",
    "speach_features = speech\n",
    "movement_features = final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first 37 columns from movement_features\n",
    "movement_features = movement_features.iloc[:, :39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the group index from speech\n",
    "X_speech = speech.reset_index(level='group', drop=True)\n",
    "y_speech = movement_features['label']\n",
    "groups_speech = speech.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PERFORMING GRID SEARCH FOR SPEECH DATASET\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "\n",
    "X = X_speech\n",
    "y = y_speech\n",
    "groups = groups_speech\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {'lr__C': [0.01, 0.1, 1, 10, 100], 'lr__penalty': ['l1', 'l2'], 'lr__solver': ['liblinear', 'saga']}),\n",
    "    ('svc_linear', SVC(kernel='linear'), {'svc_linear__C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {'svc_rbf__C': [0.01, 0.1, 1, 10, 100], 'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('rf', RandomForestClassifier(), {'rf__n_estimators': [10, 50, 100, 200], 'rf__max_depth': [None, 5, 10, 15], 'rf__min_samples_split': [2, 5, 10]})\n",
    "]\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled)\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GroupKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'X_movement', 'y_movement', and 'groups_movement' are your data\n",
    "X_shuffled, y_shuffled, groups_shuffled = shuffle(X_speech, y_speech, groups_speech, random_state=0)\n",
    "\n",
    "# Create a pipeline with preprocessing and SVC with linear kernel\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "# Define a GroupKFold cross-validator\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, average='macro'),\n",
    "           'recall': make_scorer(recall_score, average='macro'),\n",
    "           'f1': make_scorer(f1_score, average='macro')}\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_validate(pipeline, X_shuffled, y_shuffled, groups=groups_shuffled, cv=gkf, scoring=scoring)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movement = movement_features.drop(['label','group'], axis=1)\n",
    "y_movement = movement_features['label']\n",
    "groups_movement = movement_features.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GroupKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'X', 'y', and 'groups' are your data from speech\n",
    "\n",
    "# Create a pipeline with preprocessing and Random Forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define a GroupKFold cross-validator\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, average='macro'),\n",
    "           'recall': make_scorer(recall_score, average='macro'),\n",
    "           'f1': make_scorer(f1_score, average='macro')}\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_validate(pipeline, X_movement, y_movement, groups=groups_movement, cv=gkf, scoring=scoring)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming 'X1', 'y', 'groups' are your data for the Random Forest model\n",
    "# and 'X2' is your data for the SVC model\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "y = y_movement\n",
    "groups = groups_movement\n",
    "# Assuming 'X1', 'X2', 'y', 'groups' are your data for the Random Forest model and the SVC model\n",
    "\n",
    "# Create column names for each feature\n",
    "X_movement_columns = [f'X_movement_{i}' for i in range(X_movement.shape[1])]\n",
    "X_speech_columns = [f'X_speech_{i}' for i in range(X_speech.shape[1])]\n",
    "# Combine the features and the target into a single DataFrame\n",
    "df = pd.DataFrame(np.column_stack((X_movement, X_speech, y, groups)), columns=X_movement_columns + X_speech_columns + ['y', 'groups'])\n",
    "\n",
    "# Shuffle the data within each group\n",
    "df = df.groupby('groups').apply(shuffle).reset_index(drop=True)\n",
    "\n",
    "# Split the shuffled data back into features and target\n",
    "X1_shuffled = df[X_movement_columns].values\n",
    "X2_shuffled = df[X_speech_columns].values\n",
    "y_shuffled = df['y'].values\n",
    "groups_shuffled = df['groups'].values\n",
    "\n",
    "# The rest of your code...\n",
    "\n",
    "# Train the base models with standardized data\n",
    "rf = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "svc = make_pipeline(StandardScaler(), SVC(kernel='rbf', probability=True))\n",
    "\n",
    "# Combine the base models using a StackingClassifier\n",
    "estimators = [('rf', rf), ('svc', svc)]\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "# Define a GroupKFold cross-validator\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(stacking_clf, np.column_stack((X1_shuffled, X2_shuffled)), y, groups=groups, cv=gkf)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a GroupKFold cross-validator for the outer loop\n",
    "outer_cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# Initialize a list to store the scores of the outer loop\n",
    "outer_scores = []\n",
    "\n",
    "# Loop over the outer folds\n",
    "# Loop over the outer folds\n",
    "for train_index, test_index in outer_cv.split(X1_shuffled, y_shuffled, groups_shuffled):\n",
    "    # Split the data into training and test sets\n",
    "    X1_train, X1_test = X1_shuffled[train_index], X1_shuffled[test_index]\n",
    "    X2_train, X2_test = X2_shuffled[train_index], X2_shuffled[test_index]\n",
    "    y_train, y_test = y_shuffled[train_index], y_shuffled[test_index]\n",
    "    groups_train = groups_shuffled[train_index]\n",
    "\n",
    "    # Fit the first-stage models on the training set\n",
    "    rf.fit(X1_train, y_train)\n",
    "    svc.fit(X2_train, y_train)\n",
    "\n",
    "    # Use cross_val_predict to get cross-validated predictions of the first-stage models on the training set\n",
    "    rf_pred_train = cross_val_predict(rf, X1_train, y_train, groups=groups_train, cv=inner_cv, method='predict_proba')\n",
    "    svc_pred_train = cross_val_predict(svc, X2_train, y_train, groups=groups_train, cv=inner_cv, method='predict_proba')\n",
    "\n",
    "    # Train the meta-model on the first-stage predictions\n",
    "    meta_model.fit(np.column_stack((rf_pred_train, svc_pred_train)), y_train)\n",
    "\n",
    "    # Get the first-stage predictions on the test set\n",
    "    rf_pred_test = rf.predict_proba(X1_test)\n",
    "    svc_pred_test = svc.predict_proba(X2_test)\n",
    "\n",
    "    # Use the meta-model to make predictions on the first-stage predictions\n",
    "    y_pred = meta_model.predict(np.column_stack((rf_pred_test, svc_pred_test)))\n",
    "\n",
    "    # Calculate the accuracy of the predictions and append it to the list of scores\n",
    "    outer_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print the scores of the outer loop\n",
    "print(outer_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
