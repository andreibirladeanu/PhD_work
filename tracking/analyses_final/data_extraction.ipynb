{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sorted_alpha import sorted_alpha\n",
    "from moviepy.editor import VideoFileClip\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the first 2 minutes of each video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_name = '/Users/andrei-macpro/Documents/Data/openpose/play/tracking/tracking/1206_play_1'\n",
    "\n",
    "tracks = []\n",
    "f_names = sorted_alpha(folder_name)\n",
    "for filename in f_names:\n",
    "    # load the file in a pandas dataframe\n",
    "    print(filename)\n",
    "    filepath = os.path.join(folder_name, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    tracks.append(df)\n",
    "\n",
    "# Function to average x, y values for every 50 rows in the first 3000 frames \n",
    "def average_xy(df):\n",
    "    averaged_data = []\n",
    "    df = df[df.index < 3000]\n",
    "    \n",
    "    # Iterate through the DataFrame in chunks of 50 frames\n",
    "    for i in range(0, len(df), 50):\n",
    "        chunk = df.iloc[i:i+50]\n",
    "        \n",
    "        \n",
    "        # Calculate the average x and y values for the chunk\n",
    "        avg_x = chunk['x'].mean()\n",
    "        avg_y = chunk['y'].mean()\n",
    "        averaged_data.append({'frame': chunk.index[0], 'avg_x': avg_x, 'avg_y': avg_y})\n",
    "    \n",
    "    return pd.DataFrame(averaged_data)\n",
    "\n",
    "# Apply the function to each DataFrame in tracks\n",
    "averaged_tracks = [average_xy(df) for df in tracks]\n",
    "\n",
    "# Print the averaged tracks\n",
    "for i, avg_df in enumerate(averaged_tracks):\n",
    "    print(f\"Averaged DataFrame {i+1}:\\n\", avg_df)\n",
    "\n",
    "# Print the cleaned tracks\n",
    "for i, df in enumerate(tracks):\n",
    "    print(f\"Cleaned DataFrame {i+1}:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_tracks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(averaged_tracks[0], averaged_tracks[1], on='frame', suffixes=('_caregiver', '_child'))\n",
    "print(\"Aligned Positions Data:\")\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the differences between consecutive rows for 'avg_x_child' and 'avg_y_child'\n",
    "diff_x = combined['avg_x_child'].diff().dropna()\n",
    "diff_y = combined['avg_y_child'].diff().dropna()\n",
    "\n",
    "# Calculate the Euclidean distance\n",
    "child_movement = np.sqrt(diff_x**2 + diff_y**2)\n",
    "print(\"Euclidean differences between consecutive rows for 'avg_x_child' and 'avg_y_child':\")\n",
    "print(euclidean_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distances between avg x and y positions of caregiver and child\n",
    "diff_x = combined['avg_x_caregiver'] - combined['avg_x_child']\n",
    "diff_y = combined['avg_y_caregiver'] - combined['avg_y_child']\n",
    "\n",
    "# Calculate the Euclidean distance\n",
    "proximity = np.sqrt(diff_x**2 + diff_y**2)\n",
    "print(\"Euclidean distances between avg x and y positions of caregiver and child:\")\n",
    "print(euclidean_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now for all the files in the folder\n",
    "\n",
    "video_folder = '/Users/andrei-macpro/Documents/Data/videos/play_videos' \n",
    "\n",
    "durations = []\n",
    "file_names = []\n",
    "\n",
    "for folder_name in sorted(os.listdir(video_folder)):\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(video_folder, folder_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        clip = VideoFileClip(file_path)\n",
    "        print(clip.fps)\n",
    "        durations.append(clip.duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a way to align the recordings in terms of time\n",
    "durations_meal = pd.Series(durations)\n",
    "durations_meal.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_meal.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a way to align the recordings in terms of time\n",
    "durations_play = pd.Series(durations)\n",
    "durations_play.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_play.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features are child movement, caregiver movement, and proximity \n",
    "# it doesn't make sense to align the recordings of the same participant in terms of time\n",
    "# so find a way to combine them \n",
    "# also resample for same fps \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample all to 25 fps\n",
    "def resample_df(df, original_fps, target_fps):\n",
    "    # Convert frame indices to time-based index\n",
    "    df['time'] = pd.to_timedelta(df.index / original_fps, unit='s')\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    # Resample the data to the target fps\n",
    "    resample_interval = f'{int(1e9 / target_fps)}N'  # Nanoseconds interval\n",
    "    df_resampled = df.resample(resample_interval).mean().dropna()\n",
    "    \n",
    "    # Convert time-based index back to frame indices\n",
    "    df_resampled.index = (df_resampled.index.total_seconds() * target_fps).astype(int)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_folder = '/Users/andrei-macpro/Documents/Data/openpose/play/tracking/tracking/' \n",
    "video_folder = '/Users/andrei-macpro/Documents/Data/videos/play_videos'   # Assuming video files \n",
    "\n",
    "time_series = {}\n",
    "for folder_name in sorted(os.listdir(tracking_folder)):\n",
    "    print(folder_name)\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(tracking_folder, folder_name)\n",
    "    tracks = {}\n",
    "    clip = VideoFileClip(os.path.join(video_folder, folder_name+'.mp4',))\n",
    "    if clip.fps > 25:\n",
    "        for file in sorted(os.listdir(file_path)):\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            df_resampled = resample_df(df, original_fps=clip.fps, target_fps=25)\n",
    "            if folder_name not in tracks:\n",
    "                tracks[folder_name] = []\n",
    "            tracks[folder_name].append(df_resampled)\n",
    "        # Combine the resampled DataFrames\n",
    "        all_dfs = [df for dfs in tracks.values() for df in dfs]\n",
    "        combined = pd.concat(all_dfs, axis=1).dropna()\n",
    "        # Calculate the differences between consecutive rows for 'x_child' and 'y_child'\n",
    "        time_series[folder_name] = combined\n",
    "    else:\n",
    "        for file in sorted(os.listdir(file_path)):\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            if folder_name not in tracks:\n",
    "                tracks[folder_name] = []\n",
    "            \n",
    "            # Append the resampled DataFrame to the list\n",
    "            tracks[folder_name].append(df)\n",
    "        # Combine the resampled DataFrames\n",
    "        all_dfs = [df for dfs in tracks.values() for df in dfs]\n",
    "        # Concatenate the DataFrames\n",
    "        combined = pd.concat(all_dfs, axis=1).dropna()\n",
    "        time_series[folder_name] = combined\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Identify the longest sequence\n",
    "longest_sequence_key = max(time_series, key=lambda k: len(time_series[k]))\n",
    "longest_sequence = time_series[longest_sequence_key]\n",
    "\n",
    "# Align sequences to the longest sequence using DTW\n",
    "aligned_to_longest = {}\n",
    "\n",
    "for key, sequence in time_series.items():\n",
    "    print(f\"Aligning sequence for {key} to the longest sequence...\")\n",
    "    if key == longest_sequence_key:\n",
    "        aligned_to_longest[key] = sequence\n",
    "        continue\n",
    "    \n",
    "    # Perform DTW alignment\n",
    "    distance, path = fastdtw(sequence.values, longest_sequence.values, dist=euclidean)\n",
    "\n",
    "    # Create an aligned sequence based on the DTW path\n",
    "    aligned_sequence = pd.DataFrame(np.zeros_like(longest_sequence.values), columns=longest_sequence.columns)\n",
    "    \n",
    "    for (i, j) in path:\n",
    "        # Assign values from the sequence to the aligned sequence based on DTW path\n",
    "        aligned_sequence.iloc[j] += sequence.iloc[i]  # Use += to handle multiple mappings (DTW might map multiple i to the same j)\n",
    "\n",
    "    # Optionally interpolate missing values (though may not always be needed)\n",
    "    aligned_sequence = aligned_sequence.replace(0, np.nan).interpolate(method='linear', limit_direction='both')\n",
    "    aligned_to_longest[key] = aligned_sequence\n",
    "\n",
    "aligned_sequences_list = list(aligned_to_longest.values())\n",
    "# Compute the average reference sequence, ignoring NaN values\n",
    "average_reference = pd.DataFrame(np.nanmean(np.array([df.values for df in aligned_sequences_list]), axis=0),\n",
    "                                 columns=longest_sequence.columns)\n",
    "\n",
    "# If you prefer to keep rows without any data (NaNs at the end), you can fill them using interpolation or another method\n",
    "average_reference = average_reference.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# Align sequences to the average reference using DTW\n",
    "aligned_dict = {}\n",
    "\n",
    "for key, sequence in time_series.items():\n",
    "    print(f\"Aligning sequence for {key} to the average reference...\")\n",
    "\n",
    "    # Perform DTW alignment\n",
    "    distance, path = fastdtw(sequence.values, average_reference.values, dist=euclidean)\n",
    "\n",
    "    # Create an aligned sequence based on the DTW path\n",
    "    aligned_sequence = pd.DataFrame(np.zeros_like(average_reference.values), columns=average_reference.columns)\n",
    "    \n",
    "    for (i, j) in path:\n",
    "        # Assign values from the sequence to the aligned sequence based on DTW path\n",
    "        aligned_sequence.iloc[j] += sequence.iloc[i]\n",
    "\n",
    "    # Optionally interpolate missing values (though may not always be needed)\n",
    "    aligned_sequence = aligned_sequence.replace(0, np.nan).interpolate(method='linear', limit_direction='both')\n",
    "    aligned_dict[key] = aligned_sequence\n",
    "\n",
    "# Print the aligned features\n",
    "for key, aligned_sequence in aligned_dict.items():\n",
    "    print(f\"Aligned sequence for {key}:\")\n",
    "    print(aligned_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform dtw_barycenter_averaging with progress tracking\n",
    "def dtw_barycenter_averaging_with_progress(time_series, num_iterations=10):\n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "    # Convert your time series to a list of numpy arrays\n",
    "    time_series_array = [sequence.values for sequence in time_series.values()]\n",
    "\n",
    "    # Initialize a random or pre-defined sequence for the first iteration\n",
    "    print(\"Starting DTW Barycenter Averaging with progress tracking...\")\n",
    "    \n",
    "    # Add a progress bar using tqdm\n",
    "    with tqdm(total=num_iterations, desc=\"DTW Barycenter Averaging Iterations\") as pbar:\n",
    "        for iteration in range(num_iterations):\n",
    "            # Perform the averaging in each iteration\n",
    "            barycenter = dtw_barycenter_averaging(time_series_array)\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Print how long the process has taken so far\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Iteration {iteration + 1}/{num_iterations} completed. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    total_time = time.time() - start_time  # Calculate total elapsed time\n",
    "    print(f\"Total time for DTW Barycenter Averaging: {total_time:.2f} seconds.\")\n",
    "\n",
    "    return barycenter  # Return the final barycenter result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dtw_barycenter_averaging(time_series_dict, batch_size=10, num_iterations=10):\n",
    "    keys = list(time_series_dict.keys())\n",
    "    batched_barycenters = []\n",
    "    \n",
    "    # Divide time series into batches\n",
    "    for i in range(0, len(keys), batch_size):\n",
    "        batch_keys = keys[i:i+batch_size]\n",
    "        batch = {key: time_series_dict[key] for key in batch_keys}\n",
    "        \n",
    "        # Perform DTW barycenter averaging on the batch\n",
    "        batch_barycenter = dtw_barycenter_averaging_with_progress(batch, num_iterations)\n",
    "        batched_barycenters.append(batch_barycenter)\n",
    "    \n",
    "    # Average the barycenters of all batches\n",
    "    final_barycenter = np.mean(batched_barycenters, axis=0)\n",
    "    return final_barycenter\n",
    "\n",
    "# Run batch processing\n",
    "average_sequence = batch_dtw_barycenter_averaging(time_series, batch_size=10, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the longest sequence\n",
    "longest_sequence_key = max(time_series, key=lambda k: len(time_series[k]))\n",
    "longest_sequence = time_series[longest_sequence_key]\n",
    "\n",
    "# Align sequences to the longest sequence using DTW\n",
    "aligned_to_longest = {}\n",
    "\n",
    "for key, sequence in time_series.items():\n",
    "    print(f\"Aligning sequence for {key} to the longest sequence...\")\n",
    "    if key == longest_sequence_key:\n",
    "        aligned_to_longest[key] = sequence\n",
    "        continue\n",
    "    \n",
    "    # Perform DTW alignment\n",
    "    distance, path = fastdtw(sequence.values, longest_sequence.values, dist=euclidean)\n",
    "\n",
    "    # Create an aligned sequence based on the DTW path\n",
    "    aligned_sequence = pd.DataFrame(np.zeros_like(longest_sequence.values), columns=longest_sequence.columns)\n",
    "    \n",
    "    for (i, j) in path:\n",
    "        # Assign values from the sequence to the aligned sequence based on DTW path\n",
    "        aligned_sequence.iloc[j] += sequence.iloc[i]  # Use += to handle multiple mappings (DTW might map multiple i to the same j)\n",
    "\n",
    "    # Interpolate missing values and fill remaining NaNs with the mean of the column\n",
    "    aligned_sequence = aligned_sequence.replace(0, np.nan).interpolate(method='linear', limit_direction='both').fillna(aligned_sequence.mean())\n",
    "    aligned_to_longest[key] = aligned_sequence\n",
    "\n",
    "# Ensure all aligned sequences have the same length as the longest sequence\n",
    "for key, aligned_sequence in aligned_to_longest.items():\n",
    "    if len(aligned_sequence) < len(longest_sequence):\n",
    "        # Pad the sequence with NaNs to match the length of the longest sequence\n",
    "        padding = pd.DataFrame(np.nan, index=range(len(longest_sequence) - len(aligned_sequence)), columns=aligned_sequence.columns)\n",
    "        aligned_to_longest[key] = pd.concat([aligned_sequence, padding], ignore_index=True)\n",
    "\n",
    "# Print the aligned features for verification\n",
    "for key, aligned_sequence in aligned_to_longest.items():\n",
    "    print(f\"Aligned sequence for {key}:\")\n",
    "    print(aligned_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without the averaging\n",
    "aligned_to_longest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "for key, df in aligned_to_longest.items():\n",
    "    # Calculate the differences between consecutive rows for 'x_child' and 'y_child'\n",
    "    diff_x_child = df['x_child'].diff().dropna()\n",
    "    diff_y_child = df['y_child'].diff().dropna()\n",
    "    child_movement = np.sqrt(diff_x_child**2 + diff_y_child**2)\n",
    "    \n",
    "    # Calculate the differences between consecutive rows for 'x_cg' and 'y_cg'\n",
    "    diff_x_cg = df['x_cg'].diff().dropna()\n",
    "    diff_y_cg = df['y_cg'].diff().dropna()\n",
    "    caregiver_movement = np.sqrt(diff_x_cg**2 + diff_y_cg**2)\n",
    "    \n",
    "    # Calculate proximity\n",
    "    diff_x_proximity = df['x_cg'] - df['x_child']\n",
    "    diff_y_proximity = df['y_cg'] - df['y_child']\n",
    "    proximity = np.sqrt(diff_x_proximity**2 + diff_y_proximity**2)\n",
    "    \n",
    "    # Combine the features into a single DataFrame\n",
    "    features_df = pd.concat([child_movement, caregiver_movement, proximity], axis=1).dropna()\n",
    "    features_df.columns = ['child_movement', 'caregiver_movement', 'proximity']\n",
    "    \n",
    "    # Store the features DataFrame in features_dict\n",
    "    features_dict[key] = features_df\n",
    "# Print the features for verification\n",
    "for key, features_df in features_dict.items():\n",
    "    print(f\"Features for {key}:\")\n",
    "    print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict['1043_play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last 2 minutes of each df\n",
    "features_first_2min = {}\n",
    "for key, df in features_dict.items():\n",
    "    features_first_2min[key] = df[df.index <= 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_middle_2min = {}\n",
    "frames_in_2_minutes = 3000\n",
    "\n",
    "for key, df in features_dict.items():\n",
    "    total_frames = len(df)\n",
    "    middle_index = total_frames // 2\n",
    "    start_index = max(0, middle_index - frames_in_2_minutes // 2)\n",
    "    end_index = min(total_frames, middle_index + frames_in_2_minutes // 2)\n",
    "    features_middle_2min[key] = df.iloc[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_last_2min = {}\n",
    "for key, df in features_dict.items():\n",
    "    max_index = df.index.max()\n",
    "    features_last_2min[key] = df[df.index >= max_index - 3000]\n",
    "features_last_2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_first_2min = {key: df.groupby(df.index // 50).mean() for key, df in features_first_2min.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_last_2min = {key: df.groupby(df.index // 50).mean() for key, df in features_last_2min.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_middle_2min = {key: df.groupby(df.index // 50).mean() for key, df in features_middle_2min.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 50\n",
    "num_groups = 23679 // group_size\n",
    "features_dict = {key: df.groupby(np.arange(len(df)) // group_size).mean() for key, df in features_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all DataFrames in features_dict are of the same length\n",
    "def check_same_length(features_dict):\n",
    "    lengths = [len(df) for df in features_dict.values()]\n",
    "    return all(length == lengths[0] for length in lengths)\n",
    "\n",
    "# Assuming features_dict is already defined\n",
    "if check_same_length(features_dict):\n",
    "    print(\"All DataFrames in features_dict are of the same length.\")\n",
    "else:\n",
    "    print(\"DataFrames in features_dict have different lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each DataFrame and combine them into a single DataFrame\n",
    "flattened_dict = {key: df.values.flatten() for key, df in features_dict.items()}\n",
    "flattened_df = pd.DataFrame(flattened_dict).T  # Transpose to have sequences as rows\n",
    "print(flattened_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df.index = flattened_df.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Print the original shape of flattened_df for reference\n",
    "print(\"Original shape of flattened_df:\", flattened_df.shape)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(flattened_df.values)\n",
    "\n",
    "# Apply PCA to reduce the number of columns/features by 99%\n",
    "n_components = round(flattened_df.shape[1] - 99/100*flattened_df.shape[1])  # Retain 10% of the features\n",
    "#n_components = 18\n",
    "pca = PCA(n_components=n_components) # Retain 10% of the variance\n",
    "pca_transformed = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Convert the PCA-transformed data back to a DataFrame\n",
    "pca_transformed_df = pd.DataFrame(pca_transformed, columns=[f'PC{i+1}' for i in range(pca_transformed.shape[1])])\n",
    "\n",
    "# Print the shape of the PCA-transformed DataFrame to verify the reduction\n",
    "print(\"Shape of PCA-transformed DataFrame:\", pca_transformed_df.shape)\n",
    "\n",
    "# Print the PCA-transformed DataFrame\n",
    "print(\"PCA-transformed DataFrame:\")\n",
    "print(pca_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformed_df.index = flattened_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play.index = df_play.index.str.split('_').str[0].astype(int)\n",
    "pca_transformed_df['label'] = df_play['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_transformed_df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformed_df.to_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/pca_proximity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## question: i've got 3 time series: child movement, caregiver movement, and proximity\n",
    "## do i use all of them in combination or each one separately? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pca_transformed_df\n",
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df['label'] = df['label'].map({'no_rad': 0, 'rad': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search for each classifier\n",
    "X = df.drop(['label'], axis=1)\n",
    "y = df['label']\n",
    "groups = df.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "('lr', LogisticRegression(), {'lr__C': [0.01, 0.1, 1, 10, 100], 'lr__penalty': ['l1', 'l2'], 'lr__solver': ['liblinear', 'saga']}),\n",
    "    ('svc_linear', SVC(kernel='linear'), {'svc_linear__C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {'svc_rbf__C': [0.01, 0.1, 1, 10, 100], 'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('rf', RandomForestClassifier(), {'rf__n_estimators': [10, 50, 100, 200], 'rf__max_depth': [None, 5, 10, 15], 'rf__min_samples_split': [2, 5, 10]})\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled)\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
