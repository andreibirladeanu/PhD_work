{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# import dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meal Speech vs Meal Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meal = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/combined_features.csv', index_col=0)\n",
    "df_meal = df_meal.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])\n",
    "\n",
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df_meal['label'] = df_meal['label'].map({'no_rad': 0, 'rad': 1})\n",
    "df_play['label'] = df_play['label'].map({'no_rad': 0, 'rad': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_meal = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_meal['group'] = df_meal['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_meal = df_meal.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "#df_grouped_meal['s_id'] = df_meal.groupby('group')['s_id'].first()\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "\n",
    "# Reset the index\n",
    "df_play = df_play.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_play['group'] = df_play['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_play = df_play.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "df_grouped_play.index.name = 's_id'\n",
    "\n",
    "\n",
    "# Reset the index of the grouped dataframes\n",
    "#df_grouped_meal = df_grouped_meal.reset_index()\n",
    "#df_grouped_play = df_grouped_play.reset_index()\n",
    "\n",
    "# Add a new 'group' column to each DataFrame\n",
    "df_grouped_meal['group'] = 'meal'\n",
    "df_grouped_play['group'] = 'play'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = pd.read_excel('/Users/andrei-macpro/Documents/Data/classification/speech/classification.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index name to s_id\n",
    "speech.index.name = 's_id'\n",
    "# remove age column from speech\n",
    "speech = speech.drop(columns=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where True indicates a duplicated index\n",
    "mask = speech.index.duplicated(keep='first')\n",
    "\n",
    "# Use np.where to assign 'meal' to the first occurrence and 'play' to the second\n",
    "speech['group'] = np.where(mask, 'play', 'meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.set_index('group', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['label'] = speech['label'].map({'no_rad': 0, 'rad': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only meal group from speech index\n",
    "speech_meal = speech.xs('meal', level='group')\n",
    "speech_play = speech.xs('play', level='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_grouped_meal = df_grouped_meal.reset_index(level=0, drop=True)\n",
    "meal_data = pd.concat([df_grouped_meal, speech_meal], axis=1)\n",
    "meal_data = meal_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data = pd.concat([df_grouped_play, speech_play], axis=1)\n",
    "#meal_data = meal_data.drop(columns=['index'])\n",
    "play_data = play_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just movement\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = meal_data[[\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max']]\n",
    "y = meal_data['label'].iloc[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5)\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf', probability=True), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    }),\n",
    "    ('mlp', MLPClassifier(max_iter=500), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'mlp__hidden_layer_sizes': [(3,), (6,), (3, 3)],\n",
    "        'mlp__activation': ['tanh', 'relu'],\n",
    "        'mlp__alpha': [0.0001, 0.001, 0.01],\n",
    "        'mlp__solver': ['adam', 'lbfgs']\n",
    "    })\n",
    "]\n",
    "    \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled = shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                             ('pca', PCA()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro',  n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "         # Extract the number of PCA components from the best parameters\n",
    "        best_pca_components = grid_search.best_params_.get('pca__n_components', None)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),   \n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': best_pca_components\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_movement = best_models['svc_rbf']['best_estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just speech\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = play_data[['Proportion speech child']]\n",
    "y = play_data['label'].iloc[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5)\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear', probability=True), {\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf', probability=True), {\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    })\n",
    "]\n",
    "    \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled = shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                            (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro',  n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "         # Extract the number of PCA components from the best parameters\n",
    "        best_pca_components = grid_search.best_params_.get('pca__n_components', None)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),   \n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': best_pca_components\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_speech = best_models['svc_rbf']['best_estimator']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the subset samples from both meal and play "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data_selected = meal_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max', 'group','label']]\n",
    "play_data_selected = play_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max', 'group', 'label']]\n",
    "\n",
    "# Find the common indices\n",
    "common_indices = meal_data_selected.index.intersection(play_data_selected.index)\n",
    "\n",
    "# Filter the DataFrames to keep only the common indices\n",
    "meal_data_filtered = meal_data_selected.loc[common_indices]\n",
    "play_data_filtered = play_data_selected.loc[common_indices]\n",
    "\n",
    "# Concatenate the filtered DataFrames\n",
    "combined_data = pd.concat([meal_data_filtered, play_data_filtered], axis=1)\n",
    "combined_data = combined_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meal_data_filtered), len(play_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlations between the two models \n",
    "# Get predictions from the LR model\n",
    "movement_features = meal_data_filtered[[\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max']]\n",
    "speech_features = play_data_filtered[[\n",
    "         'Proportion speech child',\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_features), len(movement_features), len(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "predictions_speech = cross_val_predict(model_speech, speech_features, combined_data['label'].iloc[:,0], cv=cv)\n",
    "predictions_movement = cross_val_predict(model_movement, movement_features, combined_data['label'].iloc[:,0], cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreements = np.sum(predictions_speech == predictions_movement)\n",
    "\n",
    "# Calculate the percentage of agreements\n",
    "percentage_agreement = (agreements / len(predictions_speech)) * 100\n",
    "\n",
    "print(f'Percentage of agreement: {percentage_agreement:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, binomtest\n",
    "# Perform a binomial test to determine if the agreement rate is significantly higher than chance\n",
    "# The null hypothesis is that the agreement rate is 50% (random chance)\n",
    "p_value_agreement = binomtest(agreements, len(predictions_speech), p=0.5, alternative='greater')\n",
    "\n",
    "# Print the agreement rate and the p-value\n",
    "print(f'P-value for agreement rate being higher than chance: {p_value_agreement}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    predictions_speech = cross_val_predict(model_speech, speech_features, combined_data['label'].iloc[:,0], cv=cv)\n",
    "    predictions_movement = cross_val_predict(model_movement, movement_features, combined_data['label'].iloc[:,0], cv=cv)\n",
    "    agreements = np.sum(predictions_speech == predictions_movement)\n",
    "    p_value_agreement = binomtest(agreements, len(predictions_speech), p=0.5, alternative='greater').pvalue\n",
    "    p_values.append(p_value_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean p-value: {np.mean(p_values)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tComparing misclassifications between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels\n",
    "true_labels = combined_data['label'].iloc[:,0]\n",
    "\n",
    "# Identify misclassifications\n",
    "misclassifications_speech = predictions_speech != true_labels\n",
    "misclassifications_movement = predictions_movement != true_labels\n",
    "\n",
    "# Compare misclassifications\n",
    "speech_correct_movement_wrong = (predictions_speech == true_labels) & (predictions_movement != true_labels)\n",
    "movement_correct_speech_wrong = (predictions_movement == true_labels) & (predictions_speech != true_labels)\n",
    "\n",
    "# Calculate proportions\n",
    "total_samples = len(true_labels)\n",
    "proportion_speech_correct_movement_wrong = np.sum(speech_correct_movement_wrong) / total_samples\n",
    "proportion_movement_correct_speech_wrong = np.sum(movement_correct_speech_wrong) / total_samples\n",
    "\n",
    "# Print results \n",
    "print(f'Proportion of cases where speech model is correct but movement model is wrong: {proportion_speech_correct_movement_wrong:.2%}')\n",
    "print(f'Proportion of cases where movement model is correct but speech model is wrong: {proportion_movement_correct_speech_wrong:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "contingency_table = np.array([\n",
    "    [np.sum((predictions_speech == true_labels) & (predictions_movement == true_labels)), np.sum(speech_correct_movement_wrong)],\n",
    "    [np.sum(movement_correct_speech_wrong), np.sum((predictions_speech != true_labels) & (predictions_movement != true_labels))]\n",
    "])\n",
    "\n",
    "# Perform McNemar's test\n",
    "result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(f'McNemar\\'s test statistic: {result.statistic}')\n",
    "print(f'P-value: {result.pvalue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "predictions_speech_proba = cross_val_predict(model_speech, speech_features, combined_data['label'].iloc[:,0], cv=cv, method='predict_proba')\n",
    "predictions_movement_proba = cross_val_predict(model_movement, movement_features, combined_data['label'].iloc[:,0], cv=cv, method='predict_proba')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# Calculate the Pearson correlation coefficient\n",
    "predictions_speech = predictions_speech_proba[:, 1]\n",
    "predictions_movement = predictions_movement_proba[:, 1]\n",
    "\n",
    "correlation, p_value = pearsonr(predictions_speech, predictions_movement)\n",
    "\n",
    "# Print the results\n",
    "print(f'Pearson correlation coefficient: {correlation:.2f}')\n",
    "print(f'P-value: {p_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
