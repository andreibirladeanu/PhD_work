{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# import dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meal = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/combined_features.csv', index_col=0)\n",
    "df_meal = df_meal.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])\n",
    "\n",
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df_meal['label'] = df_meal['label'].map({'no_rad': 0, 'rad': 1})\n",
    "df_play['label'] = df_play['label'].map({'no_rad': 0, 'rad': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_meal = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_meal['group'] = df_meal['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_meal = df_meal.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "#df_grouped_meal['s_id'] = df_meal.groupby('group')['s_id'].first()\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "\n",
    "# Reset the index\n",
    "df_play = df_play.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_play['group'] = df_play['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_play = df_play.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "df_grouped_play.index.name = 's_id'\n",
    "\n",
    "\n",
    "# Reset the index of the grouped dataframes\n",
    "df_grouped_meal = df_grouped_meal.reset_index()\n",
    "df_grouped_play = df_grouped_play.reset_index()\n",
    "\n",
    "# Add a new 'group' column to each DataFrame\n",
    "df_grouped_meal['group'] = 'meal'\n",
    "df_grouped_play['group'] = 'play'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat([df_grouped_meal, df_grouped_play], axis=0)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df.set_index('s_id', inplace=True)\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.set_index('group', append=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "final_df.reset_index(inplace=True)\n",
    "\n",
    "# Convert the first level of the index to integers\n",
    "final_df.iloc[:, 0] = final_df.iloc[:, 0].astype(int)\n",
    "\n",
    "# Set the index again\n",
    "final_df.set_index(final_df.columns[:2].tolist(), inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = pd.read_excel('/Users/andrei-macpro/Documents/Data/classification/speech/classification.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index name to s_id\n",
    "speech.index.name = 's_id'\n",
    "# remove age column from speech\n",
    "speech = speech.drop(columns=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where True indicates a duplicated index\n",
    "mask = speech.index.duplicated(keep='first')\n",
    "\n",
    "# Use np.where to assign 'meal' to the first occurrence and 'play' to the second\n",
    "speech['group'] = np.where(mask, 'play', 'meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.set_index('group', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['label'] = speech['label'].map({'no_rad': 0, 'rad': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = speech.reset_index()\n",
    "# set s_id as index for final_df\n",
    "speech.set_index('s_id', inplace=True)\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan rows from speech\n",
    "speech = speech.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_df = final_df.reset_index()\n",
    "final_df.set_index(['group', 's_id'], inplace=True)\n",
    "speech = speech.reset_index()\n",
    "speech.set_index(['group', 's_id'], inplace=True)\n",
    "# drop label column from final_df\n",
    "final_df = final_df.drop(columns=['label'])\n",
    "multimodal = pd.concat([speech, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal = multimodal.dropna()\n",
    "multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal = multimodal.reset_index()\n",
    "multimodal.set_index(['s_id'], inplace=True)\n",
    "multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Custom transformer to apply PCA to a subset of features\n",
    "class SubsetPCA(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, subset_features, n_components):\n",
    "        self.subset_features = subset_features\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        data_subset = data[self.subset_features]\n",
    "        self.pca.fit(data_subset)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        # Ensure X is a DataFrame\n",
    "        data_subset = data[self.subset_features]\n",
    "        data_pca = self.pca.transform(data_subset)\n",
    "        data_remaining = data.drop(columns=self.subset_features)\n",
    "        data_pca_df = pd.DataFrame(data_pca, index=data_remaining.index)\n",
    "        return pd.concat([data_remaining, data_pca_df], axis=1)\n",
    "\n",
    "# Example data\n",
    "X = multimodal[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "y = multimodal['label']\n",
    "groups = multimodal.index\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "remaining_features = [feat for feat in X.columns if feat not in subset_pca]\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', SubsetPCA(subset_features=subset_pca, n_components=4), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'preprocessor__pca_pipeline__n_components': [2, 4],\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear'), {\n",
    "        'preprocessor__pca_pipeline__n_components': [2, 4],\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {\n",
    "        'preprocessor__pca_pipeline__n_components': [2, 4],\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'preprocessor__pca_pipeline__n_components': [2, 4],\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'preprocessor__pca_pipeline__n_components': [2, 4],\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'preprocessor__pca_pipeline__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    }),\n",
    "    ('mlp', MLPClassifier(max_iter=500), {\n",
    "        'preprocessor__pca_pipeline__n_components': [2, 4],\n",
    "        'mlp__hidden_layer_sizes': [(3,), (6,), (3, 3)],\n",
    "        'mlp__activation': ['tanh', 'relu'],\n",
    "        'mlp__alpha': [0.0001, 0.001, 0.01],\n",
    "        'mlp__solver': ['adam', 'lbfgs']\n",
    "    })\n",
    "]\n",
    "  \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled= shuffle(X, y,groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([ ('preprocessor', preprocessor), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf,groups=groups_shuffled, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled,groups=groups_shuffled, cv=gkf, scoring='precision_macro', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, groups=groups_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        best_pca_components = grid_search.best_params_.get('preprocessor__pca_pipeline__n_components', None)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': best_pca_components\n",
    "            \n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.iloc[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a model just for speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = speech.iloc[:,1:-1]\n",
    "y = speech['label']\n",
    "groups = speech.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear'), {\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    }),\n",
    "]\n",
    "    \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                              (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled,n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled, n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled, n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',groups=groups_shuffled,  n_jobs=-1)\n",
    "         # Extract the number of PCA components from the best parameters\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "\n",
    "        })\n",
    "        # Store the best model and its parameters\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "            best_models[name] = {\n",
    "                'best_estimator': grid_search.best_estimator_,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_\n",
    "            }\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier, model_info in best_models.items():\n",
    "    print(f\"Classifier: {classifier}\")\n",
    "    print(f\"Best Score: {model_info['best_score']}\")\n",
    "    print(f\"Best Parameters: {model_info['best_params']}\")\n",
    "    print(f\"Best Estimator: {model_info['best_estimator']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best parameters\n",
    "best_params = {'lr__C': 1, 'lr__penalty': 'l1', 'lr__solver': 'liblinear'}\n",
    "\n",
    "# Create the pipeline with the best parameters\n",
    "best_pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=best_params['lr__C'], penalty=best_params['lr__penalty'], solver=best_params['lr__solver']))\n",
    "])\n",
    "X = speech[['Proportion speech child']]\n",
    "y = speech['label']\n",
    "groups = speech.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "# Fit the pipeline\n",
    "\n",
    "f1_score = cross_val_score(best_pipeline_lr, X, y, cv=gkf, scoring='f1_macro', groups=groups,n_jobs=-1)\n",
    "precision_score = cross_val_score(best_pipeline_lr, X, y, cv=gkf, scoring='precision_macro', groups=groups, n_jobs=-1)\n",
    "recall_score = cross_val_score(best_pipeline_lr, X, y, cv=gkf, scoring='recall_macro', groups=groups, n_jobs=-1)\n",
    "accuracy_score = cross_val_score(best_pipeline_lr, X, y, cv=gkf, scoring='accuracy', groups=groups, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 Score: {f1_score.mean()}\")\n",
    "print(f\"Precision: {precision_score.mean()}\")\n",
    "print(f\"Recall: {recall_score.mean()}\")\n",
    "print(f\"Accuracy: {accuracy_score.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting a model just for movement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.reset_index()\n",
    "final_df.set_index(['s_id'], inplace=True)\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = final_df[[\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y = final_df['label']\n",
    "groups = final_df.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    }),\n",
    "    ('mlp', MLPClassifier(max_iter=500), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'mlp__hidden_layer_sizes': [(3,), (6,), (3, 3)],\n",
    "        'mlp__activation': ['tanh', 'relu'],\n",
    "        'mlp__alpha': [0.0001, 0.001, 0.01],\n",
    "        'mlp__solver': ['adam', 'lbfgs']\n",
    "    })\n",
    "]\n",
    "    \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                             ('pca', PCA()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled,n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled, n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled, n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy', groups=groups_shuffled, n_jobs=-1)\n",
    "         # Extract the number of PCA components from the best parameters\n",
    "        best_pca_components = grid_search.best_params_.get('pca__n_components', None)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': best_pca_components\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "            best_models[name] = {\n",
    "                'best_estimator': grid_search.best_estimator_,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_\n",
    "            }\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier, model_info in best_models.items():\n",
    "    print(f\"Classifier: {classifier}\")\n",
    "    print(f\"Best Score: {model_info['best_score']}\")\n",
    "    print(f\"Best Parameters: {model_info['best_params']}\")\n",
    "    print(f\"Best Estimator: {model_info['best_estimator']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best parameters\n",
    "best_params = {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'pca__n_components': 4}\n",
    "\n",
    "# Create the pipeline with the best parameters\n",
    "best_pipeline_mlp = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=best_params['pca__n_components'])),\n",
    "    ('mlp', MLPClassifier(activation=best_params['mlp__activation'], alpha=best_params['mlp__alpha'], hidden_layer_sizes=best_params['mlp__hidden_layer_sizes']))\n",
    "])\n",
    "\n",
    "# Define the features and target\n",
    "X = final_df[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "              'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y = final_df['label']\n",
    "groups = final_df.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Evaluate the pipeline using cross-validation\n",
    "f1_scores = cross_val_score(best_pipeline_mlp, X, y, cv=gkf, scoring='f1_macro', groups=groups, n_jobs=-1)\n",
    "precision_scores = cross_val_score(best_pipeline_mlp, X, y, cv=gkf, scoring='precision_macro', groups=groups, n_jobs=-1)\n",
    "recall_scores = cross_val_score(best_pipeline_mlp, X, y, cv=gkf, scoring='recall_macro', groups=groups, n_jobs=-1)\n",
    "accuracy_scores = cross_val_score(best_pipeline_mlp, X, y, cv=gkf, scoring='accuracy', groups=groups, n_jobs=-1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"F1 Score: {f1_scores.mean()}\")\n",
    "print(f\"Precision: {precision_scores.mean()}\")\n",
    "print(f\"Recall: {recall_scores.mean()}\")\n",
    "print(f\"Accuracy: {accuracy_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.reset_index()\n",
    "final_df.set_index(['group', 's_id'], inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assuming multimodal is the merged dataset with common participants\n",
    "# Define features for the LR model (speech features)\n",
    "X_speech = multimodal[['Proportion speech child']]\n",
    "y_speech = multimodal['label']\n",
    "\n",
    "# Define features for the MLP model (movement features)\n",
    "X_movement = multimodal[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "                         'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y_movement = multimodal['label']\n",
    "\n",
    "# Define the best parameters for LR and MLP models\n",
    "best_params_lr = {'lr__C': 1, 'lr__penalty': 'l1', 'lr__solver': 'liblinear'}\n",
    "best_params_mlp = {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'pca__n_components': 4}\n",
    "\n",
    "# Create the LR pipeline with the best parameters\n",
    "best_pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=best_params_lr['lr__C'], penalty=best_params_lr['lr__penalty'], solver=best_params_lr['lr__solver']))\n",
    "])\n",
    "\n",
    "# Create the MLP pipeline with the best parameters\n",
    "best_pipeline_mlp = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=best_params_mlp['pca__n_components'])),\n",
    "    ('mlp', MLPClassifier(activation=best_params_mlp['mlp__activation'], alpha=best_params_mlp['mlp__alpha'], hidden_layer_sizes=best_params_mlp['mlp__hidden_layer_sizes']))\n",
    "])\n",
    "\n",
    "# Fit the pipelines to the respective datasets\n",
    "best_pipeline_lr.fit(X_speech, y_speech)\n",
    "best_pipeline_mlp.fit(X_movement, y_movement)\n",
    "\n",
    "# Get predictions from the LR model\n",
    "lr_predictions = best_pipeline_lr.predict_proba(X_speech)[:, 1]\n",
    "\n",
    "# Get predictions from the MLP model\n",
    "mlp_predictions = best_pipeline_mlp.predict_proba(X_movement)[:, 1]\n",
    "\n",
    "# Compute the correlation between the predictions\n",
    "correlation, p_value = pearsonr(lr_predictions, mlp_predictions)\n",
    "\n",
    "print(f\"Correlation between LR and MLP model predictions: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual predictions from the LR model\n",
    "lr_predictions = best_pipeline_lr.predict(X_speech)\n",
    "\n",
    "# Get actual predictions from the MLP model\n",
    "mlp_predictions = best_pipeline_mlp.predict(X_movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score = cohen_kappa_score(lr_predictions, mlp_predictions)\n",
    "print(\"Cohen's Kappa score:\", kappa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = confusion_matrix(lr_predictions, mlp_predictions)\n",
    "result = mcnemar(contingency_table)\n",
    "print(\"McNemar's test p-value:\", result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agreement_rate = np.mean(lr_predictions == mlp_predictions)\n",
    "print(\"Prediction Agreement Rate:\", agreement_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mi_score = mutual_info_score(lr_predictions, mlp_predictions)\n",
    "print(\"Mutual Information score:\", mi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "# Define features for the LR model (speech features)\n",
    "X_speech = multimodal[['Proportion speech child']]\n",
    "y_speech = multimodal['label']\n",
    "\n",
    "# Define features for the MLP model (movement features)\n",
    "X_movement = multimodal[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "                         'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y_movement = multimodal['label']\n",
    "\n",
    "# Define the best parameters for LR and MLP models\n",
    "best_params_lr = {'lr__C': 1, 'lr__penalty': 'l1', 'lr__solver': 'liblinear'}\n",
    "best_params_mlp = {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'pca__n_components': 4}\n",
    "\n",
    "# Create the LR pipeline with the best parameters\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=best_params_lr['lr__C'], penalty=best_params_lr['lr__penalty'], solver=best_params_lr['lr__solver']))\n",
    "])\n",
    "\n",
    "# Create the MLP pipeline with the best parameters\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=best_params_mlp['pca__n_components'])),\n",
    "    ('mlp', MLPClassifier(activation=best_params_mlp['mlp__activation'], alpha=best_params_mlp['mlp__alpha'], hidden_layer_sizes=best_params_mlp['mlp__hidden_layer_sizes']))\n",
    "])\n",
    "\n",
    "# Combine the features for the ensemble model\n",
    "X_combined = pd.concat([X_speech, X_movement], axis=1)\n",
    "y_combined = multimodal['label']\n",
    "groups = multimodal.index  # Assuming 'participant_id' is the column for groups\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the base models for stacking\n",
    "base_models = [\n",
    "    ('lr', lr_pipeline),\n",
    "    ('mlp', mlp_pipeline)\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create the stacking ensemble model\n",
    "stacking_ensemble = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Initialize lists to store the evaluation metrics\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Perform manual cross-validation\n",
    "for train_index, test_index in gkf.split(X_combined, y_combined, groups):\n",
    "    X_train, X_test = X_combined.iloc[train_index], X_combined.iloc[test_index]\n",
    "    y_train, y_test = y_combined.iloc[train_index], y_combined.iloc[test_index]\n",
    "    \n",
    "    # Fit the stacking ensemble model\n",
    "    stacking_ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = stacking_ensemble.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"F1 Score: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming multimodal is the merged dataset with common participants\n",
    "# Define features for the LR model (speech features)\n",
    "X_speech = multimodal[['Proportion speech child']]\n",
    "y_speech = multimodal['label']\n",
    "\n",
    "# Define features for the MLP model (movement features)\n",
    "X_movement = multimodal[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "                         'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y_movement = multimodal['label']\n",
    "\n",
    "# Define the best parameters for LR and MLP models\n",
    "best_params_lr = {'lr__C': 1, 'lr__penalty': 'l1', 'lr__solver': 'liblinear'}\n",
    "best_params_mlp = {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'pca__n_components': 4}\n",
    "\n",
    "# Create the LR pipeline with the best parameters\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=best_params_lr['lr__C'], penalty=best_params_lr['lr__penalty'], solver=best_params_lr['lr__solver']))\n",
    "])\n",
    "\n",
    "# Create the MLP pipeline with the best parameters\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=best_params_mlp['pca__n_components'])),\n",
    "    ('mlp', MLPClassifier(activation=best_params_mlp['mlp__activation'], alpha=best_params_mlp['mlp__alpha'], hidden_layer_sizes=best_params_mlp['mlp__hidden_layer_sizes']))\n",
    "])\n",
    "\n",
    "# Create additional classifiers\n",
    "svc_linear_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "svc_rbf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='rbf', probability=True))\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "et_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('et', ExtraTreesClassifier())\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Combine the features for the ensemble model\n",
    "X_combined = pd.concat([X_speech, X_movement], axis=1)\n",
    "y_combined = multimodal['label']\n",
    "groups = multimodal['participant_id'].values  # Assuming 'participant_id' is the column for groups\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the base models for stacking\n",
    "base_models = [\n",
    "    ('lr', lr_pipeline),\n",
    "    ('mlp', mlp_pipeline),\n",
    "    ('svc_linear', svc_linear_pipeline),\n",
    "    ('svc_rbf', svc_rbf_pipeline),\n",
    "    ('rf', rf_pipeline),\n",
    "    ('et', et_pipeline),\n",
    "    ('gb', gb_pipeline)\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the stacking ensemble model\n",
    "stacking_ensemble = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Define the parameter grid for the meta-model and base models\n",
    "param_distributions = {\n",
    "    'final_estimator__n_estimators': [50, 100, 200],\n",
    "    'final_estimator__max_depth': [None, 10, 20],\n",
    "    'final_estimator__min_samples_split': [2, 5, 10],\n",
    "    'lr__lr__C': [0.01, 0.1, 1, 10],\n",
    "    'lr__lr__penalty': ['l1', 'l2'],\n",
    "    'lr__lr__solver': ['liblinear', 'saga'],\n",
    "    'mlp__mlp__activation': ['tanh', 'relu'],\n",
    "    'mlp__mlp__alpha': [0.0001, 0.001, 0.01],\n",
    "    'mlp__mlp__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'svc_linear__svc__C': [0.01, 0.1, 1, 10],\n",
    "    'svc_rbf__svc__C': [0.01, 0.1, 1, 10],\n",
    "    'svc_rbf__svc__gamma': [0.01, 0.1, 1, 10],\n",
    "    'rf__rf__n_estimators': [50, 100, 200],\n",
    "    'rf__rf__max_depth': [None, 10, 20],\n",
    "    'rf__rf__min_samples_split': [2, 5, 10],\n",
    "    'et__et__n_estimators': [50, 100, 200],\n",
    "    'et__et__max_depth': [None, 10, 20],\n",
    "    'et__et__min_samples_split': [2, 5, 10],\n",
    "    'gb__gb__n_estimators': [50, 100, 200],\n",
    "    'gb__gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb__gb__max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=stacking_ensemble, param_distributions=param_distributions, n_iter=50, cv=gkf, scoring='f1_macro', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_combined, y_combined, groups=groups)\n",
    "\n",
    "# Get the best estimator\n",
    "best_stacking_ensemble = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best stacking ensemble using cross-validation\n",
    "f1_scores = cross_val_score(best_stacking_ensemble, X_combined, y_combined, cv=gkf, scoring='f1_macro', groups=groups, n_jobs=-1)\n",
    "precision_scores = cross_val_score(best_stacking_ensemble, X_combined, y_combined, cv=gkf, scoring='precision_macro', groups=groups, n_jobs=-1)\n",
    "recall_scores = cross_val_score(best_stacking_ensemble, X_combined, y_combined, cv=gkf, scoring='recall_macro', groups=groups, n_jobs=-1)\n",
    "accuracy_scores = cross_val_score(best_stacking_ensemble, X_combined, y_combined, cv=gkf, scoring='accuracy', groups=groups, n_jobs=-1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"F1 Score: {f1_scores.mean()}\")\n",
    "print(f\"Precision: {precision_scores.mean()}\")\n",
    "print(f\"Recall: {recall_scores.mean()}\")\n",
    "print(f\"Accuracy: {accuracy_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Assuming multimodal is the merged dataset with common participants\n",
    "# Define features for the LR model (speech features)\n",
    "X_speech = multimodal[['Proportion speech child']]\n",
    "y_speech = multimodal['label']\n",
    "\n",
    "# Define features for the MLP model (movement features)\n",
    "X_movement = multimodal[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "                         'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y_movement = multimodal['label']\n",
    "\n",
    "# Define the best parameters for LR and MLP models\n",
    "best_params_lr = {'lr__C': 1, 'lr__penalty': 'l1', 'lr__solver': 'liblinear'}\n",
    "best_params_mlp = {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'pca__n_components': 4}\n",
    "\n",
    "# Create the LR pipeline with the best parameters\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=best_params_lr['lr__C'], penalty=best_params_lr['lr__penalty'], solver=best_params_lr['lr__solver']))\n",
    "])\n",
    "\n",
    "# Create the MLP pipeline with the best parameters\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=best_params_mlp['pca__n_components'])),\n",
    "    ('mlp', MLPClassifier(activation=best_params_mlp['mlp__activation'], alpha=best_params_mlp['mlp__alpha'], hidden_layer_sizes=best_params_mlp['mlp__hidden_layer_sizes']))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Combine the features for the ensemble model\n",
    "X_combined = pd.concat([X_speech, X_movement], axis=1)\n",
    "y_combined = multimodal['label']\n",
    "groups = multimodal.index  # Assuming 'participant_id' is the column for groups\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the base models for voting\n",
    "base_models = [\n",
    "    ('lr', lr_pipeline),\n",
    "    ('mlp', mlp_pipeline)\n",
    "]\n",
    "\n",
    "# Create the voting classifier\n",
    "voting_ensemble = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "# Evaluate the voting ensemble using cross-validation\n",
    "f1_scores = cross_val_score(voting_ensemble, X_combined, y_combined, cv=gkf, scoring='f1_macro', groups=groups, n_jobs=-1)\n",
    "precision_scores = cross_val_score(voting_ensemble, X_combined, y_combined, cv=gkf, scoring='precision_macro', groups=groups, n_jobs=-1)\n",
    "recall_scores = cross_val_score(voting_ensemble, X_combined, y_combined, cv=gkf, scoring='recall_macro', groups=groups, n_jobs=-1)\n",
    "accuracy_scores = cross_val_score(voting_ensemble, X_combined, y_combined, cv=gkf, scoring='accuracy', groups=groups, n_jobs=-1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"F1 Score: {f1_scores.mean()}\")\n",
    "print(f\"Precision: {precision_scores.mean()}\")\n",
    "print(f\"Recall: {recall_scores.mean()}\")\n",
    "print(f\"Accuracy: {accuracy_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming multimodal is the merged dataset with common participants\n",
    "# Define features for the LR model (speech features)\n",
    "X_speech = multimodal[['Proportion speech child']]\n",
    "y_speech = multimodal['label']\n",
    "\n",
    "# Define features for the MLP model (movement features)\n",
    "X_movement = multimodal[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "                         'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y_movement = multimodal['label']\n",
    "\n",
    "# Define the best parameters for LR and MLP models\n",
    "best_params_lr = {'lr__C': 1, 'lr__penalty': 'l1', 'lr__solver': 'liblinear'}\n",
    "best_params_mlp = {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'pca__n_components': 4}\n",
    "\n",
    "# Create the LR pipeline with the best parameters\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=best_params_lr['lr__C'], penalty=best_params_lr['lr__penalty'], solver=best_params_lr['lr__solver']))\n",
    "])\n",
    "\n",
    "# Create the MLP pipeline with the best parameters\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=best_params_mlp['pca__n_components'])),\n",
    "    ('mlp', MLPClassifier(activation=best_params_mlp['mlp__activation'], alpha=best_params_mlp['mlp__alpha'], hidden_layer_sizes=best_params_mlp['mlp__hidden_layer_sizes']))\n",
    "])\n",
    "\n",
    "# Combine the features for the ensemble model\n",
    "X_combined = pd.concat([X_speech, X_movement], axis=1)\n",
    "y_combined = multimodal['label']\n",
    "groups = multimodal.index  # Assuming 'participant_id' is the column for groups\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('lr', lr_pipeline),\n",
    "    ('mlp', mlp_pipeline)\n",
    "]\n",
    "\n",
    "# Define weights for the base models\n",
    "weights = [0.7, 0.3]  # Adjust these weights based on model performance\n",
    "\n",
    "# Initialize lists to store the evaluation metrics\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Perform manual cross-validation\n",
    "for train_index, test_index in gkf.split(X_combined, y_combined, groups):\n",
    "    X_train, X_test = X_combined.iloc[train_index], X_combined.iloc[test_index]\n",
    "    y_train, y_test = y_combined.iloc[train_index], y_combined.iloc[test_index]\n",
    "    \n",
    "    # Fit the base models\n",
    "    for name, model in base_models:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the predicted probabilities from each base model\n",
    "    lr_probs = lr_pipeline.predict_proba(X_test)[:, 1]\n",
    "    mlp_probs = mlp_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Combine the predicted probabilities using weighted averaging\n",
    "    combined_probs = (weights[0] * lr_probs) + (weights[1] * mlp_probs)\n",
    "    \n",
    "    # Convert combined probabilities to class predictions\n",
    "    combined_preds = np.where(combined_probs > 0.5, 1, 0)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    f1_scores.append(f1_score(y_test, combined_preds, average='macro'))\n",
    "    precision_scores.append(precision_score(y_test, combined_preds, average='macro'))\n",
    "    recall_scores.append(recall_score(y_test, combined_preds, average='macro'))\n",
    "    accuracy_scores.append(accuracy_score(y_test, combined_preds))\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"F1 Score: {np.mean(f1_scores)}\")\n",
    "print(f\"Precision: {np.mean(precision_scores)}\")\n",
    "print(f\"Recall: {np.mean(recall_scores)}\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use base models predictions as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming multimodal is the merged dataset with common participants\n",
    "# Define features for the LR model (speech features)\n",
    "X_speech = multimodal[['Proportion speech child']]\n",
    "y_speech = multimodal['label']\n",
    "\n",
    "# Define features for the MLP model (movement features)\n",
    "X_movement = multimodal[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "                         'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y_movement = multimodal['label']\n",
    "\n",
    "# Combine the features for the ensemble model\n",
    "X_combined = pd.concat([X_speech, X_movement], axis=1)\n",
    "y_combined = multimodal['label']\n",
    "groups = multimodal.index  # Assuming 'participant_id' is the column for groups\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('lr', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lr', LogisticRegression(C=1, penalty='l1', solver='liblinear'))\n",
    "    ])),\n",
    "    ('mlp', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=4)),\n",
    "        ('mlp', MLPClassifier(activation='tanh', alpha=0.01, hidden_layer_sizes=(50, 50)))\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to store the new features\n",
    "new_features = pd.DataFrame(index=X_combined.index)\n",
    "\n",
    "# Generate out-of-fold predictions for each base model\n",
    "for name, model in base_models:\n",
    "    oof_preds = cross_val_predict(model, X_combined, y_combined, cv=gkf, method='predict_proba', groups=groups, n_jobs=-1)[:, 1]\n",
    "    new_features[name] = oof_preds\n",
    "\n",
    "# Combine the new features with the original features\n",
    "X_meta = pd.concat([X_combined, new_features], axis=1)\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Evaluate the meta-model using cross-validation\n",
    "f1_scores = cross_val_score(meta_model, X_meta, y_combined, cv=gkf, scoring='f1_macro', groups=groups, n_jobs=-1)\n",
    "precision_scores = cross_val_score(meta_model, X_meta, y_combined, cv=gkf, scoring='precision_macro', groups=groups, n_jobs=-1)\n",
    "recall_scores = cross_val_score(meta_model, X_meta, y_combined, cv=gkf, scoring='recall_macro', groups=groups, n_jobs=-1)\n",
    "accuracy_scores = cross_val_score(meta_model, X_meta, y_combined, cv=gkf, scoring='accuracy', groups=groups, n_jobs=-1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"F1 Score: {f1_scores.mean()}\")\n",
    "print(f\"Precision: {precision_scores.mean()}\")\n",
    "print(f\"Recall: {recall_scores.mean()}\")\n",
    "print(f\"Accuracy: {accuracy_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
