{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# import dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meal = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/combined_features.csv', index_col=0)\n",
    "df_meal = df_meal.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])\n",
    "\n",
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df_meal['label'] = df_meal['label'].map({'no_rad': 0, 'rad': 1})\n",
    "df_play['label'] = df_play['label'].map({'no_rad': 0, 'rad': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_meal = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_meal['group'] = df_meal['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_meal = df_meal.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "#df_grouped_meal['s_id'] = df_meal.groupby('group')['s_id'].first()\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "\n",
    "# Reset the index\n",
    "df_play = df_play.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_play['group'] = df_play['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_play = df_play.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "df_grouped_play.index.name = 's_id'\n",
    "\n",
    "\n",
    "# Reset the index of the grouped dataframes\n",
    "#df_grouped_meal = df_grouped_meal.reset_index()\n",
    "#df_grouped_play = df_grouped_play.reset_index()\n",
    "\n",
    "# Add a new 'group' column to each DataFrame\n",
    "df_grouped_meal['group'] = 'meal'\n",
    "df_grouped_play['group'] = 'play'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_play.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_play.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = pd.read_excel('/Users/andrei-macpro/Documents/Data/classification/speech/classification.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index name to s_id\n",
    "speech.index.name = 's_id'\n",
    "# remove age column from speech\n",
    "speech = speech.drop(columns=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where True indicates a duplicated index\n",
    "mask = speech.index.duplicated(keep='first')\n",
    "\n",
    "# Use np.where to assign 'meal' to the first occurrence and 'play' to the second\n",
    "speech['group'] = np.where(mask, 'play', 'meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.set_index('group', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['label'] = speech['label'].map({'no_rad': 0, 'rad': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only meal group from speech index\n",
    "speech_meal = speech.xs('meal', level='group')\n",
    "speech_play = speech.xs('play', level='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_grouped_meal = df_grouped_meal.reset_index(level=0, drop=True)\n",
    "meal_data = pd.concat([df_grouped_meal, speech_meal], axis=1)\n",
    "meal_data = meal_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data = meal_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data = pd.concat([df_grouped_play, speech_play], axis=1)\n",
    "#meal_data = meal_data.drop(columns=['index'])\n",
    "play_data = play_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with just movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform a grid search for each classifier\n",
    "X = df_grouped_meal[[\n",
    "                     'cg_movement_mean', 'cg_movement_var',\n",
    "       'cg_movement_min', 'cg_movement_max', \n",
    "       'child_movement_mean', 'child_movement_var',\n",
    "       'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "y = df_grouped_meal['label']\n",
    "\n",
    "\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5)\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'pca__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    })\n",
    "]\n",
    "    \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled = shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                             ('pca', PCA()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro',  n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "         # Extract the number of PCA components from the best parameters\n",
    "        best_pca_components = grid_search.best_params_.get('pca__n_components', None)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),   \n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': best_pca_components\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(play_data), len(df_grouped_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech child + movement play best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = play_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "\n",
    "y = play_data['label'].iloc[:, 0]   \n",
    "\n",
    "\n",
    "remaining_features = [feat for feat in X.columns if feat not in subset_pca]\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear', probability=True), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    })\n",
    "]\n",
    "  \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "conf_matrix_sum = np.zeros((2, 2))\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled= shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([ ('preprocessor', preprocessor), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': grid_search.best_params_.get('preprocessor__pca_pipeline__pca__n_components', None)\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }\n",
    "    best_model_info = max(best_models.values(), key=lambda x: x['best_score'])\n",
    "    best_model = best_model_info['best_estimator']\n",
    "\n",
    "    # Use the best model to make predictions on the test set\n",
    "    y_pred = best_model.predict(X_shuffled)\n",
    "\n",
    "    # Compute the confusion matrix for this iteration\n",
    "    conf_matrix = confusion_matrix(y_shuffled, y_pred)\n",
    "    conf_matrix_sum += conf_matrix\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "average_conf_matrix = conf_matrix_sum / 10\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Visualize the average confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(average_conf_matrix, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=['no rad', 'rad'], yticklabels=['no rad', 'rad'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Average Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=6)\n",
    "cross_val_score(best_models['xgb']['best_estimator'] , X_meal, y_meal, cv=cv, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_play = best_models['svc_linear']['best_estimator']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_meal = best_models['xgb']['best_estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_meal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for significant difference between the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the datasets:\n",
    "\n",
    "X_meal = meal_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "y_meal = meal_data['label'].iloc[:, 0]\n",
    "X_play = play_data[['Proportion speech child', 'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_meal = []\n",
    "acc_play = []\n",
    "cv = KFold(n_splits=5, shuffle= False)\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_meal_shuffled, y_meal_shuffled = shuffle(X_meal, y_meal, random_state=i)\n",
    "    X_play_shuffled, y_play_shuffled = shuffle(X_play, y_play, random_state=i)\n",
    "    # Perform a grid search for each classifier\n",
    "    acc_meal.append(cross_val_score(pipeline_meal, X_meal_shuffled, y_meal_shuffled, cv=cv, scoring='accuracy').mean())\n",
    "    acc_play.append(cross_val_score(pipeline_play, X_play_shuffled, y_play_shuffled, cv=cv, scoring='accuracy').mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_meal, acc_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "t_stat, p_value = ttest_rel(acc_meal, acc_play)\n",
    "t_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meal = meal_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "         'average_proximity', 'variance_proximity', 'min_proximity', 'max_proximity'\n",
    " ]]\n",
    "y_meal = meal_data['label'].iloc[:, 0]\n",
    "X_play = play_data[['Proportion speech child', 'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "accs = []\n",
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    accs.append(cross_val_score(pipeline_play, X_meal, y_meal, cv=cv, scoring='accuracy').mean())\n",
    "    y_pred = cross_val_predict(pipeline_play, X_meal, y_meal, cv=cv)\n",
    "    f1s.append(f1_score(y_meal, y_pred))\n",
    "    precisions.append(precision_score(y_meal, y_pred))\n",
    "    recalls.append(recall_score(y_meal, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = np.mean(accs)\n",
    "std_acc = np.std(accs)\n",
    "\n",
    "mean_f1 = np.mean(f1s)\n",
    "std_f1 = np.std(f1s)\n",
    "\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "print('Accuracy of model trained on play data evaluated on meal data')\n",
    "print(f'Mean accuracy: {mean_acc}, Standard deviation: {std_acc}')\n",
    "print(f'Mean F1 score: {mean_f1}, Standard deviation: {std_f1}')\n",
    "print(f'Mean precision: {mean_precision}, Standard deviation: {std_precision}')\n",
    "print(f'Mean recall: {mean_recall}, Standard deviation: {std_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "accs = []\n",
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    accs.append(cross_val_score(pipeline_play, X_meal, y_meal, cv=cv, scoring='accuracy').mean())\n",
    "    y_pred = cross_val_predict(pipeline_play, X_meal, y_meal, cv=cv)\n",
    "    f1s.append(f1_score(y_meal, y_pred))\n",
    "    precisions.append(precision_score(y_meal, y_pred))\n",
    "    recalls.append(recall_score(y_meal, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = np.mean(accs)\n",
    "std_acc = np.std(accs)\n",
    "\n",
    "mean_f1 = np.mean(f1s)\n",
    "std_f1 = np.std(f1s)\n",
    "\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "print('Accuracy of model trained on play data evaluated on meal data')\n",
    "print(f'Mean accuracy: {mean_acc}, Standard deviation: {std_acc}')\n",
    "print(f'Mean F1 score: {mean_f1}, Standard deviation: {std_f1}')\n",
    "print(f'Mean precision: {mean_precision}, Standard deviation: {std_precision}')\n",
    "print(f'Mean recall: {mean_recall}, Standard deviation: {std_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = meal_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "\n",
    "y = play_data['label'].iloc[:, 0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(pipeline_play, X, y, n_repeats=30, random_state=0, scoring='accuracy')\n",
    "feature_importances_play = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': result['importances_mean']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = meal_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "         'average_proximity', 'variance_proximity', 'min_proximity', 'max_proximity'\n",
    " ]]\n",
    "y = meal_data['label'].iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(pipeline_meal, X, y, n_repeats=30, random_state=0, scoring='accuracy')\n",
    "feature_importances_meal = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': result['importances_mean']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_play = feature_importances_play.sort_values(by='Importance', ascending=False)\n",
    "feature_importances_meal = feature_importances_meal.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Define a custom color palette\n",
    "features = list(set(feature_importances_play['Feature']).union(set(feature_importances_meal['Feature'])))\n",
    "palette = sns.color_palette('Set1', len(features))\n",
    "color_mapping = {feature: palette[i] for i, feature in enumerate(features)}\n",
    "\n",
    "# Function to map colors to features\n",
    "def get_colors(data, color_mapping):\n",
    "    return data['Feature'].map(color_mapping).tolist()\n",
    "\n",
    "# Plot the feature importances for Play\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_play, palette=get_colors(feature_importances_play, color_mapping))\n",
    "plt.title('Feature Importances Play')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Plot the feature importances for Meal\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_meal, palette=get_colors(feature_importances_meal, color_mapping))\n",
    "plt.title('Feature Importances Meal')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate proportion of outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_importance = feature_importances_meal['Importance'].sum()\n",
    "\n",
    "proportion_speech_child = feature_importances_meal.loc[feature_importances_meal['Feature'] == 'Proportion speech child', 'Importance'].sum() / total_importance\n",
    "\n",
    "# Calculate the proportion for the rest of the features combined\n",
    "proportion_rest = feature_importances_meal.loc[feature_importances_meal['Feature'] != 'Proportion speech child', 'Importance'].sum() / total_importance\n",
    "\n",
    "print(f\"Proportion driven by 'Proportion speech child': {proportion_speech_child:.2%}\")\n",
    "print(f\"Proportion driven by the rest of the features combined: {proportion_rest:.2%}\")\n",
    "\n",
    "# Plot the proportions\n",
    "labels = ['Child Speech', 'Movement Features']\n",
    "sizes = [proportion_speech_child, proportion_rest]\n",
    "colors = ['#ff9999','#66b3ff']\n",
    "explode = (0.1, 0)  # explode the first slice\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.title('Proportion of Total Feature Importance Meal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_importance = feature_importances_play['Importance'].sum()\n",
    "proportion_speech_child = feature_importances_play.loc[feature_importances_play['Feature'] == 'Proportion speech child', 'Importance'].sum() / total_importance\n",
    "\n",
    "# Calculate the proportion for the rest of the features combined\n",
    "proportion_rest = feature_importances_play.loc[feature_importances_play['Feature'] != 'Proportion speech child', 'Importance'].sum() / total_importance\n",
    "\n",
    "# Plot the proportions\n",
    "labels = ['Child Speech', 'Movement Features']\n",
    "sizes = [proportion_speech_child, proportion_rest]\n",
    "colors = ['#ff9999','#66b3ff']\n",
    "explode = (0.1, 0)  # explode the first slice\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.title('Proportion of Total Feature Importance Play')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution of speech vs movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = play_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "\n",
    "y = play_data['label'].iloc[:, 0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rad = X_play.loc[ X_play['label'] == 1]\n",
    "X_rad = X_rad[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "X_non_rad = X_play.loc[ X_play['label'] == 0]\n",
    "X_non_rad = X_non_rad[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "y_rad = y[y == 1]\n",
    "y_non_rad = y[y == 0]\n",
    "results_rad = []\n",
    "results_non_rad = []\n",
    "for i in range(10):\n",
    "    results_rad.append(permutation_importance(pipeline_play, X_rad, y_rad, n_repeats=30, random_state=i, scoring='accuracy'))\n",
    "    results_non_rad.append(permutation_importance(pipeline_play, X_non_rad, y_non_rad, n_repeats=30, random_state=i, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(np.mean([result['importances_mean'] for result in results_rad], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_rad = pd.DataFrame({\n",
    "    'Feature': X_rad.columns,\n",
    "    'Importance': np.abs(np.mean([result['importances_mean'] for result in results_rad], axis=0)),\n",
    "    'Std': np.mean([result['importances_std'] for result in results_rad], axis=0)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importances_non_rad = pd.DataFrame({\n",
    "    'Feature': X_non_rad.columns,\n",
    "    'Importance':  np.abs(np.mean([result['importances_mean'] for result in results_non_rad], axis=0)),\n",
    "    'Std': np.mean([result['importances_std'] for result in results_non_rad], axis=0)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "features = list(set(feature_importances_rad['Feature']).union(set(feature_importances_non_rad['Feature'])))\n",
    "palette = sns.color_palette('Set1', len(features))\n",
    "color_mapping = {feature: palette[i] for i, feature in enumerate(features)}\n",
    "feature_importances_rad['Importance_lower'] = np.clip(feature_importances_rad['Importance'] - feature_importances_rad['Std'], 0, None)\n",
    "feature_importances_rad['Importance_upper'] = feature_importances_rad['Importance'] + feature_importances_rad['Std']\n",
    "\n",
    "feature_importances_non_rad['Importance_lower'] = np.clip(feature_importances_non_rad['Importance'] - feature_importances_non_rad['Std'], 0, None)\n",
    "feature_importances_non_rad['Importance_upper'] = feature_importances_non_rad['Importance'] + feature_importances_non_rad['Std']\n",
    "\n",
    "\n",
    "# Plot the feature importances for RAD class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_rad, palette=color_mapping)\n",
    "plt.errorbar(feature_importances_rad['Importance'], feature_importances_rad['Feature'],\n",
    "             xerr=[feature_importances_rad['Importance'] - feature_importances_rad['Importance_lower'],\n",
    "                   feature_importances_rad['Importance_upper'] - feature_importances_rad['Importance']],\n",
    "             fmt='none', c='black', capsize=5)\n",
    "plt.title('Feature Importances for RAD Class')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Plot the feature importances for non-RAD class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_non_rad, palette=color_mapping)\n",
    "plt.errorbar(feature_importances_non_rad['Importance'], feature_importances_non_rad['Feature'],\n",
    "             xerr=[feature_importances_non_rad['Importance'] - feature_importances_non_rad['Importance_lower'],\n",
    "                   feature_importances_non_rad['Importance_upper'] - feature_importances_non_rad['Importance']],\n",
    "             fmt='none', c='black', capsize=5)\n",
    "plt.title('Feature Importances for Non-RAD Class')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_non_rad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Assuming best_models is a dictionary containing the best models\n",
    "best_estimator = pipeline_play\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_pred_proba = best_estimator.predict_proba(X)[:,1]\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, thresholds = roc_curve(y, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#plt.xlim([-0.02, 1.0])  # Adjust x-axis limits to provide more space on the left\n",
    "#plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate', labelpad=5)  # Adjust labelpad to move the y-axis label to the left\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movement feature differences per class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_play = df_grouped_play[['cg_movement_max', 'cg_movement_min', 'cg_movement_mean', 'cg_movement_var',\n",
    " 'child_movement_mean', 'child_movement_max', 'child_movement_var', 'child_movement_min', 'label']]\n",
    "features_meal = df_grouped_meal[['cg_movement_max', 'cg_movement_min', 'cg_movement_mean', 'cg_movement_var',\n",
    "                                 'child_movement_mean', 'child_movement_max', 'child_movement_var', 'child_movement_min', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a figure with subplots\n",
    "features = ['child_movement_max', 'child_movement_min', 'child_movement_mean', 'child_movement_var']\n",
    "label_column = 'label'\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(features):\n",
    "    sns.boxplot(x=label_column, y=feature, data=df_grouped_play, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by {label_column}')\n",
    "    axes[i].set_xlabel('Label')\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a figure with subplots\n",
    "features = ['child_movement_max', 'child_movement_min', 'child_movement_mean', 'child_movement_var']\n",
    "label_column = 'label'\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(features):\n",
    "    sns.boxplot(x=label_column, y=feature, data=df_grouped_meal, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by {label_column}')\n",
    "    axes[i].set_xlabel('Label')\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df_grouped_meal, df_grouped_play], axis=0)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a figure with subplots\n",
    "features = ['child_movement_max', 'child_movement_min', 'child_movement_mean', 'child_movement_var']\n",
    "label_column = 'label'\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(features):\n",
    "    sns.boxplot(x=label_column, y=feature, data=final_df, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by {label_column}')\n",
    "    axes[i].set_xlabel('Label')\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=final_df['label'], y=final_df['average_proximity'], data=final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for differences between features in the two classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the combined dataset\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Example data\n",
    "# Assuming final_df is your DataFrame and 'label' is the column with the labels\n",
    "feature = 'cg_movement_max'\n",
    "label_column = 'label'\n",
    "\n",
    "# Separate the data into two groups based on the labels\n",
    "group_0 = final_df[final_df[label_column] == 0][feature]\n",
    "group_1 = final_df[final_df[label_column] == 1][feature]\n",
    "\n",
    "# Perform the Mann-Whitney U test\n",
    "stat, p_value = mannwhitneyu(group_0, group_1)\n",
    "\n",
    "print(f'Mann-Whitney U test statistic: {stat}')\n",
    "print(f'Mann-Whitney U test p-value: {p_value}')\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print('There is a significant difference between the two groups.')\n",
    "else:\n",
    "    print('There is no significant difference between the two groups.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the separate datasets\n",
    "# For the combined dataset\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Example data\n",
    "# Assuming final_df is your DataFrame and 'label' is the column with the labels\n",
    "feature = 'cg_movement_var'\n",
    "label_column = 'label'\n",
    "\n",
    "# Separate the data into two groups based on the labels\n",
    "group_0 = df_grouped_play[df_grouped_play[label_column] == 0][feature]\n",
    "group_1 = df_grouped_play[df_grouped_play[label_column] == 1][feature]\n",
    "\n",
    "# Perform the Mann-Whitney U test\n",
    "stat, p_value = mannwhitneyu(group_0, group_1)\n",
    "\n",
    "print(f'Mann-Whitney U test statistic: {stat}')\n",
    "print(f'Mann-Whitney U test p-value: {p_value}')\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print('There is a significant difference between the two groups.')\n",
    "else:\n",
    "    print('There is no significant difference between the two groups.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_meal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take the best performing model outside of the pipeline and test it and then validate it on the other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(pipeline_meal, X_meal, y_meal, n_repeats=30, random_state=0, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X_meal.columns,\n",
    "    'Importance': result['importances_mean']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_meal = feature_importances_df.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(pipeline_play, X_play, y_play, n_repeats=30, random_state=0, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data_selected = meal_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max', 'group','label']]\n",
    "play_data_selected = play_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max', 'group', 'label']]\n",
    "\n",
    "# Find the common indices\n",
    "common_indices = meal_data_selected.index.intersection(play_data_selected.index)\n",
    "\n",
    "# Filter the DataFrames to keep only the common indices\n",
    "meal_data_filtered = meal_data_selected.loc[common_indices]\n",
    "play_data_filtered = play_data_selected.loc[common_indices]\n",
    "\n",
    "#meal_data_filtered = meal_data_filtered.set_index('group', append=True)\n",
    "#play_data_filtered = play_data_filtered.set_index('group', append=True)\n",
    "\n",
    "# Concatenate the filtered DataFrames\n",
    "combined_data = pd.concat([meal_data_filtered, play_data_filtered],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlations between the two models \n",
    "# Get predictions from the LR model\n",
    "meal_predictions =  pipeline_meal.predict_proba(meal_data_filtered.iloc[:,:-3])[:,1]\n",
    "\n",
    "# Get predictions from the MLP model\n",
    "play_predictions = pipeline_play.predict_proba(play_data_filtered.iloc[:,:-3])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_meal = pipeline_play.score(X_meal, y_meal)\n",
    "print(f'Accuracy of model trained on play data evaluated on meal data: {accuracy_meal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_play.predict(X_meal)\n",
    "f1 = f1_score(y_meal, y_pred, average='weighted')\n",
    "precision = precision_score(y_meal, y_pred, average='weighted')\n",
    "recall = recall_score(y_meal, y_pred, average='weighted')\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "accs = []\n",
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    pipeline_play.fit(X_play, y_play)\n",
    "    accs.append(cross_val_score(pipeline_play, X_meal, y_meal, cv=cv, scoring='accuracy').mean())\n",
    "    y_pred = cross_val_predict(pipeline_play, X_meal, y_meal, cv=cv)\n",
    "    f1s.append(f1_score(y_meal, y_pred))\n",
    "    precisions.append(precision_score(y_meal, y_pred))\n",
    "    recalls.append(recall_score(y_meal, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = np.mean(accs)\n",
    "std_acc = np.std(accs)\n",
    "\n",
    "mean_f1 = np.mean(f1s)\n",
    "std_f1 = np.std(f1s)\n",
    "\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "print('Accuracy of model trained on play data evaluated on meal data')\n",
    "print(f'Mean accuracy: {mean_acc}, Standard deviation: {std_acc}')\n",
    "print(f'Mean F1 score: {mean_f1}, Standard deviation: {std_f1}')\n",
    "print(f'Mean precision: {mean_precision}, Standard deviation: {std_precision}')\n",
    "print(f'Mean recall: {mean_recall}, Standard deviation: {std_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_play = pipeline_meal.score(X_play, y_play)\n",
    "print(f'Accuracy of model trained on meal data evaluated on play data: {accuracy_play}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_meal.predict(X_play)\n",
    "f1 = f1_score(y_play, y_pred, average='weighted')\n",
    "precision = precision_score(y_play, y_pred, average='weighted')\n",
    "recall = recall_score(y_play, y_pred, average='weighted')\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    pipeline_meal.fit(X_meal, y_meal)\n",
    "    accs.append(cross_val_score(pipeline_meal, X_play, y_play, cv=cv, scoring='accuracy').mean())\n",
    "    y_pred = cross_val_predict(pipeline_meal, X_play, y_play, cv=cv)\n",
    "    f1s.append(f1_score(y_play, y_pred))\n",
    "    precisions.append(precision_score(y_play, y_pred))\n",
    "    recalls.append(recall_score(y_play, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = np.mean(accs)\n",
    "std_acc = np.std(accs)\n",
    "\n",
    "mean_f1 = np.mean(f1s)\n",
    "std_f1 = np.std(f1s)\n",
    "\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "print('Accuracy of model trained on meal data evaluated on play data')\n",
    "print(f'Mean accuracy: {mean_acc}, Standard deviation: {std_acc}')\n",
    "print(f'Mean F1 score: {mean_f1}, Standard deviation: {std_f1}')\n",
    "print(f'Mean precision: {mean_precision}, Standard deviation: {std_precision}')\n",
    "print(f'Mean recall: {mean_recall}, Standard deviation: {std_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.dropna()\n",
    "combined_labels = combined_data['label'].iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Define the base models for voting\n",
    "base_models = [\n",
    "    ('play', pipeline_play),\n",
    "    ('meal', pipeline_meal)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "# Create the voting classifier\n",
    "voting_ensemble = VotingClassifier(estimators=base_models, voting='soft', weights = [0.6, 0.4])\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_meal_shuffled, y_meal_shuffled = shuffle(X_meal, y_meal, random_state=i)\n",
    "    X_play_shuffled, y_play_shuffled = shuffle(X_play, y_play, random_state=i)\n",
    "    \n",
    "    # Combine the shuffled data\n",
    "    X_combined = pd.concat([X_meal_shuffled, X_play_shuffled])\n",
    "    y_combined = pd.concat([y_meal_shuffled, y_play_shuffled])\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    \n",
    "    # Evaluate the voting ensemble using cross-validation\n",
    "    accs.append(cross_val_score(voting_ensemble, X_combined, y_combined, cv=cv, scoring='accuracy').mean())\n",
    "    y_pred = cross_val_predict(voting_ensemble, X_combined, y_combined, cv=cv)\n",
    "    f1s.append(f1_score(y_combined, y_pred, average='weighted'))\n",
    "    precisions.append(precision_score(y_combined, y_pred, average='weighted'))\n",
    "    recalls.append(recall_score(y_combined, y_pred, average='weighted'))\n",
    "\n",
    "# Evaluate the voting ensemble using cross-validation\n",
    "\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"F1 Score: {f1s.mean()}\")\n",
    "print(f\"Precision: {precisions.mean()}\")\n",
    "print(f\"Recall: {recalls.mean()}\")\n",
    "print(f\"Accuracy: {accs.mean()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(accs), np.std(accs), np.mean(f1s), np.std(f1s), np.mean(precisions), np.std(precisions), np.mean(recalls), np.std(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meal = meal_data[['Proportion speech child',\n",
    "    'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "    'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "\n",
    "X_play = play_data[['Proportion speech child',\n",
    "    'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "    'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "\n",
    "y_meal = meal_data['label'].iloc[:,0]\n",
    "\n",
    "\n",
    "y_play = play_data['label'].iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meal = X_meal.add_suffix('_meal')\n",
    "X_play = X_play.add_suffix('_play')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore performance per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_predict(pipeline_play, X_play, y_play, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold, len(y_play_high_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the results for Play model\n",
    "play_accuracy_list = []\n",
    "play_precision_list = []\n",
    "play_recall_list = []\n",
    "play_f1_list = []\n",
    "\n",
    "# Initialize lists to store the results for Meal model\n",
    "meal_accuracy_list = []\n",
    "meal_precision_list = []\n",
    "meal_recall_list = []\n",
    "meal_f1_list = []\n",
    "\n",
    "# Run the analysis 10 times with different fold splits\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    \n",
    "    # Play model analysis\n",
    "    play_preds = cross_val_predict(pipeline_play, play_data_filtered.iloc[:, :-3], combined_labels, cv=cv, method='predict_proba')\n",
    "    play_confidences = np.max(play_preds, axis=1)\n",
    "    threshold = np.percentile(play_confidences, 50)\n",
    "    high_confidence_indices = play_confidences <= threshold\n",
    "    play_preds_high_confidence = cross_val_predict(pipeline_play, play_data_filtered.iloc[:, :-3], combined_labels, cv=cv)[high_confidence_indices]\n",
    "    y_play_high_confidence = combined_labels[high_confidence_indices]\n",
    "    play_accuracy_list.append(accuracy_score(y_play_high_confidence, play_preds_high_confidence))\n",
    "    play_precision_list.append(precision_score(y_play_high_confidence, play_preds_high_confidence, average=None))\n",
    "    play_recall_list.append(recall_score(y_play_high_confidence, play_preds_high_confidence, average=None))\n",
    "    play_f1_list.append(f1_score(y_play_high_confidence, play_preds_high_confidence, average=None))\n",
    "    \n",
    "    # Meal model analysis\n",
    "    meal_preds = cross_val_predict(pipeline_meal, meal_data_filtered.iloc[:, :-3], combined_labels, cv=cv, method='predict_proba')\n",
    "    meal_confidences = np.max(meal_preds, axis=1)\n",
    "    threshold = np.percentile(meal_confidences, 50)\n",
    "    high_confidence_indices = meal_confidences >= threshold\n",
    "    meal_preds_high_confidence = cross_val_predict(pipeline_meal, meal_data_filtered.iloc[:, :-3], combined_labels, cv=cv)[high_confidence_indices]\n",
    "    y_meal_high_confidence = combined_labels[high_confidence_indices]\n",
    "    meal_accuracy_list.append(accuracy_score(y_meal_high_confidence, meal_preds_high_confidence))\n",
    "    meal_precision_list.append(precision_score(y_meal_high_confidence, meal_preds_high_confidence, average=None))\n",
    "    meal_recall_list.append(recall_score(y_meal_high_confidence, meal_preds_high_confidence, average=None))\n",
    "    meal_f1_list.append(f1_score(y_meal_high_confidence, meal_preds_high_confidence, average=None))\n",
    "\n",
    "# Convert lists to numpy arrays for easier averaging\n",
    "play_precision_array = np.array(play_precision_list)\n",
    "play_recall_array = np.array(play_recall_list)\n",
    "play_f1_array = np.array(play_f1_list)\n",
    "play_accuracy_array = np.array(play_accuracy_list)\n",
    "\n",
    "meal_precision_array = np.array(meal_precision_list)\n",
    "meal_recall_array = np.array(meal_recall_list)\n",
    "meal_f1_array = np.array(meal_f1_list)\n",
    "meal_accuracy_array = np.array(meal_accuracy_list)\n",
    "\n",
    "# Calculate the average metrics across all iterations for Play model\n",
    "avg_play_precision = play_precision_array.mean(axis=0)\n",
    "avg_play_recall = play_recall_array.mean(axis=0)\n",
    "avg_play_f1 = play_f1_array.mean(axis=0)\n",
    "avg_play_accuracy = play_accuracy_array.mean()\n",
    "\n",
    "# Calculate the average metrics across all iterations for Meal model\n",
    "avg_meal_precision = meal_precision_array.mean(axis=0)\n",
    "avg_meal_recall = meal_recall_array.mean(axis=0)\n",
    "avg_meal_f1 = meal_f1_array.mean(axis=0)\n",
    "avg_meal_accuracy = meal_accuracy_array.mean()\n",
    "\n",
    "# Print the final results\n",
    "\n",
    "print(f\"Overall Play Accuracy: {avg_play_accuracy}\")\n",
    "\n",
    "\n",
    "print(f\"Overall Meal Accuracy: {avg_meal_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_predict, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "subset_pca = ['cg_movement_mean_meal',\n",
    "       'cg_movement_var_meal', 'cg_movement_min_meal', 'cg_movement_max_meal',\n",
    "       'child_movement_mean_meal', 'child_movement_var_meal',\n",
    "       'child_movement_min_meal', 'child_movement_max_meal', 'cg_movement_mean_play',\n",
    "       'cg_movement_var_play', 'cg_movement_min_play', 'cg_movement_max_play',\n",
    "       'child_movement_mean_play', 'child_movement_var_play',\n",
    "       'child_movement_min_play', 'child_movement_max_play']\n",
    "remaining_features = [feat for feat in X_combined.columns if feat not in subset_pca]\n",
    "results = []\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=4))\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "# Define the meta-model\n",
    "meta_models = [\n",
    "    ('svc', Pipeline([('preprocessor', preprocessor), ('svc', SVC(probability=True))])),\n",
    "    ('logistic_regression', Pipeline([('preprocessor', preprocessor), ('logistic_regression', LogisticRegression())])),\n",
    "    ('random_forest', Pipeline([('preprocessor', preprocessor), ('random_forest', RandomForestClassifier())]))\n",
    "]\n",
    "param_grids = {\n",
    "    'svc': {\n",
    "        'svc__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'logistic_regression__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'logistic_regression__penalty': ['l1', 'l2'],\n",
    "        'logistic_regression__solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'random_forest__n_estimators': [10, 50, 100, 200],\n",
    "        'random_forest__max_depth': [None, 5, 10, 15],\n",
    "        'random_forest__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    X_meal = meal_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'average_proximity', 'variance_proximity', 'min_proximity', 'max_proximity',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "\n",
    "    X_play = play_data[['Proportion speech child',\n",
    "       'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "       'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]\n",
    "    meal_predictions = cross_val_predict(pipeline_meal, X_meal, y_meal, cv=cv, method='predict_proba')\n",
    "    play_predictions = cross_val_predict(pipeline_play, X_play, y_play, cv=cv, method='predict_proba')\n",
    "\n",
    "    meal_predictions_df = pd.DataFrame(meal_predictions, columns=['meal_class_0', 'meal_class_1'], index=X_meal.index)\n",
    "\n",
    "    play_predictions_df = pd.DataFrame(meal_predictions, columns=['play_class_0', 'play_class_1'], index=X_meal.index)\n",
    "\n",
    "    combined_predictions = pd.concat([meal_predictions_df, play_predictions_df], axis=1).dropna()\n",
    "    combined_labels = pd.concat([y_meal, y_play]).dropna()\n",
    "    \n",
    "    X_meal['group'] = 'meal'\n",
    "    X_play['group'] = 'play'\n",
    "    X_meal = X_meal.add_suffix('_meal')\n",
    "    X_play = X_play.add_suffix('_play')\n",
    "    X_combined = pd.concat([X_meal, X_play], axis=1)\n",
    "    X_combined = pd.concat([X_combined, combined_predictions], axis=1)\n",
    "\n",
    "    X_combined = X_combined.drop(columns=['group_meal', 'group_play'])\n",
    "    X_combined = X_combined.dropna()\n",
    "\n",
    "    combined_labels = pd.concat([y_meal, y_play],axis=1).dropna().iloc[:,0] \n",
    "    combined_groups = X_combined.index\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X_combined, combined_labels, combined_groups, random_state=i)\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    for model_name, meta_model in meta_models:\n",
    "        grid_search = GridSearchCV(meta_model, param_grids[model_name], cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "        best_meta_model = grid_search.best_estimator_\n",
    "        \n",
    "        accuracy_scores = cross_val_score(best_meta_model, X_shuffled, y_shuffled, groups=groups_shuffled, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(best_meta_model, X_shuffled, y_shuffled, groups=groups_shuffled, cv=cv, scoring='precision_weighted', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(best_meta_model, X_shuffled, y_shuffled, groups=groups_shuffled, cv=cv, scoring='recall_weighted', n_jobs=-1)\n",
    "        f1_scores = cross_val_score(best_meta_model, X_shuffled, y_shuffled, groups=groups_shuffled, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "        \n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'model': model_name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'f1_score': f1_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('model').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at relationship between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlations between the two models \n",
    "# Get predictions from the LR model\n",
    "meal_predictions =  pipeline_meal.predict_proba(meal_data_filtered.iloc[:,:-3])[:,1]\n",
    "\n",
    "# Get predictions from the MLP model\n",
    "play_predictions = pipeline_play.predict_proba(play_data_filtered.iloc[:,:-3])[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation between the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_labels = pd.concat([y_meal, y_play], axis=1).dropna().iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_meal = cross_val_predict(pipeline_meal, meal_data_filtered.iloc[:,:-3], combined_labels, cv=cv)\n",
    "predictions_play = cross_val_predict(pipeline_play, play_data_filtered.iloc[:,:-3], combined_labels, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diversity of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_accuracy = accuracy_score(combined_labels, predictions_meal)\n",
    "play_accuracy = accuracy_score(combined_labels, predictions_meal)\n",
    "\n",
    "# Measure the diversity of errors\n",
    "meal_correct_play_incorrect = 0\n",
    "play_correct_meal_incorrect = 0\n",
    "both_incorrect = 0\n",
    "for meal_pred, play_pred, true_label in zip(predictions_meal, predictions_play, combined_labels):\n",
    "    if meal_pred == true_label and play_pred != true_label:\n",
    "        meal_correct_play_incorrect += 1\n",
    "    elif play_pred == true_label and meal_pred != true_label:\n",
    "        play_correct_meal_incorrect += 1\n",
    "    elif meal_pred != true_label and play_pred != true_label:\n",
    "        both_incorrect += 1\n",
    "\n",
    "# Calculate the proportions\n",
    "total_samples = len(combined_labels)\n",
    "meal_correct_play_incorrect_proportion = meal_correct_play_incorrect / total_samples\n",
    "play_correct_meal_incorrect_proportion = play_correct_meal_incorrect / total_samples\n",
    "both_incorrect_proportion = both_incorrect / total_samples\n",
    "\n",
    "print(f\"Proportion of samples where pipeline_meal is correct and pipeline_play is incorrect: {meal_correct_play_incorrect_proportion:.2%}\")\n",
    "print(f\"Proportion of samples where pipeline_play is correct and pipeline_meal is incorrect: {play_correct_meal_incorrect_proportion:.2%}\")\n",
    "print(f\"Proportion of samples where both models are incorrect: {both_incorrect_proportion:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meals = []\n",
    "plays = []\n",
    "\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    meals.append(cross_val_predict(pipeline_meal, meal_data_filtered.iloc[:,:-3], combined_labels, cv=cv))\n",
    "    plays.append(cross_val_predict(pipeline_play, play_data_filtered.iloc[:,:-3], combined_labels, cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "correlations = []\n",
    "for x, y in zip(meals, plays):\n",
    "    correlation, p_value = pearsonr(x, y)\n",
    "    correlations.append(correlation)\n",
    "\n",
    "print(f\"Correlation between meal and play model predictions: {np.mean(correlations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "kappa_avg = []\n",
    "# Calculate Cohen's Kappa\n",
    "for x, y in zip (meals, plays):\n",
    "    kappa_score = cohen_kappa_score(x, y)\n",
    "    kappa_avg.append(kappa_score)\n",
    "print(\"Cohen's Kappa score:\", np.mean(kappa_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p_values = []\n",
    "conf_matrix = []\n",
    "for x, y in zip (meals, plays):\n",
    "    contingency_table = confusion_matrix(x, y)\n",
    "    result = mcnemar(contingency_table)\n",
    "    p_values.append(result.pvalue)\n",
    "    conf_matrix.append(contingency_table)\n",
    "\n",
    "\n",
    "print(\"McNemar's test p-value:\", np.mean(p_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.mean(conf_matrix, axis=0)\n",
    "import seaborn as sns\n",
    "# Visualize the average confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=['no rad', 'rad'], yticklabels=['no rad', 'rad'])\n",
    "plt.xlabel('Meal')\n",
    "plt.ylabel('Play')\n",
    "plt.title('Confusion Matrix Meal vs Play')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreements = []\n",
    "for x,y in zip(meals, plays):\n",
    "    agreement_rate = np.mean(x == y)\n",
    "    agreements.append(agreement_rate)\n",
    "\n",
    "print(\"Prediction Agreement Rate:\", np.mean(agreements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the accuracy for the cases where the two classifiers output the same prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data_filtered.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "lenghts = []\n",
    "for x,y in zip(meals, plays):\n",
    "    same_predictions_indices = x == y\n",
    "    filtered_meal_data = meal_data_filtered[same_predictions_indices]\n",
    "    filtered_play_data = play_data_filtered[same_predictions_indices]\n",
    "    lenghts.append(len(filtered_meal_data))\n",
    "    true_labels_meal = meal_data_filtered.iloc[:, -1][same_predictions_indices]\n",
    "    true_labels_play = play_data_filtered.iloc[:, -1][same_predictions_indices]\n",
    "    # Calculate accuracy score\n",
    "    accuracy_meal = accuracy_score(true_labels_meal, x[same_predictions_indices])\n",
    "    # Calculate precision, recall, and f1 score\n",
    "    precision_meal = precision_score(true_labels_meal, x[same_predictions_indices], average='weighted')\n",
    "    recall_meal = recall_score(true_labels_meal, x[same_predictions_indices], average='weighted')\n",
    "    f1_meal = f1_score(true_labels_meal, x[same_predictions_indices], average='weighted')\n",
    "    accuracies.append(accuracy_meal)\n",
    "    precisions.append(precision_meal)\n",
    "    recalls.append(recall_meal)\n",
    "    f1_scores.append(f1_meal)\n",
    "\n",
    "\n",
    "print(f\"Accuracy for same predictions: {np.mean(accuracies)}\")\n",
    "print(f\"Precision for same predictions: {np.mean(precisions)}\")\n",
    "print(f\"Recall for same predictions: {np.mean(recalls)}\")\n",
    "print(f\"F1 Score for same predictions: {np.mean(f1_scores)}\")\n",
    "print(f\"Number of same predictions: {np.mean(lenghts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(accuracies), np.std(precisions), np.std(recalls), np.std(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30/49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on disagreement zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = play_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "\n",
    "y = play_data['label'].iloc[:, 0]   \n",
    "\n",
    "\n",
    "remaining_features = [feat for feat in X.columns if feat not in subset_pca]\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear', probability=True), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    })\n",
    "]\n",
    "  \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "conf_matrix_sum = np.zeros((2, 2))\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled= shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([ ('preprocessor', preprocessor), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': grid_search.best_params_.get('preprocessor__pca_pipeline__pca__n_components', None)\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, KFold, GroupKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    ('svc_linear', SVC(kernel='linear', probability=True), {\n",
    "        'svc_linear__C': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('svc_rbf', SVC(kernel='rbf', probability=True), {\n",
    "        'svc_rbf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "    }),\n",
    "    ('rf', RandomForestClassifier(), {\n",
    "        'rf__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__max_depth': [None, 5, 10, 15],\n",
    "        'rf__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('et', ExtraTreesClassifier(), {\n",
    "        'et__n_estimators': [50, 100, 200],\n",
    "        'et__max_depth': [None, 5, 10, 20],\n",
    "        'et__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    })\n",
    "]\n",
    "\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    meal_preds = cross_val_predict(pipeline_meal, meal_data_filtered.iloc[:, :-3], combined_labels, cv=cv)\n",
    "    play_preds = cross_val_predict(pipeline_play, play_data_filtered.iloc[:, :-3], combined_labels, cv=cv)\n",
    "    meal_probs = cross_val_predict(pipeline_meal, meal_data_filtered.iloc[:, :-3], combined_labels, cv=cv, method='predict_proba')\n",
    "    play_probs = cross_val_predict(pipeline_play, play_data_filtered.iloc[:, :-3], combined_labels, cv=cv, method='predict_proba')\n",
    "    dif_predictions_indices = meal_preds != play_preds\n",
    "\n",
    "    # Create DataFrames for probabilities and predictions\n",
    "    meal_results = pd.DataFrame(meal_probs, columns=['prob_0', 'prob_1'])\n",
    "    meal_results['pred'] = meal_preds\n",
    "    play_results = pd.DataFrame(play_probs, columns=['prob_0', 'prob_1'])\n",
    "    play_results['pred'] = play_preds\n",
    "    meal_results = meal_results[dif_predictions_indices]\n",
    "    play_results = play_results[dif_predictions_indices]\n",
    "    disagreements = pd.concat([meal_results, play_results], axis=0)\n",
    "    true_labels_meal = meal_data_filtered.iloc[:, -1][dif_predictions_indices]\n",
    "    true_labels_play = play_data_filtered.iloc[:, -1][dif_predictions_indices]\n",
    "    disagreements_labels = pd.concat([true_labels_meal, true_labels_play], axis=0)\n",
    "    X = disagreements\n",
    "    y = disagreements_labels\n",
    "    groups = disagreements.index\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    X_shuffled, y_shuffled, groups = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=cv, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, groups=groups, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, groups=groups, cv=cv, scoring='precision_macro', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, groups=groups, cv=cv, scoring='recall_macro', n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, groups=groups, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        \n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreements['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.6, 0.4]  # Adjust these weights based on model performance\n",
    "\n",
    "# Initialize lists to store the evaluation metrics\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "X = combined_data.iloc[:,:-3]\n",
    "y = combined_data['label'].iloc[:,0]\n",
    "groups = combined_data.index\n",
    "# Perform manual cross-validation\n",
    "for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit the base models\n",
    "    for name, model in base_models:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the predicted probabilities from each base model\n",
    "    meal_probs = pipeline_meal.predict_proba(X_test)[:, 1]\n",
    "    play_probs = pipeline_play.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Combine the predicted probabilities using weighted averaging\n",
    "    combined_probs = (weights[0] * meal_probs) + (weights[1] * play_probs)\n",
    "    \n",
    "    # Convert combined probabilities to class predictions\n",
    "    combined_preds = np.where(combined_probs > 0.5, 1, 0)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    f1_scores.append(f1_score(y_test, combined_preds, average='macro'))\n",
    "    precision_scores.append(precision_score(y_test, combined_preds, average='macro'))\n",
    "    recall_scores.append(recall_score(y_test, combined_preds, average='macro'))\n",
    "    accuracy_scores.append(accuracy_score(y_test, combined_preds))\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"F1 Score: {np.mean(f1_scores)}\")\n",
    "print(f\"Precision: {np.mean(precision_scores)}\")\n",
    "print(f\"Recall: {np.mean(recall_scores)}\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
