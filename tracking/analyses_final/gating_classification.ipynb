{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# import dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meal = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/meal/combined_features.csv', index_col=0)\n",
    "df_meal = df_meal.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])\n",
    "\n",
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df_meal['label'] = df_meal['label'].map({'no_rad': 0, 'rad': 1})\n",
    "df_play['label'] = df_play['label'].map({'no_rad': 0, 'rad': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_meal = df_meal.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_meal['group'] = df_meal['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_meal = df_meal.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "#df_grouped_meal['s_id'] = df_meal.groupby('group')['s_id'].first()\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "\n",
    "# Reset the index\n",
    "df_play = df_play.reset_index()\n",
    "\n",
    "# Create the 'group' column and group by it\n",
    "df_play['group'] = df_play['s_id'].str.split('_').str[0].astype(int)\n",
    "df_grouped_play = df_play.drop(columns=['s_id']).groupby('group').mean()\n",
    "\n",
    "# Set the index back to 's_id'\n",
    "# change index name to s_id\n",
    "df_grouped_meal.index.name = 's_id'\n",
    "df_grouped_play.index.name = 's_id'\n",
    "\n",
    "\n",
    "# Reset the index of the grouped dataframes\n",
    "#df_grouped_meal = df_grouped_meal.reset_index()\n",
    "#df_grouped_play = df_grouped_play.reset_index()\n",
    "\n",
    "# Add a new 'group' column to each DataFrame\n",
    "df_grouped_meal['group'] = 'meal'\n",
    "df_grouped_play['group'] = 'play'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = pd.read_excel('/Users/andrei-macpro/Documents/Data/classification/speech/classification.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index name to s_id\n",
    "speech.index.name = 's_id'\n",
    "# remove age column from speech\n",
    "speech = speech.drop(columns=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where True indicates a duplicated index\n",
    "mask = speech.index.duplicated(keep='first')\n",
    "\n",
    "# Use np.where to assign 'meal' to the first occurrence and 'play' to the second\n",
    "speech['group'] = np.where(mask, 'play', 'meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.set_index('group', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['label'] = speech['label'].map({'no_rad': 0, 'rad': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only meal group from speech index\n",
    "speech_meal = speech.xs('meal', level='group')\n",
    "speech_play = speech.xs('play', level='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = speech.reset_index()\n",
    "# set s_id as index for final_df\n",
    "speech.set_index('s_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_grouped_meal = df_grouped_meal.reset_index(level=0, drop=True)\n",
    "meal_data = pd.concat([df_grouped_meal, speech_meal], axis=1)\n",
    "meal_data = meal_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data = pd.concat([df_grouped_play, speech_play], axis=1)\n",
    "#meal_data = meal_data.drop(columns=['index'])\n",
    "play_data = play_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data = meal_data.add_suffix('_meal')\n",
    "play_data = play_data.add_suffix('_play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([meal_data, play_data], axis=0)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['group'] = all_data['group'].map({'meal': 0, 'play': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting scenario predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.dropna(inplace=True)\n",
    "X = all_data[['Proportion speech child','Proportion speech caregiver', 'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "            'average_proximity','correlation', 'cross_correlation','avg interval duration' ,'avg silence duration',\n",
    "       'variance_proximity', 'min_proximity','max_proximity'\n",
    " ]]\n",
    "\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "            'average_proximity',\n",
    "       'variance_proximity', 'min_proximity','max_proximity'\n",
    " ]\n",
    "\n",
    "y = all_data['group']\n",
    "\n",
    "\n",
    "remaining_features = [feat for feat in X.columns if feat not in subset_pca]\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "classifiers = [\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'), {}),\n",
    "    ('lr', LogisticRegression(), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4,6],\n",
    "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear', 'saga']\n",
    "    })\n",
    "]\n",
    "  \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled= shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([ ('preprocessor', preprocessor), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': grid_search.best_params_.get('preprocessor__pca_pipeline__pca__n_components', None)\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }\n",
    "    best_model_info = max(best_models.values(), key=lambda x: x['best_score'])\n",
    "    best_model = best_model_info['best_estimator']\n",
    "\n",
    "    # Use the best model to make predictions on the test set\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gating = best_models['lr']['best_estimator']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gating.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now get the the meal and play models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = meal_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "\n",
    "y = meal_data['label'].iloc[:, 0]   \n",
    "\n",
    "\n",
    "\n",
    "remaining_features = [feat for feat in X.columns if feat not in subset_pca]\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5, shuffle=False)\n",
    "classifiers = [\n",
    "('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    })\n",
    "]\n",
    "  \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled= shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([ ('preprocessor', preprocessor), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': grid_search.best_params_.get('preprocessor__pca_pipeline__pca__n_components', None)\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }\n",
    "    best_model_info = max(best_models.values(), key=lambda x: x['best_score'])\n",
    "    best_model = best_model_info['best_estimator']\n",
    "\n",
    "    # Use the best model to make predictions on the test set\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_play = best_models['svc_linear']['best_estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_meal = best_models['xgb']['best_estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = play_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "\n",
    "y = play_data['label'].iloc[:, 0]   \n",
    "\n",
    "\n",
    "\n",
    "remaining_features = [feat for feat in X.columns if feat not in subset_pca]\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "classifiers = [\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10]\n",
    "    })\n",
    "]\n",
    "  \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled= shuffle(X, y, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([ ('preprocessor', preprocessor), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro',  n_jobs=-1)\n",
    "        accuracy_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='accuracy',  n_jobs=-1)\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'pca_components': grid_search.best_params_.get('preprocessor__pca_pipeline__pca__n_components', None)\n",
    "        })\n",
    "        if name not in best_models or grid_search.best_score_ > best_models[name]['best_score']:\n",
    "                best_models[name] = {\n",
    "                    'best_estimator': grid_search.best_estimator_,\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'random_state': i\n",
    "                }\n",
    "    best_model_info = max(best_models.values(), key=lambda x: x['best_score'])\n",
    "    best_model = best_model_info['best_estimator']\n",
    "\n",
    "    # Use the best model to make predictions on the test set\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meal = meal_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "X_play = play_data[['Proportion speech child','cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    " ]]\n",
    "y_meal = meal_data['label'].iloc[:,0]\n",
    "y_play = play_data['label'].iloc[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meal['group'] = X_meal['group'].map({'meal': 0})\n",
    "X_play['group'] = X_play['group'].map({'play': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[['Proportion speech child','Proportion speech caregiver', 'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "            'average_proximity','correlation', 'cross_correlation','avg interval duration' ,'avg silence duration',\n",
    "       'variance_proximity', 'min_proximity','max_proximity'\n",
    " ]]\n",
    "y_gating = all_data['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_speeches = (pd.concat([X_meal['Proportion speech child'], X_play['Proportion speech child']], axis=1).dropna())\n",
    "combined_speeches.columns = ['Proportion speech child_meal', 'Proportion speech child_play']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_movements = (pd.concat([X_meal[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']], \n",
    "            X_play[['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']]], axis=1).dropna())\n",
    "combined_movements.columns = ['cg_movement_mean_meal', 'cg_movement_var_meal', 'cg_movement_min_meal', 'cg_movement_max_meal',\n",
    "            'child_movement_mean_meal', 'child_movement_var_meal', 'child_movement_min_meal', 'child_movement_max_meal',\n",
    "            'cg_movement_mean_play', 'cg_movement_var_play', 'cg_movement_min_play', 'cg_movement_max_play',\n",
    "            'child_movement_mean_play', 'child_movement_var_play', 'child_movement_min_play', 'child_movement_max_play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking with gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "all_data = pd.concat([combined_speeches, combined_movements], axis=1)\n",
    "\n",
    "subset_pca = ['cg_movement_mean_meal', 'cg_movement_var_meal', 'cg_movement_min_meal', 'cg_movement_max_meal',\n",
    "            'child_movement_mean_meal', 'child_movement_var_meal', 'child_movement_min_meal', 'child_movement_max_meal',\n",
    "            'cg_movement_mean_play', 'cg_movement_var_play', 'cg_movement_min_play', 'cg_movement_max_play',\n",
    "            'child_movement_mean_play', 'child_movement_var_play', 'child_movement_min_play', 'child_movement_max_play']\n",
    "\n",
    "\n",
    "remaining_features = [feat for feat in all_data.columns if feat not in subset_pca]\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "\n",
    "meta_models = [\n",
    "    ('svc', Pipeline([('preprocessor', preprocessor), ('svc', SVC(probability=True))])),\n",
    "    ('logistic_regression', Pipeline([('preprocessor', preprocessor), ('logistic_regression', LogisticRegression())])),\n",
    "    ('random_forest', Pipeline([('preprocessor', preprocessor), ('random_forest', RandomForestClassifier())]))\n",
    "]\n",
    "\n",
    "\n",
    "results = []\n",
    "param_grids = {\n",
    "    'svc': {\n",
    "        'svc__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc__kernel': ['linear', 'rbf'],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4, 6, 8]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'logistic_regression__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'logistic_regression__penalty': ['l1', 'l2'],\n",
    "        'logistic_regression__solver': ['liblinear', 'saga'],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4, 6, 8]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'random_forest__n_estimators': [10, 50, 100, 200],\n",
    "        'random_forest__max_depth': [None, 5, 10, 15],\n",
    "        'random_forest__min_samples_split': [2, 5, 10],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4, 6, 8]\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    meal_predictions = cross_val_predict(pipeline_meal, X_meal, y_meal, cv=cv, method='predict_proba')\n",
    "    play_predictions = cross_val_predict(pipeline_play, X_play, y_play, cv=cv, method='predict_proba')\n",
    "    gating_predictions = cross_val_predict(pipeline_gating, X_gating, y_gating, cv=cv, method='predict_proba')\n",
    "    gating_predictions_meal = gating_predictions[:len(X_meal)]\n",
    "    gating_predictions_play = gating_predictions[len(X_meal):]\n",
    "   # Concatenate predictions horizontally\n",
    "    combined_predictions_meal = np.hstack((meal_predictions, gating_predictions_meal))\n",
    "    combined_predictions_play = np.hstack((play_predictions, gating_predictions_play))\n",
    "    \n",
    "    # Create DataFrames for combined predictions\n",
    "    combined_predictions_meal_df = pd.DataFrame(combined_predictions_meal, index=X_meal.index)\n",
    "    combined_predictions_play_df = pd.DataFrame(combined_predictions_play, index=X_play.index)\n",
    "    \n",
    "    # Concatenate the combined predictions vertically\n",
    "    combined_predictions_df = pd.concat([combined_predictions_meal_df, combined_predictions_play_df], axis=1)\n",
    "    combined_predictions_df['Proportion speech child_meal'] = combined_speeches['Proportion speech child_meal']\n",
    "    combined_predictions_df['Proportion speech child_play'] = combined_speeches['Proportion speech child_play']\n",
    "    combined_predictions_df = pd.concat([combined_predictions_df, combined_movements], axis=1)\n",
    "    y = pd.concat([y_meal, y_play], axis=1).dropna().iloc[:,0]\n",
    "    combined_predictions_df.dropna(inplace=True)\n",
    "\n",
    "    X = combined_predictions_df\n",
    "    X.columns = X.columns.astype(str)\n",
    "\n",
    "    for model_name, meta_model in meta_models:\n",
    "        grid_search = GridSearchCV(meta_model, param_grids[model_name], cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X, y)\n",
    "        best_meta_model = grid_search.best_estimator_\n",
    "        \n",
    "        accuracy_scores = cross_val_score(best_meta_model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(best_meta_model, X, y,  cv=cv, scoring='precision_weighted', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(best_meta_model, X, y,  cv=cv, scoring='recall_weighted', n_jobs=-1)\n",
    "        f1_scores = cross_val_score(best_meta_model, X, y, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "        \n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'model': model_name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'f1_score': f1_scores.mean()\n",
    "        })\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('model').agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gating = all_data[['Proportion speech child','Proportion speech caregiver', 'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max', \n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max',\n",
    "            'average_proximity','correlation', 'cross_correlation','avg interval duration' ,'avg silence duration',\n",
    "       'variance_proximity', 'min_proximity','max_proximity'\n",
    " ]]\n",
    "y_gating = all_data['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use the predictions probabilities as weights for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X, all_data['group']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "data = all_data[['Proportion speech child', 'cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max','group']]\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "            'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max'\n",
    "         ]\n",
    "\n",
    "\n",
    "remaining_features = [feat for feat in data.columns if feat not in subset_pca]\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "meta_models = [\n",
    "    ('svc', Pipeline([('preprocessor', preprocessor), ('svc', SVC(probability=True))])),\n",
    "    ('logistic_regression', Pipeline([('preprocessor', preprocessor), ('logistic_regression', LogisticRegression())])),\n",
    "    ('random_forest', Pipeline([('preprocessor', preprocessor), ('random_forest', RandomForestClassifier())])),\n",
    "    ('xgb', Pipeline([('preprocessor', preprocessor), ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))]))\n",
    "]\n",
    "\n",
    "\n",
    "results = []\n",
    "param_grids = {\n",
    "    'svc': {\n",
    "        'svc__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "        'svc__kernel': ['linear', 'rbf'],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'logistic_regression__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'logistic_regression__penalty': ['l1', 'l2'],\n",
    "        'logistic_regression__solver': ['liblinear', 'saga'],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'random_forest__n_estimators': [10, 50, 100, 200],\n",
    "        'random_forest__max_depth': [None, 5, 10, 15],\n",
    "        'random_forest__min_samples_split': [2, 5, 10],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4]\n",
    "    },\n",
    "    'xgb': {\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 10],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    meal_predictions = cross_val_predict(pipeline_meal, X_meal, y_meal, cv=cv, method='predict_proba')\n",
    "    play_predictions = cross_val_predict(pipeline_play, X_play, y_play, cv=cv, method='predict_proba')\n",
    "    gating_predictions = cross_val_predict(pipeline_gating, X_gating, y_gating, cv=cv, method='predict_proba')\n",
    "    weights_meal = meal_predictions[:, 1]\n",
    "    weights_play = play_predictions[:, 1]\n",
    "    \n",
    "    # Use gated predictions as weights for raw features\n",
    "    weighted_X_meal = X_meal.multiply(weights_meal, axis=0)\n",
    "    weighted_X_play = X_play.multiply(weights_play, axis=0)\n",
    "   # Concatenate predictions horizontally\n",
    "    X = pd.concat([weighted_X_meal, weighted_X_play], axis=0)\n",
    "    X = pd.concat([X, all_data['group']], axis=1)\n",
    "\n",
    "    y = pd.concat([y_meal, y_play], axis=0)\n",
    "    combined_predictions_df.dropna(inplace=True)\n",
    "    groups = X.index\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "\n",
    "    for model_name, meta_model in meta_models:\n",
    "        grid_search = GridSearchCV(meta_model, param_grids[model_name], cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X, y, groups=groups)\n",
    "        best_meta_model = grid_search.best_estimator_\n",
    "        \n",
    "        accuracy_scores = cross_val_score(best_meta_model, X, y, groups=groups,cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        precision_scores = cross_val_score(best_meta_model, X, y, groups=groups, cv=cv, scoring='precision_weighted', n_jobs=-1)\n",
    "        recall_scores = cross_val_score(best_meta_model, X, y,  groups=groups,cv=cv, scoring='recall_weighted', n_jobs=-1)\n",
    "        f1_scores = cross_val_score(best_meta_model, X, y, groups=groups,cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "        \n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'model': model_name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean(),\n",
    "            'f1_score': f1_scores.mean()\n",
    "        })\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('model').agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_meals = pd.concat([y_meal, y_play], axis=0)\n",
    "y_label_plays = pd.concat([y_meal, y_play], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_predictions[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi task learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data and subset for PCA\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "data = pd.concat([X_meal, X_play], axis=0)\n",
    "subset_pca = ['cg_movement_mean', 'cg_movement_var', 'cg_movement_min', 'cg_movement_max',\n",
    "              'child_movement_mean', 'child_movement_var', 'child_movement_min', 'child_movement_max']\n",
    "\n",
    "remaining_features = [feat for feat in data.columns if feat not in subset_pca]\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA())\n",
    "    ]), subset_pca),\n",
    "    ('remaining', StandardScaler(), remaining_features)\n",
    "])\n",
    "\n",
    "# Define the base models\n",
    "xgb_model = MultiOutputClassifier(XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "rf_model = MultiOutputClassifier(RandomForestClassifier())\n",
    "\n",
    "# Define the parameter grids\n",
    "param_grids = {\n",
    "    'xgb': {\n",
    "        'xgb__estimator__n_estimators': [50, 100, 200],\n",
    "        'xgb__estimator__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__estimator__max_depth': [3, 5, 10],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4]\n",
    "    },\n",
    "    'rf': {\n",
    "        'rf__estimator__n_estimators': [10, 50, 100, 200],\n",
    "        'rf__estimator__max_depth': [None, 5, 10, 15],\n",
    "        'rf__estimator__min_samples_split': [2, 5, 10],\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [2, 4]\n",
    "    }\n",
    "}\n",
    "results = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    y_combined = pd.concat([y_label_meals, y_label_plays], axis=1)\n",
    "    gating_predictions = cross_val_predict(pipeline_gating, X_gating, y_gating, cv=cv, method='predict_proba')\n",
    "    groups = data.index\n",
    "    data['gating_0'] = gating_predictions[:,0]\n",
    "    data['gating_1'] = gating_predictions[:,1]\n",
    "    X_shuffled, y_shuffled, groups = shuffle(data, y_combined, groups, random_state=i)\n",
    "    # Combine the labels for multi-task learning\n",
    "\n",
    "    # Shuffle the data\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "\n",
    "\n",
    "    # Evaluate XGBoost\n",
    "    xgb_pipeline = Pipeline([('preprocessor', preprocessor), ('xgb', xgb_model)])\n",
    "    xgb_grid_search = GridSearchCV(xgb_pipeline, param_grids['xgb'], cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    xgb_grid_search.fit(X_shuffled, y_shuffled, groups = groups)\n",
    "\n",
    "    # Calculate the cross-validated scores\n",
    "    xgb_accuracy_scores = cross_val_score(xgb_grid_search.best_estimator_, X_shuffled, y_shuffled,groups = groups, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    xgb_f1_scores = cross_val_score(xgb_grid_search.best_estimator_, X_shuffled, y_shuffled,groups = groups, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "    xgb_precision_scores = cross_val_score(xgb_grid_search.best_estimator_, X_shuffled, y_shuffled,groups = groups, cv=cv, scoring='precision_weighted', n_jobs=-1)\n",
    "    xgb_recall_scores = cross_val_score(xgb_grid_search.best_estimator_, X_shuffled, y_shuffled,groups = groups, cv=cv, scoring='recall_weighted', n_jobs=-1)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'model': 'xgb',\n",
    "        'best_params': xgb_grid_search.best_params_,\n",
    "        'accuracy': xgb_accuracy_scores.mean(),\n",
    "        'f1_score': xgb_f1_scores.mean(),\n",
    "        'precision': xgb_precision_scores.mean(),\n",
    "        'recall': xgb_recall_scores.mean()\n",
    "    })\n",
    "\n",
    "    # Evaluate Random Forest\n",
    "    rf_pipeline = Pipeline([('preprocessor', preprocessor), ('rf', rf_model)])\n",
    "    rf_grid_search = GridSearchCV(rf_pipeline, param_grids['rf'], cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    rf_grid_search.fit(X_shuffled, y_shuffled, groups=groups)\n",
    "\n",
    "    # Calculate the cross-validated scores\n",
    "    rf_accuracy_scores = cross_val_score(rf_grid_search.best_estimator_, X_shuffled, y_shuffled,groups = groups, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    rf_f1_scores = cross_val_score(rf_grid_search.best_estimator_, X_shuffled, y_shuffled, groups = groups,cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "    rf_precision_scores = cross_val_score(rf_grid_search.best_estimator_, X_shuffled, y_shuffled,groups = groups, cv=cv, scoring='precision_weighted', n_jobs=-1)\n",
    "    rf_recall_scores = cross_val_score(rf_grid_search.best_estimator_, X_shuffled, y_shuffled,groups = groups, cv=cv, scoring='recall_weighted', n_jobs=-1)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'model': 'rf',\n",
    "        'best_params': rf_grid_search.best_params_,\n",
    "        'accuracy': rf_accuracy_scores.mean(),\n",
    "        'f1_score': rf_f1_scores.mean(),\n",
    "        'precision': rf_precision_scores.mean(),\n",
    "        'recall': rf_recall_scores.mean()\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('model').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
