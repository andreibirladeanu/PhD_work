{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sorted_alpha import sorted_alpha\n",
    "from moviepy.editor import VideoFileClip\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample all to 25 fps\n",
    "def resample_df(df, original_fps, target_fps):\n",
    "    # Convert frame indices to time-based index\n",
    "    df['time'] = pd.to_timedelta(df.index / original_fps, unit='s')\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    # Resample the data to the target fps\n",
    "    resample_interval = f'{int(1e9 / target_fps)}N'  # Nanoseconds interval\n",
    "    df_resampled = df.resample(resample_interval).mean().dropna()\n",
    "    \n",
    "    # Convert time-based index back to frame indices\n",
    "    df_resampled.index = (df_resampled.index.total_seconds() * target_fps).astype(int)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_folder = '/Users/andrei-macpro/Documents/Data/openpose/play/tracking/tracking/' \n",
    "video_folder = '/Users/andrei-macpro/Documents/Data/videos/play_videos'   # Assuming video files \n",
    "\n",
    "time_series = {}\n",
    "for folder_name in sorted(os.listdir(tracking_folder)):\n",
    "    print(folder_name)\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    file_path = os.path.join(tracking_folder, folder_name)\n",
    "    tracks = {}\n",
    "    clip = VideoFileClip(os.path.join(video_folder, folder_name+'.mp4',))\n",
    "    if clip.fps > 25:\n",
    "        for file in sorted(os.listdir(file_path)):\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            df_resampled = resample_df(df, original_fps=clip.fps, target_fps=25)\n",
    "            if folder_name not in tracks:\n",
    "                tracks[folder_name] = []\n",
    "            tracks[folder_name].append(df_resampled)\n",
    "        # Combine the resampled DataFrames\n",
    "        all_dfs = [df for dfs in tracks.values() for df in dfs]\n",
    "        combined = pd.concat(all_dfs, axis=1).dropna()\n",
    "        diff_x = combined['x_child'].diff().dropna()\n",
    "        diff_y = combined['y_child'].diff().dropna()\n",
    "        child_movement = np.sqrt(diff_x**2 + diff_y**2)\n",
    "        # Calculate the differences between consecutive rows for 'x_cg' and 'y_cg'\n",
    "        diff_x = combined['x_cg'].diff().dropna()\n",
    "        diff_y = combined['y_cg'].diff().dropna()\n",
    "        caregiver_movement = np.sqrt(diff_x**2 + diff_y**2)\n",
    "        # Calculate proximiy\n",
    "        diff_x = combined['x_cg'] - combined['x_child']\n",
    "        diff_y = combined['y_cg'] - combined['y_child']\n",
    "        proximity = np.sqrt(diff_x**2 + diff_y**2)\n",
    "        merged_df = pd.concat([child_movement, caregiver_movement, proximity], axis=1).dropna()\n",
    "        merged_df.columns = ['child_movement', 'caregiver_movement', 'proximity']   \n",
    "        time_series[folder_name] = merged_df\n",
    "    else:\n",
    "        for file in sorted(os.listdir(file_path)):\n",
    "            if file == \".DS_Store\":\n",
    "                continue    \n",
    "            filepath = os.path.join(tracking_folder,folder_name, file)\n",
    "            df = pd.read_csv(filepath, index_col=0)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df.columns = ['x_' + file.split('.')[0], 'y_' + file.split('.')[0]]\n",
    "            if folder_name not in tracks:\n",
    "                tracks[folder_name] = []\n",
    "            tracks[folder_name].append(df)\n",
    "        all_dfs = [df for dfs in tracks.values() for df in dfs]\n",
    "        combined = pd.concat(all_dfs, axis=1).dropna()\n",
    "        diff_x = combined['x_child'].diff().dropna()\n",
    "        diff_y = combined['y_child'].diff().dropna()\n",
    "        child_movement = np.sqrt(diff_x**2 + diff_y**2)\n",
    "        # Calculate the differences between consecutive rows for 'x_cg' and 'y_cg'\n",
    "        diff_x = combined['x_cg'].diff().dropna()\n",
    "        diff_y = combined['y_cg'].diff().dropna()\n",
    "        caregiver_movement = np.sqrt(diff_x**2 + diff_y**2)\n",
    "        # Calculate proximiy\n",
    "        diff_x = combined['x_cg'] - combined['x_child']\n",
    "        diff_y = combined['y_cg'] - combined['y_child']\n",
    "        proximity = np.sqrt(diff_x**2 + diff_y**2)\n",
    "        merged_df = pd.concat([child_movement, caregiver_movement, proximity], axis=1).dropna()\n",
    "        merged_df.columns = ['child_movement', 'caregiver_movement', 'proximity']   \n",
    "        time_series[folder_name] = merged_df    \n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the averaged DataFrames\n",
    "averaged_time_series = {}\n",
    "\n",
    "# Iterate over the dictionary\n",
    "for key, df in time_series.items():\n",
    "    # Compute the mean of each column\n",
    "    averaged_df = df.mean()\n",
    "    # Store the result in the new dictionary\n",
    "    averaged_time_series[key] = averaged_df\n",
    "averaged_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df = pd.DataFrame(averaged_time_series)\n",
    "# Transpose the DataFrame to have the keys as rows\n",
    "averaged_df = averaged_df.T\n",
    "averaged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df['label'] = df_play['label']\n",
    "averaged_df.index = averaged_df.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play = pd.read_csv('/Users/andrei-macpro/Documents/Data/tracking/features/play/combined_features.csv', index_col=0)\n",
    "df_play = df_play.drop(columns=['Age', 'DAI', 'Rinab', 'IQ_T2', 'duration_meal', 'duration_play','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = averaged_df\n",
    "\n",
    "\n",
    "# Map 'no_rad' to 0 and 'rad' to 1\n",
    "df['label'] = df['label'].map({'no_rad': 0, 'rad': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search for each classifier\n",
    "#X = df.drop(['label'], axis=1)\n",
    "X = df[['child_movement']]\n",
    "y = df['label']\n",
    "groups = df.index\n",
    "\n",
    "# Create a GroupKFold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define the classifiers and their parameters\n",
    "classifiers = [\n",
    "('lr', LogisticRegression(), {'lr__C': [0.01, 0.1, 1, 10, 100], 'lr__penalty': ['l1', 'l2'], 'lr__solver': ['liblinear', 'saga']}),\n",
    "    ('svc_linear', SVC(kernel='linear'), {'svc_linear__C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('svc_rbf', SVC(kernel='rbf'), {'svc_rbf__C': [0.01, 0.1, 1, 10, 100], 'svc_rbf__gamma': [0.01, 0.1, 1, 10, 100]}),\n",
    "    ('rf', RandomForestClassifier(), {'rf__n_estimators': [10, 50, 100, 200], 'rf__max_depth': [None, 5, 10, 15], 'rf__min_samples_split': [2, 5, 10]})\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform the grid search 10 times with different random states\n",
    "for i in range(10):\n",
    "    # Shuffle the data with a different random state each time\n",
    "    X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=i)\n",
    "\n",
    "    # Perform a grid search for each classifier\n",
    "    for name, classifier, params in classifiers:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name, classifier)])\n",
    "        grid_search = GridSearchCV(pipeline, params, cv=gkf, n_jobs=-1)\n",
    "        grid_search.fit(X_shuffled, y_shuffled, groups=groups_shuffled)\n",
    "\n",
    "        # Calculate the cross-validated F1 score, precision, and recall\n",
    "        f1_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='f1_macro', groups=groups_shuffled, n_jobs=-1)\n",
    "        precision_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='precision_macro', groups=groups_shuffled, n_jobs=-1)\n",
    "        recall_scores = cross_val_score(grid_search.best_estimator_, X_shuffled, y_shuffled, cv=gkf, scoring='recall_macro', groups=groups_shuffled, n_jobs=-1)\n",
    "\n",
    "        # Store the results in a dictionary and add it to the list\n",
    "        results.append({\n",
    "            'random_state': i,\n",
    "            'classifier': name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'f1_score': f1_scores.mean(),\n",
    "            'precision': precision_scores.mean(),\n",
    "            'recall': recall_scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('classifier').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
