{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyannote.core\n",
    "from pydub import AudioSegment\n",
    "file = {'uri':'1043_meal', 'audio':'/Users/andrei-macpro/Documents/Data/Audio/1043_meal.wav'}\n",
    "import torch\n",
    "# speech activity detection model trained on AMI training set\n",
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami')\n",
    "# obtain raw SAD scores (as `pyannote.core.SlidingWindowFeature` instance)\n",
    "sad_scores = sad(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.utils.signal import Binarize\n",
    "binarize = Binarize(offset=0.52, onset=0.52, log_scale=True, \n",
    "                    min_duration_off=0.1, min_duration_on=0.1)\n",
    "\n",
    "# speech regions (as `pyannote.core.Timeline` instance)\n",
    "speech = binarize.apply(sad_scores, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamps to miliseconds \n",
    "type(speech[0])\n",
    "# speech is a timeline made of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract speech and non-speech segments (start and end time) as a dictionary\n",
    "timestamps_speech=dict(speech)\n",
    "timestamps_silence = dict(speech.gaps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=AudioSegment.from_wav('/Users/andrei-macpro/Documents/Data/Audio/1043_meal.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydub works in miliseconds so we're going to convert the timestamps from the pyannote seconds to miliseconds\n",
    "start_times_speech=[x*1000 for x in list(timestamps_speech.keys())]\n",
    "end_times_speech=[x*1000 for x in list(timestamps_speech.values())]\n",
    "start_times_silence=[x*1000 for x in list(timestamps_silence.keys())]\n",
    "end_times_silence=[x*1000 for x in list(timestamps_silence.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_segments=[file[start_time:end_time] for start_time,end_time in zip(start_times_speech, end_times_speech)]\n",
    "silence_segments=[file[start_time:end_time] for start_time,end_time in zip(start_times_silence, end_times_silence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the concatenated speech and silence segments to disk\n",
    "sum(speech_segments).export('/Users/andrei-macpro/Documents/Data/Audio/speech.wav', format=\"wav\")\n",
    "sum(silence_segments).export('/Users/andrei-macpro/Documents/Data/Audio/non-speech.wav', format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to write a function that does everything sequentially while saving in memory only the result of binarization \n",
    "# and not the actual sliding window feature\n",
    "# what we're going to need:\n",
    "from pyannote.audio.utils.signal import Binarize\n",
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami') # initialize the model; this isn't taking up too much memory\n",
    "binarize = Binarize(offset=0.52, onset=0.52, log_scale=True, \n",
    "                    min_duration_off=0.1, min_duration_on=0.1)\n",
    "\n",
    "# iterate through a list of speech/non-speech timelines that can store the binarized scores for each file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/andrei-macpro/Documents/Data/Audio/Audio/Meal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/Users/andrei-macpro/Documents/Data/Audio/Audio/Meal'  #if string starts with slash it is considered absolute\n",
    "dirs = os.listdir( path )\n",
    "file_names=sorted([i for i in os.listdir(\".\") if not i.startswith(\".\")])\n",
    "file_names = [x.replace('.wav','') for x in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to only return the timeline not the sliding window \n",
    "def timeline(file_name):\n",
    "    speech=binarize.apply(sad(file_name),dimension=1)\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami') # ok so this isn't taking up too much memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeline(file_name):\n",
    "    sad_scores=sad(file_name)\n",
    "    speech=binarize.apply(sad_scores,dimension=1)\n",
    "    del sad_scores\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps=timeline(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sad_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gc module \n",
    "import gc \n",
    "  \n",
    "# Returns the number of \n",
    "# objects it has collected \n",
    "# and deallocated \n",
    "collected = gc.collect() \n",
    "  \n",
    "# Prints Garbage collector  \n",
    "# as 0 object \n",
    "print(\"Garbage collector: collected\", \n",
    "          \"%d objects.\" % collected) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "foo = range(10000000)\n",
    "del foo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/andrei-macpro/Documents/Data/Audio/Audio/Meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that returns a list\n",
    "def timeline(file_name):\n",
    "    timelines=list()\n",
    "    sad_scores=sad(file_name)\n",
    "    speech=binarize.apply(sad_scores,dimension=1)\n",
    "    del sad_scores\n",
    "    timelines.append(speech)\n",
    "    return timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeline(file_names):\n",
    "    trial_list=list()\n",
    "    for file_name in file_names:\n",
    "        sad_score=sad(file_name)\n",
    "        speech=binarize.apply(sad_scores,dimension=1)\n",
    "        del sad_scores\n",
    "        trial_list.append(speech)\n",
    "    return trial_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeline(file_names):\n",
    "    temp_list1=list()\n",
    "    temp_list2=list()\n",
    "    for file_name in file_names:\n",
    "        sad_scores=sad(file_name)\n",
    "        speech=binarize.apply(sad_scores,dimension=1)\n",
    "        del sad_scores\n",
    "        temp_list1.append(file_name.get('audio'))\n",
    "        temp_list2.append(speech)\n",
    "    trial_dict=dict(zip(temp_list1,temp_list2))\n",
    "    return trial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary for files and filenames\n",
    "list_of_dictionaries=list()\n",
    "files = dict()\n",
    "for file_name in file_names:\n",
    "    list_of_dictionaries.append({'audio':str(file_name)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict=timeline(list_of_dictionaries[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict['1043_meal.wav'].duration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_speech=dict(new_dict['1047_meal.wav'])\n",
    "timestamps_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new dictionaries that can hold the speech of each file\n",
    "dictionary1={}\n",
    "for file,timeline in zip(file_names, new_dict):\n",
    "    dictionary1= dictionary1.update({str(file):new_dict[str(file)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dictionary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict1 = dict.fromkeys([file for file in file_names], new_dict.values()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
