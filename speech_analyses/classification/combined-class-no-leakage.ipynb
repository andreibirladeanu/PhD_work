{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Classification/speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('classification.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,2:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Proportion speech child'].to_numpy()\n",
    "X = X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.array(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list()\n",
    "#steps.append(('scaler', StandardScaler()))\n",
    "steps.append(('model', SVC(kernel='linear')))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipeline, X_shuffled, y_shuffled, scoring='accuracy', cv=cv, n_jobs=-1, groups=groups_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = cross_val_predict(model, X_shuffled, y_shuffled, cv=cv, groups = groups_shuffled, n_jobs=-1)\n",
    "conf_mat = confusion_matrix(y_shuffled, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: %.4f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "n_shuffles = 10\n",
    "coefficients = []\n",
    "for i in range(n_shuffles):\n",
    "    X_shuffle, y_shuffle, groups_shuffle = shuffle(X, y, groups, random_state=i)\n",
    "    scores = cross_val_score(pipeline, X_shuffle, y_shuffle, groups=groups_shuffle, scoring='accuracy', cv=cv )\n",
    "    print('Accuracy: %.4f (%.3f)' % (mean(scores)*100, std(scores)*100))\n",
    "    accuracy.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy and coefficients \n",
    "accuracies_combined= []\n",
    "final_coef =[]\n",
    "recall_all = []\n",
    "precision_all=[]\n",
    "for i in range(10):\n",
    "    X_shuffle, y_shuffle, groups_shuffle = shuffle(X, y, groups, random_state=i)\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    coefficients = []\n",
    "    accuracies = []\n",
    "    recalls = []\n",
    "    precisions=[]\n",
    "    for train_index, test_index in kf.split(X_shuffle,y_shuffle, groups_shuffle):\n",
    "\n",
    "            # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X_shuffle[train_index], X_shuffle[test_index]\n",
    "        y_train_, y_test_ = y_shuffle[train_index], y_shuffle[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "            # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "        print(model.predict(X_test_))\n",
    "\n",
    "        accuracy = accuracy_score(y_test_, model.predict(X_test_))\n",
    "        accuracies.append(accuracy)\n",
    "        recall = recall_score(y_test_, model.predict(X_test_))\n",
    "        precision = precision_score(y_test_, model.predict(X_test_))\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        print(f\"Accuracy {accuracy:3f}\")\n",
    "        print(model.coef_)\n",
    "        coefficients.append(model.coef_)\n",
    "    accuracies_combined.append(mean(accuracies))\n",
    "    recall_all.append(mean(recalls))\n",
    "    precision_all.append(mean(precisions))\n",
    "    final_coef.append(mean(coefficients, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean( accuracies_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_all, precision_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [data.iloc[:,2:12].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_coefs = mean(abs(final_coef[0]), axis=0)\n",
    "std_combined = np.std(abs(final_coef[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(abs(final_coef[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_combined, combined_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features_imp = pd.DataFrame(list(zip(combined_coefs, std_combined)),index=features_names, columns = ['coefficient', 'std'])\n",
    "combined_features_imp.T.to_csv('combined_coefs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_shuffle, y_shuffle, groups_shuffle = shuffle(X, y, groups, random_state=i)\n",
    "kf = GroupKFold(n_splits=5)\n",
    "predictions = []\n",
    "labels = []\n",
    "s_id=[]\n",
    "for train_index, test_index in kf.split(X_shuffle,y_shuffle, groups_shuffle):\n",
    "\n",
    "            # scale training and test data based on statistics of only training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_, X_test_ = X_shuffle[train_index], X_shuffle[test_index]\n",
    "    y_train_, y_test_ = y_shuffle[train_index], y_shuffle[test_index]\n",
    "    groups_train_, groups_test_ = groups_shuffle[train_index], groups_shuffle[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_)\n",
    "            # normalize data\n",
    "    X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(X_train_, y_train_)\n",
    "    print(model.predict(X_test_))\n",
    "        \n",
    "\n",
    "    labels.append(y_test_)\n",
    "    predictions.append(model.predict(X_test_))\n",
    "    s_id.append(groups_test_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_shuffle, y_shuffle, groups=groups_shuffle, cv=kf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "tuple_index_pred = list()\n",
    "while k<5:\n",
    "    for x, y,z in zip(s_id[k], predictions[k], labels[k]):\n",
    "        tuple_index_pred.append((x,y,z))\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = list()\n",
    "t_list2 = list()\n",
    "t_list3=list()\n",
    "for x in tuple_index_pred:\n",
    "    t_list.append(x[0])\n",
    "    t_list2.append(x[1])\n",
    "    t_list3.append(x[2])\n",
    "\n",
    "final_df = pd.DataFrame(list(zip(t_list, t_list2, t_list3)), columns = ['s_id', 'prediction', 'label'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(list(zip(labels, predictions, s_id)), columns = ['label', 'prediction', 's_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['prediction']==1].duplicated().describe() #41 with False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['prediction']==0].duplicated().describe() # 42 with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. discard all people where there is a disagree in label \n",
    "final_df['discarded'] = final_df[['s_id','prediction']].duplicated(keep=False) # duplicated means True; disagree in label means false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['discarded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['s_id','prediction']].duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those where there is agree  in label ; so duplicated ones \n",
    "final_df.loc[final_df['discarded']==True] # so we have 76 where there is agreement in label in the classifier output\n",
    "# and 41 where there isn't\n",
    "# so 76 with both recordings in agreement; so actually 38 people out of the 56 that have both recordings\n",
    "# so the rest of 18 people we leave to clinician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df.loc[final_df['discarded']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(final_df.loc[final_df['discarded']==True].sort_values(by=['s_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree = final_df.loc[final_df['discarded']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree.sort_values(by=['s_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree= df_disagree.drop([113, 52, 89, 43, 31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_disagree.loc[df_disagree['label']==0])  # 14 out of 46 \n",
    "# 7 out of 23 children for whom there was a disagree actually got a no_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discarded = final_df.loc[final_df['discarded']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_discarded) # 38 people with both recordings agreeement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_discarded.loc[final_discarded['discarded']==True]\n",
    "accuracy=0\n",
    "for x,y in zip(final_discarded['label'], final_discarded['prediction']):\n",
    "        if x==y:\n",
    "            accuracy +=1\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so 29 are correct out of 56 and 11 are wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy/76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "for x,y in zip(final_discarded['label'], final_discarded['prediction']):\n",
    "        if x!=y and x==1:\n",
    "            fp +=1\n",
    "        if x!=y and x==0:\n",
    "            fn +=1\n",
    "        if x==y and x==1:\n",
    "            tp +=1\n",
    "        if x==0 and y ==0:\n",
    "            tn +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "specificity = tn/(tn+fp)\n",
    "recall, precision, specificity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['s_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discarded.sort_values(by='s_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
