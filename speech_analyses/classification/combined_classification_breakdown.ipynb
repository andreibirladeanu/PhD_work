{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools \n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Classification/speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('classification.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.array(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=1)\n",
    "group_k_fold = GroupKFold(n_splits=6)\n",
    "splits = group_k_fold.split(X_shuffled, y_shuffled, groups_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indexes_train = list()\n",
    "list_indexes_test =list()\n",
    "temp_list = list()\n",
    "predictions = list()\n",
    "y_test_index = list()\n",
    "\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled):\n",
    "    model.fit(X_shuffled[train_index], y_shuffled[train_index])\n",
    "    temp_list = model.predict(X_shuffled[test_index])\n",
    "    temp_list2=list()\n",
    "    temp_list3=list()\n",
    "    temp_list2.append(groups_shuffled[test_index])\n",
    "    temp_list3.append(y_shuffled[test_index])\n",
    "    y_test_index.append(temp_list3)\n",
    "    list_indexes_test.append(temp_list2)\n",
    "    #coefs.append(model.coef_)\n",
    "    predictions.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=list()\n",
    "for train_index, test_index in group_k_fold.split(X_shuffled, y=y_shuffled, groups=groups_shuffled):\n",
    "    labels.append(y_shuffled[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "tuple_index_pred = list()\n",
    "while k<6:\n",
    "    for x, y,z in zip(list_indexes_test[k][0], predictions[k], y_test_index[k][0]):\n",
    "        tuple_index_pred.append((x,y,z))\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = list()\n",
    "t_list2 = list()\n",
    "t_list3=list()\n",
    "for x in tuple_index_pred:\n",
    "    t_list.append(x[0])\n",
    "    t_list2.append(x[1])\n",
    "    t_list3.append(x[2])\n",
    "\n",
    "final_df = pd.DataFrame(list(zip(t_list, t_list2, t_list3)), columns = ['index', 'label', 'ground_truth'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['label']==1].duplicated().describe() #46 with False and 19 with True so 19 people have both 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['label']==0].duplicated().describe() #38 with False and 14 with True so 14 people have both 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of 56 we have 12 that have both recordings correct and 44 where either both are or only one is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. discard all people where there is a disagree in label \n",
    "final_df['discarded'] = final_df[['index','label']].duplicated(keep=False) # duplicated means True; disagree in label means false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those where there is agree  in label ; so duplicated ones \n",
    "final_df.loc[final_df['discarded']==True] # so we have 66 where there is agreement in label in the classifier output\n",
    "# and 51 where there isn't\n",
    "# so 66 with both recordings in agreement; so actually 33 people out of the 56 that have both recordings\n",
    "# so the rest of 23 people we leave to clinician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree = final_df.loc[final_df['discarded']==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_disagree)-5 = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree= df_disagree.drop([102, 73, 83, 44, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate accuracy over these\n",
    "final_discarded = final_df.loc[final_df['discarded']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=0\n",
    "for x,y in zip(final_discarded['label'], final_discarded['ground_truth']):\n",
    "        if x==y:\n",
    "            accuracy +=1\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "for x,y in zip(final_discarded['label'], final_discarded['ground_truth']):\n",
    "        if x!=y and x==1:\n",
    "            fp +=1\n",
    "        if x!=y and x==0:\n",
    "            fn +=1\n",
    "        if x==y and x==1:\n",
    "            tp +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy/66*100 # out of those that there's agreement 69% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what would we expect if it was random: \n",
    "final_discarded['ground_truth'].value_counts() # 18 people with no rad and 15 with rad out of those we keep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot new cm and maybe ROC \n",
    "# confusion matrix at person level \n",
    "y_true = np.array(final_discarded['ground_truth'])\n",
    "y_pred = np.array(final_discarded['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_person = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.imshow(cm_person,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix Person level')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "tick_marks = np.arange(len(set(y_shuffled[test_index]))) # length of classes\n",
    "class_labels = ['no rad','rad']\n",
    "tick_marks\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks(tick_marks,class_labels)\n",
    "# plotting text value inside cells\n",
    "thresh = cm_person.max() / 2.\n",
    "for i,j in itertools.product(range(cm_person.shape[0]),range(cm_person.shape[1])):\n",
    "    plt.text(j,i,format(cm_person[i,j],'d'),horizontalalignment='center',color='white' if cm_person[i,j] >thresh else 'black')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
