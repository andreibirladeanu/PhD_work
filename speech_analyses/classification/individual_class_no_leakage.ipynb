{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Classification/speech')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('features.xlsx', sheet_name=\"Meal\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = np.array(data[\"Subject_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:11].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel='linear')\n",
    "steps = list()\n",
    "steps.append(('scaler', StandardScaler()))\n",
    "steps.append(('model', classifier))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "scores = cross_val_score(pipeline, X,y, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy and coefficients \n",
    "accuracies_meal = []\n",
    "final_coef =[]\n",
    "for i in range(10):\n",
    "    kf = KFold(n_splits=5, random_state=i, shuffle=True)\n",
    "    coefficients_meal = []\n",
    "    accuracies = []\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "\n",
    "            # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X[train_index], X[test_index]\n",
    "        y_train_, y_test_ = y[train_index], y[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "            # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "        print(model.predict(X_test_))\n",
    "\n",
    "        accuracy = accuracy_score(y_test_, model.predict(X_test_))\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Accuracy {accuracy:3f}\")\n",
    "        print(model.coef_)\n",
    "        coefficients_meal.append(model.coef_)\n",
    "    accuracies_meal.append(mean(accuracies))\n",
    "    final_coef.append(mean(coefficients_meal, axis=1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision and recall \n",
    "recall_meal = []\n",
    "precision_meal=[]\n",
    "f_score_meal = []\n",
    "for i in range(10):\n",
    "    kf = KFold(n_splits=5, random_state=i, shuffle=True)\n",
    "    recalls = []\n",
    "    precisions=[]\n",
    "    f_scores=[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "            # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X[train_index], X[test_index]\n",
    "        y_train_, y_test_ = y[train_index], y[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "            # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "\n",
    "        recall = recall_score(y_test_, model.predict(X_test_))\n",
    "        precision = precision_score(y_test_, model.predict(X_test_))\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1 = f1_score(y_test_, model.predict(X_test_))\n",
    "        f_scores.append(f1)\n",
    "        \n",
    "    recall_meal.append(mean(recalls))\n",
    "    precision_meal.append(mean(precisions))\n",
    "    f_score_meal.append(mean(f_scores))\n",
    "recall_meal, precision_meal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(f_score_meal), statistics.stdev(f_score_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(recall_meal), statistics.stdev(recall_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(precision_meal), statistics.stdev(precision_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(mean(final_coef, axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_coefficients = mean([mean(abs(coef), axis=1) for coef in final_coef], axis=1)\n",
    "meal_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_coefs = np.array([mean(abs(coef), axis=1) for coef in final_coef])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_coefs = np.std(iterations_coefs, axis=1)\n",
    "std_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(accuracies_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "statistics.stdev(accuracies_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meal_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names_meal = [data.iloc[:,1:11].columns]\n",
    "features_names_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names_meal[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_features_imp = pd.DataFrame(list(zip(features_names_meal[0], meal_coefficients, std_coefs)), columns = ['feature', 'coefficient', 'std'])\n",
    "meal_features_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_features_imp.T.to_csv('feature_coefs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "coefs_meal = sum(coefficients_meal)/len(coefficients_meal)\n",
    "coefs_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "    \n",
    "features_names_meal = [data.iloc[:,1:12].columns]\n",
    "pd.Series(np.transpose(abs(coefs_meal[0])), index=features_names_meal[0]).nlargest(12).plot(kind='barh', figsize=(10,10))\n",
    "#plt.savefig('meal_features.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_meal = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    scores = cross_val_score(pipeline, X,y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    accuracy_meal.append(mean(scores)*100)\n",
    "\n",
    "mean(accuracy_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_meal = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i\n",
    "    scores_recall_meal = cross_val_score(pipeline, X,y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "    recall_meal.append(mean(scores_recall_meal)*100)\n",
    "    \n",
    "mean(recall_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_meal = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i\n",
    "    scores_precision_meal = cross_val_score(pipeline, X,y, scoring='precision', cv=cv, n_jobs=-1)\n",
    "    precision_meal.append(mean(scores_precision_meal)*100)\n",
    "    \n",
    "mean(precision_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(accuracy_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the confusion matrix \n",
    "\n",
    "kf = KFold(n_splits=5, random_state=i, shuffle=True)\n",
    "cm_holder = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "        # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X[train_index], X[test_index]\n",
    "        y_train_, y_test_ = y[train_index], y[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "        # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "        print(model.predict(X_test_))\n",
    "        cm_temp = []\n",
    "        cm_temp.append(confusion_matrix(y_test_, model.predict(X_test_)))\n",
    "        print(cm_temp)\n",
    "        accuracy = accuracy_score(y_test_, model.predict(X_test_))\n",
    "        print(f\"Accuracy {accuracy:3f}\")\n",
    "        print(model.coef_)\n",
    "        coefficients_meal.append(model.coef_)\n",
    "        cm_holder.append(sum(cm_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_meal= sum(cm_holder)\n",
    "matrix_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.imshow(matrix_meal,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Meal', fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize=16)\n",
    "plt.ylabel('Actual', fontsize=16)\n",
    "tick_marks = np.arange(len(set(y_test_))) # length of classes\n",
    "class_labels = ['no rad','rad']\n",
    "tick_marks\n",
    "plt.xticks(tick_marks,class_labels, fontsize=16)\n",
    "plt.yticks(tick_marks,class_labels, fontsize=16)\n",
    "# plotting text value inside cells\n",
    "thresh = matrix_meal.max() / 2.\n",
    "for i,j in itertools.product(range(matrix_meal.shape[0]),range(matrix_meal.shape[1])):\n",
    "    plt.text(j,i,format(matrix_meal[i,j],'d'),horizontalalignment='center',color='black' if matrix_meal[i,j] >thresh else 'black', fontsize='xx-large')\n",
    "plt.show();\n",
    "plt.savefig('matrix_meal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do it for play\n",
    "data = pd.read_excel('features.xlsx', sheet_name=\"Play\", engine='openpyxl')\n",
    "X = data.iloc[:,1:11].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])\n",
    "subject_id = np.array(data[\"Subject_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy and coefficients \n",
    "accuracies_play = []\n",
    "final_coef =[]\n",
    "for i in range(10):\n",
    "    kf = KFold(n_splits=5, random_state=i, shuffle=True)\n",
    "    coefficients_play = []\n",
    "    accuracies = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "            # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X[train_index], X[test_index]\n",
    "        y_train_, y_test_ = y[train_index], y[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "            # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "        print(model.predict(X_test_))\n",
    "\n",
    "        accuracy = accuracy_score(y_test_, model.predict(X_test_))\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Accuracy {accuracy:3f}\")\n",
    "        print(model.coef_)\n",
    "        coefficients_play.append(model.coef_)\n",
    "    accuracies_play.append(mean(accuracies))\n",
    "    final_coef.append(mean(coefficients_play, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision and recall \n",
    "recall_play = []\n",
    "precision_play=[]\n",
    "f1_play = []\n",
    "for i in range(10):\n",
    "    kf = KFold(n_splits=5, random_state=i, shuffle=True)\n",
    "    recalls = []\n",
    "    precisions=[]\n",
    "    f_scores=[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "            # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X[train_index], X[test_index]\n",
    "        y_train_, y_test_ = y[train_index], y[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "            # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "\n",
    "        recall = recall_score(y_test_, model.predict(X_test_))\n",
    "        precision = precision_score(y_test_, model.predict(X_test_))\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1 = f1_score(y_test_, model.predict(X_test_))\n",
    "        f_scores.append(f1)\n",
    "    recall_play.append(mean(recalls))\n",
    "    precision_play.append(mean(precisions))\n",
    "    f1_play.append(mean(f_scores))\n",
    "recall_play, precision_play\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(recall_play), statistics.stdev(recall_play), mean(precision_play), statistics.stdev(precision_play), mean(accuracies_play), statistics.stdev(accuracies_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(f1_play), statistics.stdev(f1_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_coefficients = mean([mean(abs(coef), axis=1) for coef in final_coef], axis=1)\n",
    "iterations_coefs = np.array([mean(abs(coef), axis=1) for coef in final_coef])\n",
    "std_coefs = np.std(((iterations_coefs)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(mean(final_coef, axis=1), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_features_imp = pd.DataFrame(list(zip(features_names_meal[0], play_coefficients, std_coefs)), columns = ['feature', 'coefficient', 'std'])\n",
    "play_features_imp.T.to_csv('feature_coefs2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_play = sum(coefficients_play)/len(coefficients_play)\n",
    "coefs_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names_play = [data.iloc[:,1:12].columns]\n",
    "pd.Series(np.transpose(abs(coefs_play[0])), index=features_names_play[0]).nlargest(12).plot(kind='barh', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_play = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    scores_play = cross_val_score(pipeline, X,y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    accuracy_play.append(mean(scores_play)*100)\n",
    "\n",
    "mean(accuracy_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_play = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    scores_recall_play = cross_val_score(pipeline, X,y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "    recall_play.append(mean(scores_recall_play)*100)\n",
    "    \n",
    "mean(recall_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_play = []\n",
    "for i in range(10):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    scores_precision_play = cross_val_score(pipeline, X,y, scoring='precision', cv=cv, n_jobs=-1)\n",
    "    precision_play.append(mean(scores_precision_play)*100)\n",
    "    \n",
    "mean(precision_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(accuracy_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['Speech activity', 'intervals/min', 'avg speech duration', 'std speech duration', 'avg silence duration', \n",
    "        'std silence duration', 'overlapping speech', 'avg os', 'std os', 'speech caregiver', 'speech child']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_meal[0], coefs_play[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'meal':abs(coefs_meal[0]), 'play':abs(coefs_play[0])}, index = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [35, 10]\n",
    "df.sort_values(by='meal', ascending=False).plot.bar(rot=0)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.legend(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.savefig('feature_importance.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the confusion matrix \n",
    "\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "cm_holder = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "        # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X[train_index], X[test_index]\n",
    "        y_train_, y_test_ = y[train_index], y[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "        # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "        print(model.predict(X_test_))\n",
    "        cm_temp = []\n",
    "        cm_temp.append(confusion_matrix(y_test_, model.predict(X_test_)))\n",
    "        print(cm_temp)\n",
    "        accuracy = accuracy_score(y_test_, model.predict(X_test_))\n",
    "        print(f\"Accuracy {accuracy:3f}\")\n",
    "        print(model.coef_)\n",
    "        coefficients_meal.append(model.coef_)\n",
    "        cm_holder.append(sum(cm_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_play= sum(cm_holder)\n",
    "matrix_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.imshow(matrix_play,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Play', fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize=16)\n",
    "plt.ylabel('Actual', fontsize=16)\n",
    "tick_marks = np.arange(len(set(y_test_))) # length of classes\n",
    "class_labels = ['no rad','rad']\n",
    "tick_marks\n",
    "plt.xticks(tick_marks,class_labels, fontsize=16)\n",
    "plt.yticks(tick_marks,class_labels, fontsize=16)\n",
    "# plotting text value inside cells\n",
    "thresh = matrix_play.max() / 2.\n",
    "for i,j in itertools.product(range(matrix_play.shape[0]),range(matrix_play.shape[1])):\n",
    "    plt.text(j,i,format(matrix_play[i,j],'d'),horizontalalignment='center',color='black' if matrix_play[i,j] >thresh else 'black', fontsize='xx-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown of meal and play agreements and disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "s_id = []\n",
    "labels=[]\n",
    "for i in range(10):\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=5, random_state=i, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        prediction = []\n",
    "        subject=[]\n",
    "        label = []\n",
    "            # scale training and test data based on statistics of only training data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_, X_test_ = X[train_index], X[test_index]\n",
    "        y_train_, y_test_ = y[train_index], y[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_)\n",
    "            # normalize data\n",
    "        X_train_, X_test_ = scaler.transform(X_train_), scaler.transform(X_test_)\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(X_train_, y_train_)\n",
    "       # prediction.append(model.predict(X_test_))\n",
    "        predictions.append(model.predict(X_test_))\n",
    "        #label.append(y_test_)\n",
    "      #  subject.append(model.predict(X_test_))\n",
    "        s_id.append(subject_id[test_index])\n",
    "        labels.append(y_test_)\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuple_index_pred = list()\n",
    "\n",
    "for x, y,z in zip(predictions, s_id, labels):\n",
    "    for l,m,n in zip(x,y,z):\n",
    "        tuple_index_pred.append((l,m,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = list()\n",
    "t_list2 = list()\n",
    "t_list3=list()\n",
    "for x in tuple_index_pred:\n",
    "    t_list.append(x[0])\n",
    "    t_list2.append(x[1])\n",
    "    t_list3.append(x[2])\n",
    "\n",
    "final_df = pd.DataFrame(list(zip(t_list, t_list2, t_list3)), columns = ['prediction', 's_id', 'label'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df=final_df.set_index('s_id')\n",
    "#final_df.sort_index()\n",
    "final_df.groupby('s_id').prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sort_values(by='s_id').head(11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
