{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Classification/speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('classification.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.array(data['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "print(scaler.fit(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=12)\n",
    "group_k_fold = GroupKFold(n_splits=6)\n",
    "splits = group_k_fold.split(X_shuffled, y_shuffled, groups_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_shuffled, y_shuffled, scoring='accuracy', cv=group_k_fold, n_jobs=-1, groups=groups_shuffled)\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = []\n",
    "n_shuffles = 10\n",
    "for i in range(n_shuffles):\n",
    "    X_shuffle, y_shuffle, groups_shuffle = shuffle(X, y, groups, random_state=i)\n",
    "   #print(groups_shuffle)\n",
    "    splits = group_k_fold.split(X_shuffle, y_shuffle, groups_shuffle)\n",
    "    scores2 = cross_val_score(model, X_shuffle, y_shuffle, scoring='recall', cv=splits, n_jobs=-1, groups=groups_shuffle)\n",
    "    accs.append(mean(scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a classification for each settging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/classification/speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('features.xlsx', sheet_name=\"Meal\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"Percent speech meal\":\"Total speaking time\", \"intervals/min meal\":\"segments/min meal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = np.array(data[\"Subject_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=7, shuffle=True, random_state=0)\n",
    "splits = k_fold.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "labels = list()\n",
    "coefs = list()\n",
    "for train_index, test_index in k_fold.split(X,y):\n",
    "    model.fit(X[train_index], y[train_index])\n",
    "    predictions.append(model.predict(X[test_index]))\n",
    "    labels.append(y[test_index])\n",
    "    coefs.append(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=0\n",
    "for x,y in zip(predictions,labels):\n",
    "    for prediction, label in zip(x,y):\n",
    "        if int(prediction)==int(label):\n",
    "            accuracy +=1\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "38/56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_meal = cross_val_score(model, X, y, scoring='accuracy', cv=loo.split(X), n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores_meal), std(scores_meal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_meal = cross_val_score(model, X, y, scoring='recall', cv=splits, n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores_meal), std(scores_meal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_meal = cross_val_score(model, X, y, scoring='precision', cv=splits, n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores_meal), std(scores_meal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_meal =list()\n",
    "precision_meal = list()\n",
    "recall_meal = list()\n",
    "n_shuffles = 10\n",
    "coefs = list()\n",
    "recall = list()\n",
    "predictions = list()\n",
    "cm_holder =[]\n",
    "for i in range(n_shuffles):\n",
    "    k_fold = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    split = k_fold.split(X,y)\n",
    "    cm_temp = []\n",
    "    for train_indexes, test_indexes in split:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        cm_temp.append(confusion_matrix(y[test_index], model.predict(X[test_index])))\n",
    "    cm_holder.append(sum(cm_temp))\n",
    "    #scores = cross_val_score(model, X, y, scoring='recall', cv=split, n_jobs=-1)\n",
    "    coefs.append(np.array(temp_list).mean(axis=0))\n",
    "    #recall_meal.append(mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_meal =list()\n",
    "precision_meal = list()\n",
    "recall_meal = list()\n",
    "n_shuffles = 10\n",
    "coefs = list()\n",
    "recall = list()\n",
    "predictions = list()\n",
    "cm_holder =[]\n",
    "\n",
    "for i in range(n_shuffles):\n",
    "    k_fold = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    split = k_fold.split(X,y)  \n",
    "    for train_indexes, test_indexes in split:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        temp_list = []\n",
    "    temp_list.append(model.coef_)\n",
    "        \n",
    "    #cm_holder.append(sum(cm_temp))\n",
    "    #scores = cross_val_score(model, X, y, scoring='accuracy', cv=split, n_jobs=-1)\n",
    "   # accuracy_meal.append(scores)\n",
    "    coefs.append(np.array(temp_list).mean(axis=0))\n",
    "    #recall_meal.append(mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std(accuracy_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_meal= np.rint(sum(cm_holder)/len(cm_holder)).astype(int)\n",
    "matrix_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.imshow(matrix_meal,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix Meal')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "tick_marks = np.arange(len(set(y[test_index]))) # length of classes\n",
    "class_labels = ['no rad','rad']\n",
    "tick_marks\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks(tick_marks,class_labels)\n",
    "# plotting text value inside cells\n",
    "thresh = matrix_meal.max() / 2.\n",
    "for i,j in itertools.product(range(matrix_meal.shape[0]),range(matrix_meal.shape[1])):\n",
    "    plt.text(j,i,format(matrix_meal[i,j],'d'),horizontalalignment='center',color='white' if matrix_meal[i,j] >thresh else 'black')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(recall_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(accuracy_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(precision), mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_coefs = sum(coefs)/len(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_coefs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(final_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(accs, popmean=0.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(accuracy_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.array(coefs)\n",
    "avg_coefs = coefs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "    \n",
    "features_names = [data.iloc[:,1:12].columns]\n",
    "pd.Series(np.transpose(abs(final_coefs[0])), index=features_names[0]).nlargest(12).plot(kind='barh', figsize=(10,10))\n",
    "plt.savefig('meal_features.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(abs(final_coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shuffles = 10\n",
    "for i in range(n_shuffles):\n",
    "    k_fold = KFold(n_splits=7, shuffle=True, random_state=i)\n",
    "    split = k_fold.split(X,y)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=split, n_jobs=-1)\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do it for play\n",
    "data = pd.read_excel('features.xlsx', sheet_name=\"Play\", engine='openpyxl')\n",
    "X = data.iloc[:,1:12].to_numpy()\n",
    "y = np.array([0 if x=='no_rad' else 1 for x in data.iloc[:,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"Percent speech play\":\"Total speaking time\", \"intervals/min play\":\"segments/min play\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shuffles = 10\n",
    "accuracy_play =list()\n",
    "#recall_play=list()\n",
    "precision_play=list()\n",
    "coefs_play = list()\n",
    "cm_holder =[]\n",
    "for i in range(n_shuffles):\n",
    "    k_fold = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    split = k_fold.split(X,y)\n",
    "    #cm_temp = []\n",
    "    for train_indexes, test_indexes in split:\n",
    "        model.fit(X[train_indexes], y[train_indexes])\n",
    "        temp_list = []\n",
    "    temp_list.append(model.coef_)\n",
    "      #  cm_temp.append(confusion_matrix(y[test_index], model.predict(X[test_index])))\n",
    "    #cm_holder.append(sum(cm_temp))\n",
    "    #scores = cross_val_score(model, X, y, scoring='accuracy', cv=split, n_jobs=-1)\n",
    "   # accuracy_play.append(scores)\n",
    "   # precision_play.append(mean(scores))\n",
    "    coefs_play.append(np.array(temp_list).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std(accuracy_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_play= np.rint(sum(cm_holder)/len(cm_holder)).astype(int)\n",
    "matrix_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.imshow(matrix_play,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix Play')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "tick_marks = np.arange(len(set(y[test_index]))) # length of classes\n",
    "class_labels = ['no rad','rad']\n",
    "tick_marks\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks(tick_marks,class_labels)\n",
    "# plotting text value inside cells\n",
    "thresh = matrix_play.max() / 2.\n",
    "for i,j in itertools.product(range(matrix_play.shape[0]),range(matrix_play.shape[1])):\n",
    "    plt.text(j,i,format(matrix_play[i,j],'d'),horizontalalignment='center',color='white' if matrix_play[i,j] >thresh else 'black')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(precision_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(accuracy_play, popmean=0.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(accuracy_meal,accuracy_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_play_final = sum(coefs_play)/len(coefs_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [data.iloc[:,1:12].columns]\n",
    "pd.Series(np.transpose(abs(coefs_play_final[0])), index=features_names[0]).nlargest(12).plot(kind='barh', figsize=(10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
