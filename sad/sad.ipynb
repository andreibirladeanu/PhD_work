{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out just one file and concatenating the speech segments to assess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyannote.core\n",
    "from pydub import AudioSegment\n",
    "file = {'uri':'1043_meal', 'audio':'/Users/andrei-macpro/Documents/Data/Audio/Meal/1043_meal.wav'}\n",
    "import torch\n",
    "# speech activity detection model trained on AMI training set\n",
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami')\n",
    "# obtain raw SAD scores (as `pyannote.core.SlidingWindowFeature` instance)\n",
    "sad_scores = sad(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.utils.signal import Binarize\n",
    "binarize = Binarize(offset=0.52, onset=0.52, log_scale=True, \n",
    "                    min_duration_off=0.1, min_duration_on=0.1)\n",
    "\n",
    "# speech regions (as `pyannote.core.Timeline` instance)\n",
    "speech = binarize.apply(sad_scores, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamps to miliseconds \n",
    "type(speech[0])\n",
    "# speech is a timeline made of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract speech and non-speech segments (start and end time) as a dictionary\n",
    "timestamps_speech=dict(speech)\n",
    "timestamps_silence = dict(speech.gaps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=AudioSegment.from_wav('/Users/andrei-macpro/Documents/Data/Audio/1043_meal.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydub works in miliseconds so we're going to convert the timestamps from the pyannote seconds to miliseconds\n",
    "start_times_speech=[x*1000 for x in list(timestamps_speech.keys())]\n",
    "end_times_speech=[x*1000 for x in list(timestamps_speech.values())]\n",
    "start_times_silence=[x*1000 for x in list(timestamps_silence.keys())]\n",
    "end_times_silence=[x*1000 for x in list(timestamps_silence.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_segments=[file[start_time:end_time] for start_time,end_time in zip(start_times_speech, end_times_speech)]\n",
    "silence_segments=[file[start_time:end_time] for start_time,end_time in zip(start_times_silence, end_times_silence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the concatenated speech and silence segments to disk\n",
    "sum(speech_segments).export('/Users/andrei-macpro/Documents/Data/Audio/speech.wav', format=\"wav\")\n",
    "sum(silence_segments).export('/Users/andrei-macpro/Documents/Data/Audio/non-speech.wav', format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's get on with the full speech detection processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyannote.core\n",
    "import torch\n",
    "from pyannote.core import Timeline, Segment\n",
    "from pyannote.audio.utils.signal import Binarize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/Meal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/andrei-macpro/Documents/Data/Audio/Meal'  #if string starts with slash it is considered absolute\n",
    "dirs = os.listdir( path )\n",
    "file_names=sorted([i for i in os.listdir(\".\") if not i.startswith(\".\")])\n",
    "#file_names = [x.replace('.wav','') for x in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami') # ok so this isn't taking up too much memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize = Binarize(offset=0.52, onset=0.52, log_scale=True, \n",
    "                    min_duration_off=0.1, min_duration_on=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a dictionary of key-filename : value-#pyannoteTimeline \n",
    "def timeline(file_names):\n",
    "    temp_list1=list()\n",
    "    temp_list2=list()\n",
    "    for file_name in tqdm(file_names):\n",
    "        sad_scores=sad(file_name)\n",
    "        speech=binarize.apply(sad_scores,dimension=1)\n",
    "        del sad_scores\n",
    "        temp_list1.append(file_name.get('audio'))\n",
    "        temp_list2.append(speech)\n",
    "    trial_dict=dict(zip(temp_list1,temp_list2))\n",
    "    return trial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract json files of the #pyannoteTimeline objects which can then be loaded again\n",
    "def write_disk(sad_segments):\n",
    "    for key in sad_segments.keys():\n",
    "        temp_json=sad_segments[key].for_json()\n",
    "        with open(str(key)+'.json', 'w') as outfile:\n",
    "            json.dump(temp_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary for files and filenames\n",
    "list_dict_filenames=list()\n",
    "files = dict()\n",
    "for file_name in file_names:\n",
    "    list_dict_filenames.append({'audio':str(file_name)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_segments=timeline(list_dict_filenames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "write_disk(sad_segments) #save Timeline objects as json files to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now we load the json files back to timelines to get the durations and no of intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first for meal\n",
    "import os\n",
    "import json\n",
    "import pyannote.core\n",
    "from pyannote.core import Timeline, Segment\n",
    "import statistics\n",
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/speech_detection_timestamps/speech_detection_meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes as input a list of file-names and outputs a list of pyannote timelines \n",
    "path = '/Users/andrei-macpro/Documents/Data/Audio/speech_detection_timestamps/speech_detection_meal'\n",
    "json_files = [pos_json for pos_json in sorted(os.listdir(path))]\n",
    "def get_timelines(json_files):\n",
    "    list_json=list()\n",
    "    list_timelines=list()\n",
    "    for filename in json_files: # loop that imports all json data into separate dictionaries\n",
    "        with open(filename) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            list_json.append(data)\n",
    "    for file, segments in zip(json_files, list_json): # loop to iterate through the files and create a new timeline for each of them \n",
    "        timeline=Timeline()\n",
    "        list_timelines.append(timeline.from_json(segments))\n",
    "    return list_timelines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_intervals(list_timelines): # standard dev of intervals of speech\n",
    "    std_interval_duration_speech_meal=list()\n",
    "    for timeline in list_timelines: # go thru each timeline\n",
    "        segment_duration=list()\n",
    "        for segment in timeline: # take the duration of each interval/segment\n",
    "            segment_duration.append(segment.duration) #append to a list ==> list of all interval durations from one TL\n",
    "        std_interval_duration_speech_meal.append(statistics.stdev(segment_duration)) # take std of that list\n",
    "    return std_interval_duration_speech_meal\n",
    "\n",
    "\n",
    "def std_intervals_silence(list_timelines):\n",
    "    std_interval_duration_silence_meal=list()\n",
    "    for timeline in list_timelines: # go thru each timeline\n",
    "        segment_duration=list()\n",
    "        for segment in timeline.gaps(): # take the duration of each interval/segment\n",
    "            segment_duration.append(segment.duration) #append to a list ==> list of all interval durations from one TL\n",
    "        std_interval_duration_silence_meal.append(statistics.stdev(segment_duration)) # take std of that list\n",
    "    return std_interval_duration_silence_meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_timelines_meal=get_timelines(json_files) # list containing the pyannote timeline of each meal recording "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to extract duration of speech and duration of non-speech and number of segments/intervals for each\n",
    "duration_speech_meal = [timeline.duration()/(timeline.duration()+timeline.gaps().duration())*100 for timeline in list_timelines_meal] # this is in seconds\n",
    "intervals_speech_meal =[len(timeline) for timeline in list_timelines_meal]\n",
    "intervals_per_min_meal= [(interval/timeline.duration())*60 for interval,timeline in zip(intervals_speech_meal, list_timelines_meal)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_interval_duration__speech_meal= [timeline.duration()/len(timeline) for timeline in list_timelines_meal]\n",
    "std_interval_duration_speech_meal=std_intervals(list_timelines_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should i do silence as well ? \n",
    "avg_interval_duration__silence_meal= [timeline.gaps().duration()/len(timeline.gaps()) for timeline in list_timelines_meal]\n",
    "std_interval_duration_silence_meal=std_intervals_silence(list_timelines_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_timelines[0].gaps().duration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for play\n",
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/speech_detection_timestamps/speech_detection_play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/andrei-macpro/Documents/Data/Audio/speech_detection_timestamps/speech_detection_play'\n",
    "json_files = [pos_json for pos_json in sorted(os.listdir(path))]\n",
    "list_timelines_play=get_timelines(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_speech_play = [timeline.duration()/(timeline.duration()+timeline.gaps().duration())*100 for timeline in list_timelines_play] # this is in seconds\n",
    "intervals_speech_play =[len(timeline) for timeline in list_timelines_play]\n",
    "intervals_per_min_play= [(interval/timeline.duration())*60 for interval,timeline in zip(intervals_speech_play, list_timelines_play)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_interval_duration__speech_play= [timeline.duration()/len(timeline) for timeline in list_timelines_play]\n",
    "std_interval_duration_speech_play=std_intervals(list_timelines_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_interval_duration__silence_play= [timeline.gaps().duration()/len(timeline.gaps()) for timeline in list_timelines_play]\n",
    "std_interval_duration_silence_play=std_intervals_silence(list_timelines_play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do number of transitions after 1,2,3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write them to pandas and then save it to a spreadsheet\n",
    "# first need to get an index for pandas \n",
    "index_participants=sorted([int(i[:4]) for i in os.listdir(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas df\n",
    "df = pd.DataFrame(list(zip(duration_speech_meal, intervals_per_min_meal,avg_interval_duration__speech_meal, \n",
    "                           std_interval_duration_speech_meal,avg_interval_duration__silence_meal, std_interval_duration_silence_meal,\n",
    "                           duration_speech_play, \n",
    "                           intervals_per_min_play, \n",
    "                           avg_interval_duration__speech_play,\n",
    "                           std_interval_duration_speech_play,\n",
    "                           avg_interval_duration__silence_play,\n",
    "                           std_interval_duration_silence_play)), \n",
    "               columns =['Percent speech meal', 'intervals/min meal', 'Avg speech duration meal', \n",
    "                         'std speech duration meal', 'Avg silence duration meal',\n",
    "                         'std silence duration meal',\n",
    "                         'Percent speech play',\n",
    "                        'intervals/min play','Avg speech duration play', \n",
    "                         'std speech duration play', 'Avg silence duration play', \n",
    "                         'std silence duration play'], index=index_participants) \n",
    "df.index.name='Subject_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('speech_detection_features.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
