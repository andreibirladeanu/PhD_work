{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out just one file and concatenating the speech segments to assess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyannote.core\n",
    "from pydub import AudioSegment\n",
    "file = {'uri':'1043_meal', 'audio':'/Users/andrei-macpro/Documents/Data/Audio/Meal/1043_meal.wav'}\n",
    "import torch\n",
    "# speech activity detection model trained on AMI training set\n",
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami')\n",
    "# obtain raw SAD scores (as `pyannote.core.SlidingWindowFeature` instance)\n",
    "sad_scores = sad(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.utils.signal import Binarize\n",
    "binarize = Binarize(offset=0.52, onset=0.52, log_scale=True, \n",
    "                    min_duration_off=0.1, min_duration_on=0.1)\n",
    "\n",
    "# speech regions (as `pyannote.core.Timeline` instance)\n",
    "speech = binarize.apply(sad_scores, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamps to miliseconds \n",
    "type(speech[0])\n",
    "# speech is a timeline made of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract speech and non-speech segments (start and end time) as a dictionary\n",
    "timestamps_speech=dict(speech)\n",
    "timestamps_silence = dict(speech.gaps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=AudioSegment.from_wav('/Users/andrei-macpro/Documents/Data/Audio/1043_meal.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydub works in miliseconds so we're going to convert the timestamps from the pyannote seconds to miliseconds\n",
    "start_times_speech=[x*1000 for x in list(timestamps_speech.keys())]\n",
    "end_times_speech=[x*1000 for x in list(timestamps_speech.values())]\n",
    "start_times_silence=[x*1000 for x in list(timestamps_silence.keys())]\n",
    "end_times_silence=[x*1000 for x in list(timestamps_silence.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_segments=[file[start_time:end_time] for start_time,end_time in zip(start_times_speech, end_times_speech)]\n",
    "silence_segments=[file[start_time:end_time] for start_time,end_time in zip(start_times_silence, end_times_silence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the concatenated speech and silence segments to disk\n",
    "sum(speech_segments).export('/Users/andrei-macpro/Documents/Data/Audio/speech.wav', format=\"wav\")\n",
    "sum(silence_segments).export('/Users/andrei-macpro/Documents/Data/Audio/non-speech.wav', format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's get on with the full speech detection processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyannote.core\n",
    "import torch\n",
    "from pyannote.core import Timeline, Segment\n",
    "from pyannote.audio.utils.signal import Binarize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/Meal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/andrei-macpro/Documents/Data/Audio/Meal'  #if string starts with slash it is considered absolute\n",
    "dirs = os.listdir( path )\n",
    "file_names=sorted([i for i in os.listdir(\".\") if not i.startswith(\".\")])\n",
    "#file_names = [x.replace('.wav','') for x in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami') # ok so this isn't taking up too much memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize = Binarize(offset=0.52, onset=0.52, log_scale=True, \n",
    "                    min_duration_off=0.1, min_duration_on=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a dictionary of key-filename : value-#pyannoteTimeline \n",
    "def timeline(file_names):\n",
    "    temp_list1=list()\n",
    "    temp_list2=list()\n",
    "    for file_name in tqdm(file_names):\n",
    "        sad_scores=sad(file_name)\n",
    "        speech=binarize.apply(sad_scores,dimension=1)\n",
    "        del sad_scores\n",
    "        temp_list1.append(file_name.get('audio'))\n",
    "        temp_list2.append(speech)\n",
    "    trial_dict=dict(zip(temp_list1,temp_list2))\n",
    "    return trial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract json files of the #pyannoteTimeline objects which can then be loaded again\n",
    "def write_disk(sad_segments):\n",
    "    for key in sad_segments.keys():\n",
    "        temp_json=sad_segments[key].for_json()\n",
    "        with open(str(key)+'.json', 'w') as outfile:\n",
    "            json.dump(temp_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary for files and filenames\n",
    "list_dict_filenames=list()\n",
    "files = dict()\n",
    "for file_name in file_names:\n",
    "    list_dict_filenames.append({'audio':str(file_name)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_segments=timeline(list_dict_filenames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "write_disk(sad_segments) #save Timeline objects as json files to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now we load the json files back to timelines to get the durations and non-speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andrei-macpro/Documents/Data/Audio/speech_detection_meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('1043_meal.wav.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a loop that imports all json data into separate dictionaries\n",
    "path = '/Users/andrei-macpro/Documents/Data/Audio/speech_detection_meal'\n",
    "json_files = [pos_json for pos_json in sorted(os.listdir(path))]\n",
    "list_json=list()\n",
    "for filename in json_files:\n",
    "   with open(filename) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        list_json.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline=Timeline() # load a new Timeline object that can load json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a loop to iterate through the files and create a new timeline for each of them \n",
    "list_timelines=list()\n",
    "for file, segments in zip(json_files, list_json):\n",
    "    timeline=Timeline()\n",
    "    list_timelines.append(timeline.from_json(segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_timelines[0].duration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented=list_timelines[0].segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented.duration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_timelines[0].gaps().duration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to extract duration of speech and duration of non-speech and number of segments/intervals for each\n",
    "duration_speech_meal = [timeline.duration()/(timeline.duration()+timeline.gaps().duration())*100 for timeline in list_timelines] # this is in seconds\n",
    "intervals_speech_meal =[len(timeline) for timeline in list_timelines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write them to pandas and then save it to a spreadsheet\n",
    "# first need to get an index for pandas \n",
    "index_participants=sorted([int(i[:4]) for i in os.listdir(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas df\n",
    "df = pd.DataFrame(list(zip(duration_speech_meal, intervals_speech_meal)), \n",
    "               columns =['Speech Duration Meal(sec)', 'No of speech intervals meal'], index=index_participants) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel file\n",
    "df.to_excel('speech_detection.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
